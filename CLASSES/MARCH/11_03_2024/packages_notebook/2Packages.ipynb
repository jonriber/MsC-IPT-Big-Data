{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Text2Analytics</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Data Acquisition</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">2 - Packages</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<p><a href=\"2Packages.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages-Evaluation\" data-toc-modified-id=\"Packages-Evaluation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages Evaluation</a></span></li><li><span><a href=\"#Newspaper3k:-Article-scraping-&amp;-curation\" data-toc-modified-id=\"Newspaper3k:-Article-scraping-&amp;-curation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Newspaper3k: Article scraping &amp; curation</a></span></li><li><span><a href=\"#JusText\" data-toc-modified-id=\"JusText-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>JusText</a></span></li><li><span><a href=\"#Wikipedia\" data-toc-modified-id=\"Wikipedia-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Wikipedia</a></span><ul class=\"toc-item\"><li><span><a href=\"#Languages-Supported\" data-toc-modified-id=\"Languages-Supported-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Languages Supported</a></span></li><li><span><a href=\"#Search\" data-toc-modified-id=\"Search-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Search</a></span></li><li><span><a href=\"#Text-Extraction\" data-toc-modified-id=\"Text-Extraction-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Text Extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Page\" data-toc-modified-id=\"Page-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Page</a></span><ul class=\"toc-item\"><li><span><a href=\"#Title\" data-toc-modified-id=\"Title-4.3.2.1\"><span class=\"toc-item-num\">4.3.2.1&nbsp;&nbsp;</span>Title</a></span></li><li><span><a href=\"#URL\" data-toc-modified-id=\"URL-4.3.2.2\"><span class=\"toc-item-num\">4.3.2.2&nbsp;&nbsp;</span>URL</a></span></li><li><span><a href=\"#Content\" data-toc-modified-id=\"Content-4.3.2.3\"><span class=\"toc-item-num\">4.3.2.3&nbsp;&nbsp;</span>Content</a></span></li><li><span><a href=\"#Image-Links\" data-toc-modified-id=\"Image-Links-4.3.2.4\"><span class=\"toc-item-num\">4.3.2.4&nbsp;&nbsp;</span>Image Links</a></span></li><li><span><a href=\"#Wikipedia-Links\" data-toc-modified-id=\"Wikipedia-Links-4.3.2.5\"><span class=\"toc-item-num\">4.3.2.5&nbsp;&nbsp;</span>Wikipedia Links</a></span></li><li><span><a href=\"#Categories\" data-toc-modified-id=\"Categories-4.3.2.6\"><span class=\"toc-item-num\">4.3.2.6&nbsp;&nbsp;</span>Categories</a></span></li></ul></li><li><span><a href=\"#wiki_page-function\" data-toc-modified-id=\"wiki_page-function-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>wiki_page function</a></span></li><li><span><a href=\"#wiki_scraping\" data-toc-modified-id=\"wiki_scraping-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>wiki_scraping</a></span></li></ul></li></ul></li><li><span><a href=\"#Google\" data-toc-modified-id=\"Google-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Google</a></span><ul class=\"toc-item\"><li><span><a href=\"#Search-Results\" data-toc-modified-id=\"Search-Results-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Search Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#googlesearch-package\" data-toc-modified-id=\"googlesearch-package-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>googlesearch package</a></span></li><li><span><a href=\"#googleapiclient-package\" data-toc-modified-id=\"googleapiclient-package-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>googleapiclient package</a></span></li><li><span><a href=\"#SerpAPI\" data-toc-modified-id=\"SerpAPI-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>SerpAPI</a></span></li></ul></li><li><span><a href=\"#Google-Maps\" data-toc-modified-id=\"Google-Maps-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Google Maps</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá ter conhecimento de packages Python que facilitam a aquisição de dados.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should be aware of Python packages that ease the process of data acquisition. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Aquisição de dados com recurso a packages Python<a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos à aquisição de dados com recurso a packages Python. Extração de dados a partir de páginas web com recurso ao Newspaper3k, JusText e a bibliotecas da Wikipedia.\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Data acquisition using Python packages <a class=\"tocSkip\">\n",
    "Introducing students to data acquisition with Python packages. Data acquisition from web pages using Newspaper3k, JusText, and Wikipedia.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Is Web Scraping Legal?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Is web scraping legal?](https://scrapediary.com/is-web-scraping-legal/)\n",
    "- [Is web scraping illegal?](https://www.imperva.com/blog/is-web-scraping-illegal/)\n",
    "- [Web scraping is now legal](https://medium.com/@tjwaterman99/web-scraping-is-now-legal-6bf0e5730a78)\n",
    "- [O Web scraping é legal ou ilegal?](http://datascienceacademy.com.br/blog/web-scraping-e-web-crawling-sao-legais-ou-ilegais/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/Text2Analytics/DataAcquisition.jpg\" width=\"500\" height=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>https://github.com/tsolewski/Text_extraction_comparison_PL</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper3k: Article scraping & curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Newspaper3k](https://github.com/codelucas/newspaper/) é uma biblioteca python (inspirada no requests) que dado um URL, permite extrair conteúdos de uma página web. Apesar de funcionar em cima de qualquer página web, é especialmente funcional na extração de contéudos de artigos noticiosos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#url = 'https://pt.wikipedia.org/wiki/Merc%C3%BArio_(planeta)'\n",
    "#url = 'https://arquivo.pt/noFrame/replay/20150409001238/http://pt.wikipedia.org/wiki/Jos%C3%A9_S%C3%B3crates'\n",
    "#url = 'https://www.publico.pt/2022/04/03/local/noticia/ir-pe-emprego-escola-desafio-nao-facil-andar-lisboa-1996445?ref=hp&cx=stories_cover__important_b--507490'\n",
    "url = 'https://eco.sapo.pt/2021/07/20/temos-a-expectativa-que-a-venda-da-groundforce-pelo-montepio-seja-concluida-com-sucesso-diz-pedro-nuno-santos/'\n",
    "#url = 'https://multinews.sapo.pt/noticias/plano-de-contingencia-nos-aeroportos-entra-hoje-em-funcionamento-em-pleno-recorde-as-principais-medidas/'\n",
    "#url = \"https://arquivo.pt/noFrame/replay/20100326190237/http://www.publico.pt/Pol%C3%ADtica/passos-coelho-ganha-em-macau_1429610\"\n",
    "url = \"https://www.dinheirovivo.pt/bolsa/bolsa-de-lisboa-em-alta-com-sonae-a-subir-127-16285173.html\"\n",
    "url = \"https://arquivo.pt/noFrame/replay/20170107192028/https://www.dinheirovivo.pt/outras/morreu-mario-soares/\"\n",
    "url = \"http://www.ftof-finance.co.uk\"\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo código permite fazer um parse ao artigo e extrair informação sobre os autores, título e data de publicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "try:\n",
    "    article.parse()\n",
    "except Exception as e:\n",
    "    if \"404\" in str(e):\n",
    "        print({'message1': str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.publish_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente podemos ter acesso ao texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem como acesso a algumas keywords e ao sumário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na eventualidade de não ter o `NLTK` instalado deverá proceder à sua instalação e à execução do seguinte código:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JusText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jusText](https://github.com/miso-belica/jusText) is a tool for removing boilerplate content, such as navigation links, headers, and footers from HTML pages. It is designed to preserve mainly text containing full sentences and it is therefore well suited for creating linguistic resources such as Web corpora. Online demo: https://nlp.fi.muni.cz/projects/justext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install justext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import justext\n",
    "\n",
    "#url = \"http://planet.python.org/\"\n",
    "#url = \"https://arquivo.pt/noFrame/replay/20100326190237/http://www.publico.pt/Pol%C3%ADtica/passos-coelho-ganha-em-macau_1429610\"\n",
    "#url = \"https://arquivo.pt/noFrame/replay/20091218122806/http://www.noticiasdacovilha.pt/pt/artigos/show/scripts/core.htm?p=artigos&f=show&lang=pt&pag=1&area=1&idseccao=1&idartigo=440\"\n",
    "#url = \"https://arquivo.pt/noFrame/replay/20101129034557/http://www.noticiasdacovilha.pt/pt/artigos/show/scripts/core.htm?p=artigos&f=show&lang=pt&pag=5&area=1&idseccao=1&idartigo=763\"\n",
    "url = \"https://arquivo.pt/noFrame/replay/20200110180847/https://www.noticiasdacovilha.pt/covilhanense-nos-jogos-olimpicos-da-juventude-de-inverno/\"\n",
    "#url = \"https://arquivo.pt/noFrame/replay/20100326190237/http://www.publico.pt/Pol%C3%ADtica/passos-coelho-ganha-em-macau_1429610\"\n",
    "\n",
    "response = requests.get(url)\n",
    "paragraphs = justext.justext(response.content, justext.get_stoplist(\"Portuguese\"))\n",
    "for paragraph in paragraphs:\n",
    "    if not paragraph.is_boilerplate:\n",
    "        print (paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aceder aos dados da Wikipedia comece por instalar o package [wikipedia](https://pypi.org/project/wikipedia/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importação da biblioteca dá-se a partir do seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages Supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ter acesso às linguas suportadas pela wikipedia execute o seguinte código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pretender obter resultados a partir do domínio PT execute o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como o Google, a Wikipedia também tem o seu próprio motor de busca que permite uma pesquisa sob a enorme quantidade de artigos aí armazenados. O método `search`, em concreto, recebe uma pesquisa como input, retornando, em resposta, o conjunto de pesquisas relacionadas com o tópico mais habitualmente efetuadas. Naturalmente, os resultados devolvidos variam ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wikipedia.search(\"covid-19\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter um sumário de uma dada página use o método `summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = wikipedia.summary(\"covid-19\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para para fazer um parse dos vários elementos da página (title, url, etc) comece por executar o método `page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = wikipedia.page(\"covid-19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = covid.content\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wikipedia Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ter acesso aos labels dos links que apontam para outras páginas wikipedia execute o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid.links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wiki_page function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "def wiki_page(page_name):\n",
    "    \n",
    "    wiki = wikipedia.page(page_name)\n",
    "    \n",
    "    title = wiki.title\n",
    "    content = wiki.content\n",
    "    summary = wiki.summary\n",
    "    url = wiki.url\n",
    "    categories = wiki.categories\n",
    "    wiki_links = wiki.links\n",
    "    \n",
    "    return title, content, summary, url, categories, wiki_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, content, summary, url, categories, wiki_links = wiki_page(\"covid-19\")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wiki_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte define a função `wiki_scraping` que, dada uma página da wikipedia, deverá devolver uma lista onde cada posição é ocupada com um dicionário que guarda os títulos, conteúdos, sumário, url, categorias e links de todas as páginas wikipedia referidas na página dada como input. Posteriormente essa lista será usada para alimentar um dataframe. Uma vez que as páginas wikipedia podem ter múltiplas referências a outras tantas páginas wikipedia, possibilitamos ao utilizador a definição do número de links que pretende obter (`num_links`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "def wiki_page(page_name):\n",
    "    try:\n",
    "        wiki = wikipedia.page(page_name)       \n",
    "        data = {'title': wiki.title, 'content': wiki.content, 'summary': wiki.summary, 'url': wiki.url, 'categories': wiki.categories,  'wiki_links':  wiki.links}\n",
    "\n",
    "        return data\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def wiki_scraping(page_name, num_links):\n",
    "    data = []\n",
    "    \n",
    "    wiki_data = wiki_page(page_name)\n",
    "    if len(wiki_data) == 0: #se a pag principal não existir o processo termina\n",
    "        return\n",
    "    \n",
    "    data.append(wiki_data)\n",
    "    \n",
    "    wiki_links = wiki_data['wiki_links']\n",
    "    min_num_links = min(len(wiki_links),num_links)\n",
    "\n",
    "    for j in range(len(wiki_links[:min_num_links])):\n",
    "        wiki_data = wiki_page(wiki_links[j])\n",
    "        if len(wiki_data) > 0: #Apenas adiciona à lista se a pag wikipedia existir\n",
    "            data.append(wiki_data)\n",
    "\n",
    "        if j % 1 == 0:\n",
    "            print(f\"\\r{100*j/len(wiki_links[:min_num_links]):.2f}%\", end='')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte invoca a função `wiki_scraping` solicitando a obtenção de 20 páginas wikipedia relacionadas para o assunto dado como input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wiki_scraping(\"covid-19\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pôde observar o código é relativamento lento a executar. Para contornar o problema recorremos ao processamento [paralelo](https://analyticsindiamag.com/run-python-code-in-parallel-using-multiprocessing/) do código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import concurrent.futures\n",
    "\n",
    "def wiki_page(page_name):\n",
    "    try:\n",
    "        wiki = wikipedia.page(page_name)       \n",
    "        data = {'title': wiki.title, 'content': wiki.content, 'summary': wiki.summary, 'url': wiki.url, 'categories': wiki.categories,  'wiki_links':  wiki.links}\n",
    "\n",
    "        return data\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def wiki_scraping(page_name, num_links):\n",
    "    data = []\n",
    "    \n",
    "    wiki_data = wiki_page(page_name)  \n",
    "    if len(wiki_data) == 0: #se a pag principal não existir o processo termina\n",
    "        return\n",
    "    data.append(wiki_data)\n",
    "    \n",
    "    wiki_links = wiki_data['wiki_links']\n",
    "    min_num_links = min(len(wiki_links),num_links)\n",
    "    \n",
    "    j = 1\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_link = {executor.submit(wiki_page, link): link for link in wiki_links[:min_num_links]}\n",
    "        for future in concurrent.futures.as_completed(future_link):\n",
    "            if j % 1 == 0:\n",
    "                print(f\"\\r{100*j/len(wiki_links[:min_num_links]):.2f}%\", end='')\n",
    "                j += 1\n",
    "            \n",
    "            if len(future.result()) > 0:\n",
    "                data.append(future.result()) if data else None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte invoca a função `wiki_scraping` solicitando a devolução de 20 páginas wikipedia mencionadas no tópico dado como entrada. Observe que a execução é claramente mais rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wiki_scraping(\"covid-19\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos agora construir um dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### googlesearch package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar o seguinte código necessita do package [google-search](https://pypi.org/project/google-search/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/MarioVilas/googlesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import justext\n",
    "import requests\n",
    "\n",
    "def newspaper3k_get_text(url):\n",
    "    text = \"\"\n",
    "    response = requests.get(url)\n",
    "    paragraphs = justext.justext(response.content, justext.get_stoplist(\"Portuguese\"))\n",
    "    for paragraph in paragraphs:\n",
    "        if not paragraph.is_boilerplate:\n",
    "            text += paragraph.text\n",
    "    \n",
    "    return text\n",
    "\n",
    "query = \"marcelo rebelo de sousa\"\n",
    "my_results_list = []\n",
    "for url in search(query,        # The query you want to run\n",
    "                tld = 'pt',  # The top level domain\n",
    "                lang = 'pt',  # The language\n",
    "                num = 10,     # Number of results per page\n",
    "                start = 0,    # First result to retrieve\n",
    "                stop = 5,  # Last result to retrieve\n",
    "                pause = 2.0,  # Lapse between HTTP requests\n",
    "               ):    \n",
    "    \n",
    "    fullContentLenght_Newspaper3K = newspaper3k_get_text(url)\n",
    "    my_results_list.append({'fullContent': fullContentLenght_Newspaper3K, 'url':url})\n",
    "    print(f\"URL = {url}\")\n",
    "    \n",
    "my_results_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(my_results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### googleapiclient package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte permite-nos ter acesso aos resultados do Google dada uma query. Com vista a esse objetivo vamos usar o Custom Search JSON API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Search JSON API provides 100 search queries per day for free. If you need more, you may sign up for [billing](https://support.google.com/cloud#topic=3340599) in the API Console. Additional requests cost $5 per 1000 queries, up to 10k queries per day.\n",
    "\n",
    "If you need more than 10k queries per day and your Custom Search Engine searches 10 sites or fewer, you may be interested in the [Custom Search Site Restricted JSON API](https://developers.google.com/custom-search/v1/site_restricted_api), which does not have a daily query limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira coisa a fazer é proceder à instalação do seguinte package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida é necessário criar a:\n",
    "- [Google Api Key](https://console.developers.google.com)\n",
    "- [Google Custom Search Engine ID](https://cse.google.com/cse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja o seguinte vídeo para mais detalhes acerca da criação das keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<div align=\"center\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Bxy8Yqp5XX0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma visão mais ampla dos serviços da Google pode ser obtida na [Google Cloud Platform](https://console.cloud.google.com). Note que, um cartão de crédito, é normalmente requerido para a obtenção de uma key. A grande maioria das APIs oferece no entanto um conjunto de execuções gratuitas mensais. Em qualquer dos casos crie um cartão de crédito virtual (no valor de 5€) com validade de 1 mês."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais informações [aqui](https://github.com/googleapis/google-api-python-client/blob/master/docs/start.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte permite-nos ter acesso aos resultados de uma dada query. Não se esqueça de especificar as respetivas keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pprint\n",
    "\n",
    "#Nao se esqueca de especificar as keys\n",
    "my_api_key = \"\"\n",
    "my_cse_id = \"\"\n",
    "query = \"python\"\n",
    "\n",
    "\n",
    "def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res\n",
    "\n",
    "results = google_search(query, my_api_key, my_cse_id, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte dá-nos acesso à totalidade do JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para termos acesso aos principais nós do JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in results:\n",
    "    pprint.pprint(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ter acesso a toda a informação devolvida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['items']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A determinada informação em particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results['items']:\n",
    "    pprint.pprint(result['title'])\n",
    "    pprint.pprint(result['snippet'])\n",
    "    pprint.pprint(result['link'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informação sobre o número total de resultados indexados para a query em questão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['searchInformation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Google Custom Search Engine restringe a devolução de resultados a um máximo de 10 páginas com um máximo de 10 resultados. Os resultados acima obtidos dizem respeito aos primeiro 10 resultados (num = 10) da primeira página (default, quando não especificado = 1). Assim se necessitar de mais resultados deverá fazer 10 pedidos com o num = 10 e o parâmetro start = 1, 11, 21, ... 91."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SerpAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O site [SerpAPI](https://serpapi.com/) fornece um serviço de querying a vários motores de busca (incluindo o Google). No entanto o serviço é pago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exercício abaixo vamos recorrer à API Maps do Google para obtermos a latitute e a longitude de uma data cidade. Se ainda não tem uma API KEY veja como obter uma na seção Google Search Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#Nao se esqueca de inserir a key no codigo abaixo\n",
    "key = ''\n",
    "payload = {'sensor':'false','address': 'sport lisboa e benfica','key':key}\n",
    "r = requests.get('https://maps.googleapis.com/maps/api/geocode/json', params=payload)\n",
    "print(\"GET\",r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para termos acesso ao conteúdo JSON pode colocar simplesmente o URL num qualquer browser.\n",
    "# Alternativamente pode imprimir o weatherData. Com vista a esse objetivo recorra ao módulo pprint\n",
    "# o qual tem por objetivo imprimir estruturas de Python (no caso um dicionário) de forma o mais\n",
    "# adequada possível\n",
    "import pprint\n",
    "contentsJSon = r.json()\n",
    "pprint.pprint(contentsJSon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenha por fim a latitude, longitude e localização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = contentsJSon[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "lng = contentsJSon[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "location = contentsJSon['results'][0]['formatted_address']\n",
    "print(lat)\n",
    "print(lng)\n",
    "print(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
