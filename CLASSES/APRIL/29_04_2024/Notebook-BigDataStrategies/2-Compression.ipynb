{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Big Data</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Big Data Strategies</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">2 - Compression Techniques</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><a href=\"2-Compression.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lossless\" data-toc-modified-id=\"Lossless-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lossless</a></span><ul class=\"toc-item\"><li><span><a href=\"#Don’t-load-all-the-columns\" data-toc-modified-id=\"Don’t-load-all-the-columns-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Don’t load all the columns</a></span></li><li><span><a href=\"#Shrink-numerical-columns-with-smaller-dtypes\" data-toc-modified-id=\"Shrink-numerical-columns-with-smaller-dtypes-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Shrink numerical columns with smaller dtypes</a></span></li><li><span><a href=\"#Shrink-categorical-data-using-Categorical-dtypes\" data-toc-modified-id=\"Shrink-categorical-data-using-Categorical-dtypes-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Shrink categorical data using Categorical dtypes</a></span></li><li><span><a href=\"#Shrink-object-data-to-datetime\" data-toc-modified-id=\"Shrink-object-data-to-datetime-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Shrink object data to datetime</a></span></li></ul></li><li><span><a href=\"#Lossy\" data-toc-modified-id=\"Lossy-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Lossy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Changing-numeric-representations\" data-toc-modified-id=\"Changing-numeric-representations-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Changing numeric representations</a></span></li><li><span><a href=\"#Sampling\" data-toc-modified-id=\"Sampling-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Sampling</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression Techniques <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá ter conhecimento de técnicas de compressão de dados.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should be aware of compression techniques.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Técnicas de Compressão<a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos à compressão de elevados volumes de dados\n",
    "- sem perdas\n",
    "- com perdas\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Compression Techniques <a class=\"tocSkip\">\n",
    "Introducing students to the process of compressing big data files\n",
    "- lossless\n",
    "- lossy\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is based in the following webpage: https://pythonspeed.com/articles/pandas-load-less-data/ and https://pythonspeed.com/articles/pandas-reduce-memory-lossy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re loading a CSV into Pandas, and it’s using too much RAM: your program crashes if you load the whole thing. How do you reduce memory usage without changing any of your processing code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression means using a different representation for your data, in a way that uses less memory. There are two forms of compression:\n",
    "\n",
    "- <b>Lossless</b>: The data you’re storing has the exact same information as the original data.\n",
    "- <b>Lossy</b>: The data you’re storing loses some of the details in the original data, but in a way that ideally doesn’t impact the results of your calculation very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how to reduce the memory your DataFrame uses at the time it is initially loaded, using three different techniques (same data, less RAM: that’s the beauty of compression):\n",
    "- Don’t load all the columns\n",
    "- Shrink numerical columns with smaller dtypes.\n",
    "- Shrink categorical data using Categorical dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don’t load all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often the CSV you’re loading will include columns you don’t actually use when processing the data. If you don’t use them, there’s no point in loading them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, I am loading the full CSV (the same as before, obtained from here: https://www.kaggle.com/datasets/bharath150/taxi-data?select=yellow_tripdata_2016-03.csv). Yellow taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv')\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and checking how much memory is used by the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I will shrink it down to just the columns I’m interested in. We can observe that the peak memory usage is 3.8GB, even though we’re only really using 2.5GB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df[[\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]]\n",
    "df_filter.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we know in advance we only need those few columns, we don’t need to load everything, we can just load only the columns we care about (through `usecols`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv' , usecols=[\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"])\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', usecols=[\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " thus reducing peak memory usage to 2.5GB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrink numerical columns with smaller dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique can help reduce the memory used by columns that contain only numbers. Each column in a Pandas DataFrame is a particular data type (`dtype`). For example, for integers there is the `int64` dtype, `int32`, `int16`, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the dtype matter? First, because it affects what values you can store in that column:\n",
    "- `int8` can store integers from -128 to 127.\n",
    "- `int16` can store integers from -32768 to 32767.\n",
    "- `int64` can store integers from -9223372036854775808 to 9223372036854775807."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the larger the range, the more memory is used. For example, int64 uses 4× as much memory as int16, and 8× as much as int8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default when Pandas loads a CSV, it guesses at the dtypes. If it decides a column volumes are all integers, by default it assigns that column int64 as the dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, if you know that the numbers in a particular column will never be higher than 32767, you can use an int16 and reduce the memory usage of that column by 75%. And if the values will never be higher than 127, you can use an int8, using even less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the case of the column `passenger_count` which will never have a value higher than 9, and yet it has assigned an int64 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['passenger_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much memory it uses (93.2MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['passenger_count']].info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that `int8` won’t lose any data because the numbers in the column only range from 1 to 9. So lets define it right when loading the dataset by making use of the `dtype` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv', dtype={\"passenger_count\": \"int8\"})\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', dtype={\"passenger_count\": \"int8\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much less memory the column is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['passenger_count']].info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `astype` method do convert the datatype if we forget to convert it during the read_csv process. Example: `df = df.astype({'passenger_count':'int8'})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrink categorical data using Categorical dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about non-numerical data? In some cases you can shrink those columns as well. Imagine a gender column that only says \"FEMALE\", \"MALE\", and \"NON-BINARY\" over and over again—that’s a lot of memory being used to store the same three strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more compact representation for data with only a limited number of values is a custom dtype called `Categorical`, whose memory usage is tied to the number of different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we begin by checking the memomry usage of the `store_and_fwd_flag` which stores only `N` and `Y` values and is of `object` type. We can observer that it occupies 675MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['store_and_fwd_flag']].info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load the CSV, we can specify that a particular column uses this `dtype`, that is `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv', dtype={\"store_and_fwd_flag\": \"category\"})\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', dtype={\"store_and_fwd_flag\": \"category\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and the resulting memory usage is much smaller (11MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['store_and_fwd_flag']].info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrink object data to datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible and recommendable to convert object data in the format of dates to a datetime object. Lets start by seeing how much memory RAM does the system needs when loading the dates as an object: 3.8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv')\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then convert those columns to datetime by using the `parse_dates` parameter. From what occurrs next, we can observe that converting the object dates to datetime led the memory to reduce from 3.8GB to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv', parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique you can try is lossy compression: drop some of your data in a way that doesn’t impact your final results too much. If parts of your data don’t impact your analysis, no need to waste memory keeping extraneous details around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, in this article we’ll cover the following techniques:\n",
    "- Changing numeric column representation.\n",
    "- Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing numeric representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say you have a DataFrame with a column that represents the `trip_distance` of the taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_distance'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial representation is a floating point number, loaded as float64 by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_distance'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe how much memory does it occupies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['trip_distance']].info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for most purposes having a huge amount of accuracy isn’t too important. So one thing we can do is change from float64 to float32 (as explained in the lossy - shrink numerical columns with smaller dtypes, section), which will cut memory usage in half, in this case with only minimal loss of accuracy. But we can do even better. If we’re willing to lose some more detail, we can reduce memory usage to 1/4th of the original size. Instead of representing the values as floating numbers, we can represent them as int16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df[\"trip_distance\"] = np.round(df[\"trip_distance\"]).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_distance'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_distance'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['trip_distance']].info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load only the top-x rows of your dataframe use the `nrows` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', nrows=20)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can specify what percent of lines you want, rather than how many lines, you don't even need to get the file size and you just need to read through the file once. Assuming a header on the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "p = 0.01  # 1% of the lines keep the header, then take only 1% of lines. if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', header=0, \n",
    "         skiprows=lambda i: i>0 and random.random() > p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be more specific, then you need to know the number of your rows your full dataframe has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('data/yellow_tripdata_2016-03.csv')\n",
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then specify that we want the system to create a list of `n-s` rows within the range of `n`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "\n",
    "n = 12210952 #number of records in file\n",
    "s = 1000 #desired sample size\n",
    "ListOfRows2Skip = sorted(random.sample(range(n),n-s))\n",
    "ListOfRows2Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv', skiprows = ListOfRows2Skip)\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
