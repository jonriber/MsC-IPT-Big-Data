{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Information Retrieval</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Framewords</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">2 - Redis</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](http://www.ccc.ipt.pt/~ricardo)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><a href=\"2Redis.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Redis\" data-toc-modified-id=\"Redis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Redis</a></span><ul class=\"toc-item\"><li><span><a href=\"#When-would-we-use-Redis?\" data-toc-modified-id=\"When-would-we-use-Redis?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>When would we use Redis?</a></span></li><li><span><a href=\"#What-is-Redis-used-for?\" data-toc-modified-id=\"What-is-Redis-used-for?-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>What is Redis used for?</a></span></li><li><span><a href=\"#Can-Redis-be-used-with-Elastic-Search?\" data-toc-modified-id=\"Can-Redis-be-used-with-Elastic-Search?-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Can Redis be used with Elastic Search?</a></span></li></ul></li><li><span><a href=\"#Installation\" data-toc-modified-id=\"Installation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Installation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Docker\" data-toc-modified-id=\"Docker-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Docker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Installing-Docker\" data-toc-modified-id=\"Installing-Docker-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Installing Docker</a></span></li><li><span><a href=\"#redis\" data-toc-modified-id=\"redis-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>redis</a></span></li></ul></li></ul></li><li><span><a href=\"#Play-with-Redis-on-Python\" data-toc-modified-id=\"Play-with-Redis-on-Python-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Play with Redis on Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hello-World-Example\" data-toc-modified-id=\"Hello-World-Example-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Hello World Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Connect-to-Redis\" data-toc-modified-id=\"Connect-to-Redis-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Connect to Redis</a></span></li><li><span><a href=\"#Indexing\" data-toc-modified-id=\"Indexing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Indexing</a></span></li><li><span><a href=\"#Get\" data-toc-modified-id=\"Get-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Get</a></span></li><li><span><a href=\"#Exists\" data-toc-modified-id=\"Exists-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Exists</a></span></li><li><span><a href=\"#Delete-key-value-pair\" data-toc-modified-id=\"Delete-key-value-pair-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Delete key-value pair</a></span></li><li><span><a href=\"#Delete-database\" data-toc-modified-id=\"Delete-database-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Delete database</a></span></li><li><span><a href=\"#Persistently-Storage\" data-toc-modified-id=\"Persistently-Storage-3.1.7\"><span class=\"toc-item-num\">3.1.7&nbsp;&nbsp;</span>Persistently Storage</a></span></li></ul></li><li><span><a href=\"#Index-Different-Types-of-Data\" data-toc-modified-id=\"Index-Different-Types-of-Data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Index Different Types of Data</a></span></li><li><span><a href=\"#Redisearch\" data-toc-modified-id=\"Redisearch-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Redisearch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Traditional-Search\" data-toc-modified-id=\"Traditional-Search-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Traditional Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Indexing\" data-toc-modified-id=\"Indexing-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Indexing</a></span></li><li><span><a href=\"#Searching\" data-toc-modified-id=\"Searching-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>Searching</a></span></li><li><span><a href=\"#Quizzes\" data-toc-modified-id=\"Quizzes-3.3.1.3\"><span class=\"toc-item-num\">3.3.1.3&nbsp;&nbsp;</span>Quizzes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Big-Bang-Theory\" data-toc-modified-id=\"Big-Bang-Theory-3.3.1.3.1\"><span class=\"toc-item-num\">3.3.1.3.1&nbsp;&nbsp;</span>Big Bang Theory</a></span></li><li><span><a href=\"#Publico\" data-toc-modified-id=\"Publico-3.3.1.3.2\"><span class=\"toc-item-num\">3.3.1.3.2&nbsp;&nbsp;</span>Publico</a></span></li></ul></li></ul></li><li><span><a href=\"#Vector-Similarity-Search\" data-toc-modified-id=\"Vector-Similarity-Search-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Vector Similarity Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Indexing\" data-toc-modified-id=\"Indexing-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>Indexing</a></span></li><li><span><a href=\"#Searching\" data-toc-modified-id=\"Searching-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>Searching</a></span><ul class=\"toc-item\"><li><span><a href=\"#KNN\" data-toc-modified-id=\"KNN-3.3.2.2.1\"><span class=\"toc-item-num\">3.3.2.2.1&nbsp;&nbsp;</span>KNN</a></span></li><li><span><a href=\"#Range\" data-toc-modified-id=\"Range-3.3.2.2.2\"><span class=\"toc-item-num\">3.3.2.2.2&nbsp;&nbsp;</span>Range</a></span></li><li><span><a href=\"#Hybrid\" data-toc-modified-id=\"Hybrid-3.3.2.2.3\"><span class=\"toc-item-num\">3.3.2.2.3&nbsp;&nbsp;</span>Hybrid</a></span></li></ul></li><li><span><a href=\"#Quizzes\" data-toc-modified-id=\"Quizzes-3.3.2.3\"><span class=\"toc-item-num\">3.3.2.3&nbsp;&nbsp;</span>Quizzes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Wikipedia-(Hugging-Face-Embeddings)\" data-toc-modified-id=\"Wikipedia-(Hugging-Face-Embeddings)-3.3.2.3.1\"><span class=\"toc-item-num\">3.3.2.3.1&nbsp;&nbsp;</span>Wikipedia (Hugging Face Embeddings)</a></span></li><li><span><a href=\"#Wikipedia-(OpenAI-Embeddings)\" data-toc-modified-id=\"Wikipedia-(OpenAI-Embeddings)-3.3.2.3.2\"><span class=\"toc-item-num\">3.3.2.3.2&nbsp;&nbsp;</span>Wikipedia (OpenAI Embeddings)</a></span></li></ul></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bibliography\" data-toc-modified-id=\"Bibliography-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bibliography</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá saber trabalhar com sistemas de bases de dados nosql redis com vista à indexação de dados e à recuperação de informação, com recurso a métodos tradicionais de pesquisa e à utilização de embeddings obtidos a partir da framework HuggingFace e OpenAI.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should know how to operate with nosql databases such as redis to index and retrieve textual data, through traditional IR techniques and embeddings vectors obtained from HuggingFace and OpenAI </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Frameworks de indexação e recuperação de informação <a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos ao Redis.\n",
    "- O que é o Redis?\n",
    "- Instalação\n",
    "- Indexação\n",
    "- Recuperação de informação com recurso a métodos tradicionais e a embeddings obtidos a partir do HuggingFace e da OpenAI\n",
    "- Uso do Redis-Insight para acesso à informação indexada\n",
    "- Casos práticos de indexação e recuperação de informação (big bang theory; jornal público; wikipedia)\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Indexing and retrieval frameworks <a class=\"tocSkip\">\n",
    "Introduction to Redis.\n",
    "- What is Redis?\n",
    "- Installation\n",
    "- Indexing\n",
    "- Information retrieval using traditional methods and embedding vectors obtained from HugginFace and OpenAI\n",
    "- Use of Redis-Insight to access indexed information\n",
    "- Practical cases of indexing and information retrieval (big bang theory; jornal público; wikipedia)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis (stands for Remote Dictionary Service) is `an open-source, networked, in-memory, key-value data store with optional durability`.\n",
    "- **Open source** - Anyone can inspect Redis’ code or contribute to the [project](https://github.com/redis/redis).\n",
    "- **Networked** - Redis uses the client-server model, in which communication between clients and servers happens over a network. In this tutorial, both a client and server are run on a single machine. In production systems, a load-balanced, multinode Redis cluster (of servers) will service requests from several clients.\n",
    "- **In-memory** — Redis stores data in primary computer memory (i.e., RAM). Data stored in RAM is more quickly accessed than data stored in slower (secondary) memory such as on disk. This is what makes Redis fast and suitable for applications that require low-latency access to data (e.g., caching).\n",
    "- **Key-value data store** - Redis is a glorified dictionary. The main operations supported are setting keys with their values and getting values using their associated keys, and other variations of this\n",
    "- **Optional durability** -  being an in-memory data store, isn’t fully durable by default. There is a chance that some of the data that was successfully written to Redis may be lost in the event the Redis host fails. Notice that RAM is volatile, meaning that if the power is lost, well, so too is what’s stored in RAM. This is undesirable for anyone who wants to build systems that maximize performance without compromising durability. Fortunately, Redis provides several persistence options with different performance durability profiles, including an option that affords full durability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis has a client-server architecture and uses a request-response model. This means that you (the client) connect to a Redis server through TCP connection, on port 6379 by default. You request some action (like some form of reading, writing, getting, setting, or updating), and the server serves you back a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, there are many parallels you can draw between a Python dictionary and what Redis is and does:\n",
    "- A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands.\n",
    "- Redis keys are always strings.\n",
    "- Redis values may be a number of different data types.\n",
    "- Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When would we use Redis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis is commonly used in web stacks as the `cache` service accessed by the API. For example, the client (browser) will make a request to the API (server) for some common piece of data, like “Top 10 Product Categories”. Rather than the API going directly to the database, executing a potentially long running query statement (depending on how well your database schema is structured) and awaiting the result, we can populate both the Redis key-value store and the database with identical information.\n",
    "\n",
    "In this scenario, the database is used as the “true data source” and Redis is used as the lightweight, fast lookup for commonly accessed data — like Top Categories. If the client requested all information about a particular, unpopular Product ID, for instance, we probably should not store all of this data in-memory with Redis and eat up all of our server’s RAM; instead, we would just execute a slower database query to service the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Redis used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though Redis could be used as your primary Database it's usually not what it is used for. Redis is usually used as a cache, which is a fast way to store data in memory and as a message queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can Redis be used with Elastic Search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis and Elasticsearch are two distinct and complementary technologies that are often used together to create powerful and efficient data processing and search solutions. They serve different purposes, and their integration can provide a robust system for data storage, retrieval, and search.\n",
    "\n",
    "For example, Redis can be used as a caching layer in front of Elasticsearch. This helps reduce the load on Elasticsearch by storing frequently accessed data in Redis. When a request is made, the system checks Redis first. If the data is found, it's served directly from Redis, reducing the response time. If the data is not in Redis, the system retrieves it from Elasticsearch, stores it in Redis, and serves it to the user. This approach improves response times and reduces the load on Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many applications, you can combine Elasticsearch for search and analytics with Redis for caching and real-time data needs. For example, you can use Elasticsearch for primary data storage and retrieval, while Redis caches frequently accessed data to reduce the load on Elasticsearch and improve response times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with redis we will need to have Docker installed. A tutorial on how to install Docker can be found [here](https://www.di.ubi.pt/~rcampos/DataScience/1Toolkit/2Docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have Docker installed, we will be ready to proceed with the redis installation making use of the `docker-compose.yml` file, which should be saved in a folder of your choice (e.g., `H:\\Backup\\Research\\Programming\\CodePython\\MyDocker\\redis`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "version: '3.7'\n",
    "services:\n",
    "\n",
    "  redis:\n",
    "    image: redis/redis-stack:latest\n",
    "    ports:\n",
    "      - 6379:6379\n",
    "      - 8001:8001\n",
    "    environment:\n",
    "      - REDISEARCH_ARGS=CONCURRENT_WRITE_MODE\n",
    "    volumes:\n",
    "      - vector-db:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"-h\", \"localhost\", \"-p\", \"6379\", \"ping\"]\n",
    "      interval: 2s\n",
    "      timeout: 1m30s\n",
    "      retries: 5\n",
    "      start_period: 5s\n",
    "\n",
    "volumes:\n",
    "  vector-db:\n",
    "      driver: local\n",
    "      driver_opts:\n",
    "        type: 'none'\n",
    "        o: 'bind'\n",
    "        device: 'h:\\Backup\\Research\\Programming\\CodePython\\MyDocker\\redis\\data'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code defines a service named redis that runs a Redis database, specifically using the `redis-stack` image. Let's break down the key components of this configuration:\n",
    "\n",
    "- `redis`: This is the name of the docker service being defined.\n",
    "- `image`: It specifies the Docker image to use for this service, in this case, \"redis/redis-stack:latest.\" It will pull the latest version of the Redis image from Docker Hub.\n",
    "- `container_name: redis`: This line gives the container the name \"redis.\" You can reference this name when interacting with the container or other Docker commands.\n",
    "- `ports`: This section maps ports on the host to ports within the container. It specifies that port 6379 on the host should be mapped to port 6379 in the container, and port 8001 on the host should be mapped to port 8001 in the container. This allows external applications to connect to Redis on port 6379 and access the redis-insight on port 8001 inside the container.\n",
    "- `environment`: This sets environment variables for the container. It specifies REDISEARCH_ARGS with the value CONCURRENT_WRITE_MODE.\n",
    "- `volumes`: it defines a named volume named \"redis_data\" and binds it to the /data directory within the container. This setup is used to persist Redis data so that it survives container restarts or removals. The data will be stored in the \"redis_data\" volume on the host machine (more on this below).\n",
    "- `healthcheck`: This section specifies a health check for the service to ensure it's running correctly. It uses the redis-cli command to ping the Redis server running on \"localhost\" at port 6379. If the command succeeds, the service is considered healthy. The health check runs every 2 seconds, with a timeout of 1 minute and 30 seconds. It retries up to 5 times, and it starts checking health after a delay of 5 seconds.\n",
    "- `Volume Configuration`: This section defines the \"redis_data\" volume. It specifies that the volume should be of type \"local\" and binds the volume to a host directory (device) located at the specified path on your local machine. This is where Redis data will be stored outside the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the prompt, move to the folder where you have your `docker-compose.yml` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "h:\n",
    "cd h:\\Backup\\Research\\Programming\\CodePython\\MyDocker\\redis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next execute the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker-compose up -d`, where `-d` (from detach) means that the process will be running in background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start pulling the images from docker registry and create the containers. It may take a while depending on whether you already have the images on your machine or not and also depending on your internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, try to access **redis-insight** (redis UI):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Redis on Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect and use Redis in python, we will be using a python module **called redis-py**. It can be installed by running the following command in the command prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (5.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything ready, let’s get our hands dirty and dive into the programming part. `db=0` means that we are connecting to database 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "# connect to redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have connected to the Redis server, let’s start performing simple operations. \n",
    "To set a key-value pair, we use the `set` function that accepts the key and the value as the parameter. Please note that the key should always be either of string data type or bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 10061 connecting to localhost:6379. Nenhuma ligação pôde ser feita porque o computador de destino\r\nas recusou ativamente.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:276\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\retry.py:46\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[1;34m(self, do, fail)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:277\u001b[0m, in \u001b[0;36mAbstractConnection.connect.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m--> 277\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisconnect(error)\n\u001b[0;32m    278\u001b[0m     )\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:639\u001b[0m, in \u001b[0;36mConnection._connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket.getaddrinfo returned an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:627\u001b[0m, in \u001b[0;36mConnection._connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msocket_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Nenhuma ligação pôde ser feita porque o computador de destino\r\nas recusou ativamente",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# set a key\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest-key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest-value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\commands\\core.py:2342\u001b[0m, in \u001b[0;36mBasicKeyCommands.set\u001b[1;34m(self, name, value, ex, px, nx, xx, keepttl, get, exat, pxat)\u001b[0m\n\u001b[0;32m   2339\u001b[0m     pieces\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2340\u001b[0m     options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 2342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpieces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:540\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[1;34m(self, *args, **options)\u001b[0m\n\u001b[0;32m    538\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\n\u001b[0;32m    539\u001b[0m command_name \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 540\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_command_parse_response(\n\u001b[0;32m    545\u001b[0m             conn, command_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m    546\u001b[0m         ),\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[0;32m    548\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:1104\u001b[0m, in \u001b[0;36mConnectionPool.get_connection\u001b[1;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_use_connections\u001b[38;5;241m.\u001b[39madd(connection)\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[1;32m-> 1104\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:282\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout connecting to server\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(e))\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m sock\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mConnectionError\u001b[0m: Error 10061 connecting to localhost:6379. Nenhuma ligação pôde ser feita porque o computador de destino\r\nas recusou ativamente."
     ]
    }
   ],
   "source": [
    "# set a key\n",
    "r.set('test-key', 'test-value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on redis-insight: http://localhost:8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the value for a specific key, we use the `get` function that accepts the key for which we want the value to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a value\n",
    "value = r.get('test-key')\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice also that the type of the returned object, `b'` is Python’s bytes type, not str. It is bytes rather than str that is the most common return type across redis-py, so you may need to call `r.get(\"test-key\").decode(\"utf-8\")` depending on what you want to actually do with the returned bytestring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a value\n",
    "value = r.get('test-key').decode(\"utf-8\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exists` command check if a key exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.exists('test-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete key-value pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete some key just use the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "r.delete('test-key')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Redis, you can't delete an entire database as you can with some other databases. Redis doesn't provide a command to remove all keys and data from a specific database, but you can delete keys individually or flush the entire dataset as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "r.flushdb()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also use the following code to flush all databases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "r.flushall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistently Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Redis stores data in-memory, it does not face the same read/write (I/O) limitations and performance issues that databases face with hard disks. Although Redis stores and accesses data in-memory, it can be persisted on the hard disk for resilience; Docker Volumes makes this process easy. In our case data will be stored within the folder defined in the docker-compose.yml file, which was `H:\\Backup\\Research\\Programming\\CodePython\\MyDocker\\redis\\data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save data persistently one has two options:\n",
    "1. Under redis insight (http://localhost:8001) one can go to the `cli` option (on the bottom of the page) and write the following command: `save`\n",
    "2. Alternatively, on the CMD one can write the following command `docker exec -ti redis /bin/bash`, where `redis` is the container name to access the redis file structure. Once inside, move to the `data` folder and execute the following command: `redis-cli save`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless the option used, a file named `dump.rdb` will be saved under our local folder `H:\\Backup\\Research\\Programming\\CodePython\\MyDocker\\redis\\data`. This means that if your data gets corrupted or you run into problems with your docker container, you will always have your data here (don't forget to save it from time to time). If you want to try this, just remove the docker container (on docker desktop, for example) and run again the docker-compose command. Then go the redis-insight and you will see that your data is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Different Types of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis also allows you to set and get multiple key-value pairs in one command, `MSET` and `MGET`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new information, however, will be saved in another database. To do so, we should connect again to redis, but this time specifying `db=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "# connect to redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to `set` the multiple key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.mset({\"Croatia\": \"Zagreb\", \"Bahamas\": \"Nassau\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `get` them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfValues = r.mget('Croatia','Bahamas')\n",
    "ListOfValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to see this key-value pairs in the redis-insight UI just change on the top of the interface from db0 to db1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to store deeply nested objects with different data types, serialization techniques like using json or pickle can be used. Let’s see this in action to better understand. The following code stores information about a given employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "e1={\n",
    "    \"first_name\":\"nitin\",\n",
    "    \"last_name\":\"panwar\",\n",
    "    \"age\": 27,\n",
    "    \"about\": \"Love to play cricket\",\n",
    "    \"interests\": ['sports','music'],\n",
    "}\n",
    "r.set('e1', json.dumps(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the information stored, we can directly use “get” function and then undo the stringification performed by json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(r.get('e1'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interest in data['interests']:\n",
    "    print(interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets insert some more employees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's insert some more documents\n",
    "e2={\n",
    "    \"first_name\" :  \"Jane\",\n",
    "    \"last_name\" :   \"Smith\",\n",
    "    \"age\" :         32,\n",
    "    \"about\" :       \"I like to collect rock albums\",\n",
    "    \"interests\":  [ \"music\" ]\n",
    "}\n",
    "e3={\n",
    "    \"first_name\" :  \"Douglas\",\n",
    "    \"last_name\" :   \"Fir\",\n",
    "    \"age\" :         35,\n",
    "    \"about\":        \"I like to build cabinets\",\n",
    "    \"interests\":  [ \"forestry\" ]\n",
    "}\n",
    "e4={\n",
    "    \"first_name\":\"asd\",\n",
    "    \"last_name\":\"pafdfd\",\n",
    "    \"age\": 27,\n",
    "    \"about\": \"Love to play football\",\n",
    "    \"interests\": ['sports','music'],\n",
    "}\n",
    "\n",
    "r.set('e2', json.dumps(e2))\n",
    "r.set('e3', json.dumps(e3))\n",
    "r.set('e4', json.dumps(e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose a situation, where, instead of having multiple dictionaries (one for each employee) we have a list of employees, where each position is a dictionary keeping the info of one given employee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "\n",
    "# connect to redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Employee information\n",
    "e1={\n",
    "    \"first_name\":\"nitin\",\n",
    "    \"last_name\":\"panwar\",\n",
    "    \"age\": 27,\n",
    "    \"about\": \"Love to play cricket\",\n",
    "    \"interests\": ['sports','music'],\n",
    "}\n",
    "\n",
    "e2={\n",
    "    \"first_name\" :  \"Jane\",\n",
    "    \"last_name\" :   \"Smith\",\n",
    "    \"age\" :         32,\n",
    "    \"about\" :       \"I like to collect rock albums\",\n",
    "    \"interests\":  [ \"music\" ]\n",
    "}\n",
    "\n",
    "ListOfEmployees = [e1, e2]\n",
    "\n",
    "r.set('ListOfEmployees', json.dumps(ListOfEmployees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the list of employees we can simply run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(r.get('ListOfEmployees'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for employee in data:\n",
    "    print(employee['first_name'])\n",
    "    for interest in employee['interests']:\n",
    "        print(interest)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that you want to add a new employee to the list. How to do that?\n",
    "1. Retrieve the existing ListOfEmployees from Redis.\n",
    "2. Deserialize the JSON data to get a Python list.\n",
    "3. Append the new employee (e3) to the list.\n",
    "4. Serialize the updated list back to JSON and Store the updated JSON data in Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Retrieve the existing ListOfEmployees from Redis\n",
    "existing_data = r.get('ListOfEmployees')\n",
    "\n",
    "# Check if any data exists in Redis\n",
    "if existing_data:\n",
    "    # Deserialize the existing JSON data to a Python list\n",
    "    existing_employees = json.loads(existing_data)\n",
    "else:\n",
    "    # If no data exists, create an empty list\n",
    "    existing_employees = []\n",
    "\n",
    "# Define the new employee (e3)\n",
    "e3 = {\n",
    "    \"first_name\": \"John\",\n",
    "    \"last_name\": \"Doe\",\n",
    "    \"age\": 35,\n",
    "    \"about\": \"I enjoy hiking and photography\",\n",
    "    \"interests\": [\"outdoor activities\", \"photography\"],\n",
    "}\n",
    "\n",
    "# Append the new employee (e3) to the existing list\n",
    "existing_employees.append(e3)\n",
    "\n",
    "# Serialize the updated list back to JSON and Store the updated JSON data in Redis\n",
    "r.set('ListOfEmployees', json.dumps(existing_employees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redisearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RediSearch](https://nimblehq.co/blog/getting-started-with-redisearch) is a Full-Text Search engine, available as a module for Redis. In a full-text search, a search engine examines all of the words in every stored document as it tries to match search criteria e.g. the text specified by a user. Contrary to wildcard search offered by databases via LIKE query, full-text search is primarily based on natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by flushing the database before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.flushdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to connect to redis. Note that we are not specifying any database in particular, thus it will store the results in `db0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "def connect_redis():    \n",
    "    r = redis.Redis(\n",
    "            host=\"localhost\",\n",
    "            port=6379,\n",
    "            decode_responses=True,\n",
    "        )\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells will show how to specify and create a search index in Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.search.indexDefinition import IndexDefinition\n",
    "\n",
    "def create_index(r, index_name, doc_prefix, fields):    \n",
    "    try:\n",
    "        #  check if index exists\n",
    "        r.ft(index_name).info()\n",
    "        print(\"Index already exists!\")\n",
    "    except:\n",
    "        #create index\n",
    "        r.ft(index_name).create_index(fields=fields, definition=IndexDefinition(prefix=[doc_prefix]))\n",
    "        print(\"Index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a search index, we can load documents into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(r, doc_prefix, documents):\n",
    "    for i, doc in enumerate(documents):\n",
    "        key = f\"{doc_prefix}{str(i)}\"\n",
    "        r.hset(key, mapping=doc)\n",
    "    \n",
    "    print(\"Documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to delete all the data and the index using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_data(r, index_name, delete_documents=True):\n",
    "    try:\n",
    "        r.ft(index_name).dropindex(delete_documents=delete_documents)\n",
    "        print('Index and data dropped')\n",
    "    except:\n",
    "        print('Index does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gathers all the code together. We begin by defining the data to index (the employee information). Note that RediSearch primarily indexes and searches textual data. If `interests` is a list of strings representing the interests of employees, you should ensure that the data you store in the \"interests\" field is serialized as text (e.g., as JSON) so that it can be indexed and searched as a text field within RediSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as the `index_name` (the name of the index) and `doc_prefix` (the prefix for the document keys). In addition to that, `first_name`, `last_name`, `about` and `interests` are defined as a `TextField`, while `age` as a `NumericField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField\n",
    "\n",
    "# Employee information\n",
    "e1={\n",
    "    \"first_name\":\"nitin\",\n",
    "    \"last_name\":\"panwar\",\n",
    "    \"age\": 27,\n",
    "    \"about\": \"Love to play cricket\",\n",
    "    \"interests\": json.dumps(['sports', 'music'])\n",
    "}\n",
    "\n",
    "e2={\n",
    "    \"first_name\" :  \"Jane\",\n",
    "    \"last_name\" :   \"Smith\",\n",
    "    \"age\" :         32,\n",
    "    \"about\" :       \"I like to collect rock albums\",\n",
    "    \"interests\":  json.dumps(['music'])\n",
    "}\n",
    "\n",
    "e3={\n",
    "    \"first_name\" :  \"Douglas\",\n",
    "    \"last_name\" :   \"Fir\",\n",
    "    \"age\" :         35,\n",
    "    \"about\":        \"I like to build cabinets\",\n",
    "    \"interests\":  json.dumps([\"forestry\" ])\n",
    "}\n",
    "e4={\n",
    "    \"first_name\":\"asd\",\n",
    "    \"last_name\":\"pafdfd\",\n",
    "    \"age\": 27,\n",
    "    \"about\": \"Love to play football\",\n",
    "    \"interests\": json.dumps(['sports','music'])\n",
    "}\n",
    "\n",
    "ListOfEmployees = [e1, e2, e3, e4]\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:employees' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'employees:' #prefix for the document keys\n",
    "\n",
    "# Define RediSearch fields\n",
    "first_name = TextField(name=\"first_name\")\n",
    "last_name = TextField(name=\"last_name\")\n",
    "age = NumericField(name=\"age\")\n",
    "about = TextField(name=\"about\")\n",
    "interests = TextField(name=\"interests\")\n",
    "\n",
    "fields = [first_name, last_name, age, about, interests]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ListOfEmployees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on redis-insight: http://localhost:8001. To do so, go to the top-left corner of redis interface and click the button `search by values or keys`, specifying the `idx:employees`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all of the above functions will be used once and then from now onwards. To make things easier, we should create (in the same folder of this notebook) a file named `redis_service.py` with the code and the imports of each of the four functions: `connect_redis`, `create_index`, `index_documents` and `drop_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustrates the wildcard used to get all the results in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = r.ft(index_name).search(\"@*\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the total number of docs execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to iterate over the results the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in results.docs:   \n",
    "    print(doc['id'])\n",
    "    print(doc['first_name'])\n",
    "    print(doc['last_name'])\n",
    "    print(doc['age'])\n",
    "    print(doc['about'])\n",
    "    print(json.loads(doc['interests']))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for results among all the fields you just need to specify the query (e.g., `nitin`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = r.ft(index_name).search('nitin')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to search for a specific field you can use the `@name_of_field` (e.g., `@first_name`). You may also want to use *abc* to look for a text that has `abc` inside. The following retrieves the documents where the field `first_name` has the word `nitin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  search a specific field\n",
    "results = r.ft(index_name).search('@first_name:*nitin*')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are looking for an exact match you cannot use the `*`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  search a specific field\n",
    "results = r.ft(index_name).search('@about:play cricket')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to retrieve all those documents that either have `play` or `cricket` then you can use the or (`|`) operator. Looking at the second result you can observe that it only has the word `play` in the text: `Love to play football`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = r.ft(index_name).search(\"@about:play|@about:cricket\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the operator or (`|`) to specify a query in different fields. In the following we are looking for the word `nitin` within the fields `first_name` and `about`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  search specific fields - OR operation\n",
    "results = r.ft(index_name).search('@first_name|about:nitin')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or specify an `and` condition, that is, an exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  search specific fields - AND operation\n",
    "results = r.ft(index_name).search('@first_name:nitin @about:nitin')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for results in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  search a specific field\n",
    "results = r.ft(index_name).search('@interests:music')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the coolest features is `fuzzy search` which allows aproximate searches. This is good in cases when input query or documents can contain errors. In order to fuzzy match, we need to wrap our phrase with `%`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy search\n",
    "results = r.ft(index_name).search('@about:%criket%')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next delete all data and the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = connect_redis()\n",
    "index_name = 'idx:employees'\n",
    "drop_data(r, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quizzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Big Bang Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we want to index information related to the `Big Bang Theory` show, namely:\n",
    "- id\n",
    "- season\n",
    "- episode\n",
    "- name\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this regard, we are going to use the [Tvmaze](https://www.tvmaze.com/api) API through the following endpoint: `http://api.tvmaze.com/singlesearch/shows?q=big-bang-theory&embed=episodes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://api.tvmaze.com/singlesearch/shows?q=big+bang+theory&embed=episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the total number of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentsJSon = r.json()\n",
    "len(contentsJSon[\"_embedded\"][\"episodes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get information from the first episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentsJSon = r.json()\n",
    "contentsJSon[\"_embedded\"][\"episodes\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to create a list of dicts, where each object represents a doc to index. Note that the `summary` field includes some boilerplate which is to be removed by the `re` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ldocs = []\n",
    "for jo in contentsJSon[\"_embedded\"][\"episodes\"][0:200]:\n",
    "    d = {}\n",
    "    d['id'] = jo['id']\n",
    "    d['season'] = jo['season']\n",
    "    d['episode'] = jo['number']\n",
    "    d['name'] = jo['name']\n",
    "    d[\"summary\"] = re.sub('<[^<]+?>', '', jo['summary'])\n",
    "    ldocs.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ldocs` contains 200 docs to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(ldocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets proceed by indexing the documents. The following gathers all the code together. We begin by importing the functions `connect_redis`, `create_index` and `index_documents` from the file `redis_service`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as `index_name` (the name of the index), `doc_prefix` (the prefix for the document keys). In addition to that, `season` and `episode` are defined as a `NumericField`, while `name` and `summary` as a `TextField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField\n",
    "from redis_service import connect_redis, create_index, index_documents\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:bibbang' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'bibbang_episode:' #prefix for the document keys\n",
    "\n",
    "# Define RediSearch fields\n",
    "season = NumericField(name=\"season\")\n",
    "episode = NumericField(name=\"episode\")\n",
    "name = TextField(name=\"name\")\n",
    "summary = TextField(name=\"summary\")\n",
    "\n",
    "fields = [season, episode, name, summary]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ldocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following enables one to search for the term `rivalry` in the field `summary`. We can use the method `Query` or simply not use it (as we have done before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.search.query import Query\n",
    "\n",
    "#  search a specific field\n",
    "results = r.ft(index_name).search(Query('@summary:rivalry'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the data and indexes. To do that, we should import the function `drop_data` and `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_service import drop_data, connect_redis\n",
    "\n",
    "r = connect_redis()\n",
    "index_name = 'idx:bibbang'\n",
    "drop_data(r, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Publico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise we aim to make use of the [TLS-Covid19](https://github.com/LIAAD/tls-covid19) dataset which has the key moments of the [liveblog](https://www.publico.pt/coronavirus-ao-minuto) of the Jornal Público dedicated to Covid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we begin by loading the file into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/publico_km.json\", \"r\",encoding=\"utf8\") as readfile:\n",
    "    KeyMoments = json.load(readfile)\n",
    "\n",
    "print(KeyMoments[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets proceed by indexing the documents. The following gathers all the code together. We begin by importing the functions `connect_redis`, `create_index` and `index_documents` from the file `redis_service`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as `index_name` (the name of the index), `doc_prefix` (the prefix for the document keys). In addition to that, `source`, `lan`, `topic`, `date`, `title`, `news` and `url` are defined as a `TextField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField\n",
    "from redis_service import connect_redis, create_index, index_documents\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:tlscovid' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'tlscovid_news:' #prefix for the document keys\n",
    "\n",
    "# Define RediSearch fields\n",
    "source = TextField(name=\"source\")\n",
    "lan = TextField(name=\"lan\")\n",
    "topic = TextField(name=\"topic\")\n",
    "date = TextField(name=\"date\")\n",
    "title = TextField(name=\"title\")\n",
    "news = TextField(name=\"news\")\n",
    "url = TextField(name=\"url\")\n",
    "\n",
    "fields = [source, lan, topic, date, title, news, url]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, KeyMoments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now search for a specific topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.search.query import Query\n",
    "\n",
    "#  search a specific field\n",
    "results = r.ft(index_name).search(Query('@topic:estado de emergência'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the data and indexes. To do that, we should import the function `drop_data` and `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_service import drop_data, connect_redis\n",
    "\n",
    "r = connect_redis()\n",
    "index_name = 'idx:tlscovid'\n",
    "drop_data(r, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors (also called `Embeddings`), represent an AI model’s impression (or understanding) of a piece of unstructured data like text, images, audio, videos, etc. Vector Similarity Search (VSS) is the process of finding vectors in the vector database that are similar to a given query vector. Popular VSS uses include recommendation systems, image and video search, document retrieval, and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by having a look at this article before we proceed: https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining some dummy data related to some wikipedia pages that we want to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfContents = [\n",
    "    {\"title\": \"Wuhan\", \"url\": \"https://en.wikipedia.org/wiki/Wuhan\"},\n",
    "    {\"title\": \"Coronavirus\", \"url\": \"https://en.wikipedia.org/wiki/Coronavirus\"},\n",
    "    {\"title\": \"Vaccine\", \"url\": \"https://en.wikipedia.org/wiki/COVID-19_vaccine\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a fake embedding for each of the titles and keep them in a new column named `title_emb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for content in ListOfContents:\n",
    "    content['title_emb'] = np.random.rand(756)\n",
    "\n",
    "ListOfContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `index_documents` function, which is slightly different than the one we have defined on the `redis_service` file, as in this case we need to create byte vectors for the embeddings.\n",
    "\n",
    "Looking into the function one can observe that each `doc` of the list of `documents` (`ListOfContents`) is processed individually. All the elements of the `doc`, that is, its `url`, `title` and `title_emb` (the embeddings of the title) are added to the index. However, a special procedure is operated on top of the `title_emb` by converting the array into bytes format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def index_documents(r, doc_prefix, documents, *vector_field):\n",
    "    for i, doc in enumerate(documents):\n",
    "        key = f\"{doc_prefix}{str(i)}\"\n",
    "        \n",
    "        for field in vector_field:\n",
    "            # create byte vectors for item\n",
    "            text_embedding = np.array(doc[field], dtype=np.float32).tobytes()\n",
    "\n",
    "             # replace list of floats with byte vectors\n",
    "            doc[field] = text_embedding\n",
    "        \n",
    "        r.hset(key, mapping=doc)\n",
    "    \n",
    "    print(\"Documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets proceed by indexing the documents. The following gathers all the code together. We begin by importing the functions `connect_redis` and `create_index` from the file `redis_service`, as the `index_documents` has already been defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as `index_name` (the name of the index), `doc_prefix` (the prefix for the document keys), `VECTOR_DIM` (which is the dimension of the embeddings of the title, that is 756 in this case), `VECTOR_NUMBER` which is the number of docs to index and `DISTANCE_METRIC` (the similarity measure to evaluate the proximity of the query to the vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that, `url`, `title` and `summary` are defined as a `TextField`, while `title_emb` as a `VectorField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis_service import connect_redis, create_index\n",
    "import numpy as np\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:wiki_dummy' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'wiki_dummy:' #prefix for the document keys\n",
    "\n",
    "# Constants\n",
    "vector_field = 'title_emb'\n",
    "VECTOR_DIM = len(ListOfContents[0][vector_field]) # length of the vectors\n",
    "VECTOR_NUMBER = len(ListOfContents)              # initial number of vectors\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# Define RediSearch fields\n",
    "url = TextField(name=\"url\")\n",
    "title = TextField(name=\"title\")\n",
    "title_emb = VectorField(vector_field,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "\n",
    "fields = [url, title, title_emb]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ListOfContents, vector_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on redis-insight: http://localhost:8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use VSS queries with the `r.ft(index_name).search` query command. To use a VSS query, you must specify the option `.dialect(2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two supported types of vector queries in Redis: `KNN` and `Range`. `Hybrid` queries can work in both settings and combine elements of traditional search and VSS. More here: https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/#querying-vector-fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN queries are for finding the `top-K` most similar vectors given a query vector. We begin by defining the `search_redis` function. The parameter `hybrid_fields` is to be used in the hybrid type queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we also need an embedding for the query. We will again simulate an embedding using the numpy module. Also note that we make use of the function `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from redis.commands.search.query import Query\n",
    "from redis_service import connect_redis\n",
    "\n",
    "def search_redis(index_name, k, vector_field, return_fields, hybrid_fields = \"*\"):\n",
    "    \n",
    "    r = connect_redis()\n",
    "    \n",
    "    #get embeddings of a hyphotetical query\n",
    "    query_emb = np.random.rand(756)\n",
    "    \n",
    "    # Prepare the Query\n",
    "    query = (\n",
    "        Query(f\"{hybrid_fields}=>[KNN {k} @{vector_field} $vector as score]\")\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "\n",
    "    query_params = {\"vector\": np.array(query_emb, dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = r.ft(index_name).search(query, query_params)\n",
    "    return results.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function `search_redis`. We ask the system to retrieve the top-`5` results, to look at the `title_emb` vector_field and to get the fields designated in the `return_fields` variable. Note that we don't have a query as we are simulating this exercise. The fake embedding of the query is defined in the previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "vector_field = \"title_emb\"\n",
    "return_fields = [\"id\", \"url\", \"title\", \"score\"]\n",
    "index_name = 'idx:wiki_dummy' #the name of the index, which you will use when doing queries\n",
    "\n",
    "results = search_redis(index_name, k, vector_field, return_fields)\n",
    "for doc in results:\n",
    "    score = 1 - float(doc.score)\n",
    "    print(f\"{doc.title}; score:{score:.3f}\")\n",
    "    print(f\"{doc.url}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that a brute-force approach to find k-nearest neighbors would calculate the distance of the new observation to every other observation. With O(n) time complexity, this is an expensive operation that could take a prohibitively long time on large datasets. Most vector stores implement a few optimizations to improve their k-NN search performance. Redis implement ANN (Approximate Nearest Neighbours): https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range queries provide a way to filter results by the distance between a vector field in Redis and a query vector based on some pre-defined threshold (radius)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will be simulating an embedding for the hypothetical query. Also note that we make use of the function `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from redis.commands.search.query import Query\n",
    "from redis_service import connect_redis\n",
    "\n",
    "def search_redis(index_name, k, vector_field, return_fields, radius):\n",
    "    r = connect_redis()\n",
    "    \n",
    "    #get embeddings of a hyphotetical query\n",
    "    query_emb = np.random.rand(756)\n",
    "    \n",
    "    # Prepare the Query\n",
    "    query = (\n",
    "        Query(f\"@{vector_field}:[VECTOR_RANGE $radius $vector]=>\"+\"{$YIELD_DISTANCE_AS: score}\")\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "\n",
    "    query_params = {\n",
    "        \"radius\": radius,\n",
    "        \"vector\": np.array(query_emb, dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = r.ft(index_name).search(query, query_params)\n",
    "    return results.docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "vector_field = \"title_emb\"\n",
    "return_fields = [\"id\", \"url\", \"title\", \"score\"]\n",
    "index_name = 'idx:wiki_dummy' #the name of the index, which you will use when doing queries\n",
    "radius = 0.8\n",
    "\n",
    "results = search_redis(index_name, k, vector_field, return_fields, radius)\n",
    "for doc in results:\n",
    "    score = 1 - float(doc.score)\n",
    "    print(f\"{doc.title}; score:{score:.3f}\")\n",
    "    print(f\"{doc.url}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid queries contain both traditional filters (numeric, tags, text) and VSS in one single Redis command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will be simulating an embedding for the hypothetical query. Also note that we make use of the function `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from redis.commands.search.query import Query\n",
    "from redis_service import connect_redis\n",
    "    \n",
    "def search_redis(index_name, k, vector_field, return_fields, hybrid_fields = \"*\"):\n",
    "    r = connect_redis()\n",
    "    \n",
    "    #get embeddings of a hyphotetical query\n",
    "    query_emb = np.random.rand(756)\n",
    "    \n",
    "    # Prepare the Query\n",
    "    query = (\n",
    "        Query(f\"{hybrid_fields}=>[KNN {k} @{vector_field} $vector as score]\")\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "\n",
    "    query_params = {\"vector\": np.array(query_emb, dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = r.ft(index_name).search(query, query_params)\n",
    "    return results.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that besides using the VSS feature, we are also restricting the results to those titles that have the word Wuhan in the title field (see hybrid_field variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "vector_field = \"title_emb\"\n",
    "return_fields = [\"id\", \"url\", \"title\", \"summary\", \"score\"]\n",
    "index_name = 'idx:wiki_dummy' #the name of the index, which you will use when doing queries\n",
    "hybrid_field = f\"@title:wuhan\"\n",
    "    \n",
    "results = search_redis(index_name, k, vector_field, return_fields, hybrid_fields = hybrid_field)\n",
    "for doc in results:\n",
    "    score = 1 - float(doc.score)\n",
    "    print(f\"{doc.title}; score:{score:.3f}\")\n",
    "    print(f\"{doc.url}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next delete all data and the index. To do that, we should import the function `drop_data` and `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_service import drop_data, connect_redis\n",
    "\n",
    "r = connect_redis()\n",
    "index_name = 'idx:wiki_dummy'\n",
    "drop_data(r, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quizzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Wikipedia (Hugging Face Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to collect some texts from wikipedia pages related to Covid. For each title and text we aim to obtain a vector of embeddings using the transformer arquitecture from Hugging Face. The core steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the texts from wikipedia\n",
    "- Load the pre-trained model and tokenizer.\n",
    "- Tokenize your text using the tokenizer.\n",
    "- Get the embeddings for your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by installing the following python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/137.6 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/137.6 kB 435.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 133.1/137.6 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 137.6/137.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.0 MB 12.2 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.9/9.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.4/9.0 MB 10.7 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.5/9.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.1/9.0 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.8/9.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.3/9.0 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.8/9.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/9.0 MB 12.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/9.0 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.6/9.0 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/9.0 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 14.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 388.9/388.9 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.9/15.8 MB 29.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.2/15.8 MB 27.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.4/15.8 MB 27.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 4.7/15.8 MB 27.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 6.1/15.8 MB 27.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 7.4/15.8 MB 28.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.3/15.8 MB 26.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.7/15.8 MB 26.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.0/15.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/15.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.1/15.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 28.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 287.3/287.3 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.5/2.2 MB 49.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 172.0/172.0 kB ? eta 0:00:00\n",
      "Installing collected packages: safetensors, numpy, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.3.1 huggingface-hub-0.22.2 numpy-1.26.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to install PyTorch. To see which instalation best fits your case, please see this webpage: https://pytorch.org/get-started/locally/. Most likely this will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.2-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "   ---------------------------------------- 0.0/198.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/198.6 MB 7.5 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 0.2/198.6 MB 2.8 MB/s eta 0:01:12\n",
      "   ---------------------------------------- 0.5/198.6 MB 3.7 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 0.8/198.6 MB 4.9 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.1/198.6 MB 5.0 MB/s eta 0:00:40\n",
      "   ---------------------------------------- 1.5/198.6 MB 5.8 MB/s eta 0:00:35\n",
      "   ---------------------------------------- 2.0/198.6 MB 6.4 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 2.4/198.6 MB 6.6 MB/s eta 0:00:30\n",
      "    --------------------------------------- 2.9/198.6 MB 7.5 MB/s eta 0:00:27\n",
      "    --------------------------------------- 3.5/198.6 MB 8.1 MB/s eta 0:00:25\n",
      "    --------------------------------------- 4.1/198.6 MB 8.3 MB/s eta 0:00:24\n",
      "    --------------------------------------- 4.8/198.6 MB 8.8 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 5.4/198.6 MB 9.3 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 6.1/198.6 MB 9.7 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 7.0/198.6 MB 10.4 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 7.9/198.6 MB 11.1 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 8.8/198.6 MB 11.6 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 9.8/198.6 MB 12.1 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 11.0/198.6 MB 14.9 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 12.2/198.6 MB 16.8 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 13.4/198.6 MB 19.3 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 14.6/198.6 MB 21.1 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 16.0/198.6 MB 23.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 17.4/198.6 MB 25.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 19.0/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 20.3/198.6 MB 28.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 22.0/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 23.5/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 25.0/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 26.4/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 28.2/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 29.5/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 31.0/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 32.5/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 34.2/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 35.4/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 37.3/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 38.3/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 40.3/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 42.0/198.6 MB 34.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 43.0/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 44.7/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 46.5/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 48.0/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 50.2/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 51.5/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 53.4/198.6 MB 38.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 55.1/198.6 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 56.8/198.6 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 58.2/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 59.6/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 61.1/198.6 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 62.3/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 63.9/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 65.3/198.6 MB 31.1 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 67.2/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 68.5/198.6 MB 32.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 70.3/198.6 MB 32.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 71.7/198.6 MB 32.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 73.3/198.6 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 74.6/198.6 MB 31.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 76.3/198.6 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 77.8/198.6 MB 32.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 80.1/198.6 MB 34.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 81.2/198.6 MB 34.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 82.7/198.6 MB 34.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 85.2/198.6 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 86.8/198.6 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 88.4/198.6 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 89.9/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 91.4/198.6 MB 36.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 93.8/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 95.4/198.6 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 97.6/198.6 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 98.9/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 100.0/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 101.8/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 103.1/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 104.4/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 106.1/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 107.8/198.6 MB 31.2 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 109.5/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 111.1/198.6 MB 32.8 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 112.4/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 114.0/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 115.9/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 117.3/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 119.2/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 121.2/198.6 MB 36.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 122.9/198.6 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 124.3/198.6 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 126.1/198.6 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 127.8/198.6 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 129.2/198.6 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 130.7/198.6 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 132.3/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 134.2/198.6 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 135.9/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 137.4/198.6 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 138.8/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 140.0/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 142.0/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 143.6/198.6 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 145.4/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 146.6/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 147.9/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 148.2/198.6 MB 29.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 151.0/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 153.1/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 154.5/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.9/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.3/198.6 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 159.9/198.6 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 161.5/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 163.1/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 164.0/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 164.7/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.0/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 168.4/198.6 MB 31.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 170.5/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 172.0/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 173.1/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 174.8/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 176.1/198.6 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.6/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.1/198.6 MB 32.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.9/198.6 MB 31.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 182.4/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 184.0/198.6 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.5/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 186.9/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 188.9/198.6 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 190.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.3/198.6 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  194.3/198.6 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.6/198.6 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.8/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 198.6/198.6 MB 19.9 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.17.2-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.2/1.2 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 37.3 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.2.2-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.2/2.4 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 30.3 MB/s eta 0:00:00\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.2/133.2 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 54.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.9/5.7 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.4/5.7 MB 30.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.7 MB 33.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 33.3 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 32.9 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.3 mpmath-1.3.0 networkx-3.3 sympy-1.12 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define the `get_embeddings_bert` which receives a text and a model name (e.g., bert) and returns the embeddings of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bert(text, tokenizer, model):\n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Get the embeddings\n",
    "    output = model(**inputs)\n",
    "\n",
    "    document_embedding = output.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that when you load a pre-trained model from the Hugging Face Transformers library using `AutoModel.from_pretrained(model_name)`, the model is loaded into memory, and it doesn't get saved to a specific file location on your local filesystem. Thus the next step is to load the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\" #gpt2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the texts from wikipedia we will need the following python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11706 sha256=e33118aeac45bf6f432e7a983a549249771c8da4ca630d1c3f75a91b7a0d2b91\n",
      "  Stored in directory: c:\\users\\jonat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\8f\\ab\\cb\\45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following code to extract the url, the title and the summary of the wikipedia texts found within the list (ListWikiPages) and the `get_embeddings_bert` function to get the embeddings for both the title and the summary. Note that bert can only accept 512 tokens (the smallest units into which a text is divided for processing by the model. These tokens are often words, subwords, or even characters). Several techniques exist to consider only 512 tokens. In this case we are using the truncate technique which discard portions of the text to fit within the token limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more tokens you may use gpt2 instead of bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covid-19\n",
      "coronavirus\n",
      "COVID-19 pandemic\n",
      "symptoms of COVID‑19\n",
      "long COVID\n",
      "COVID-19_testing\n",
      "COVID-19 vaccines\n",
      "Wuhan\n",
      "Transmission_of_COVID-19\n",
      "Variants of SARS-CoV-2\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "ListWikiPages = [\"covid-19\", \"coronavirus\", \"COVID-19 pandemic\", \"symptoms of COVID‑19\", \n",
    "                 \"long COVID\", \"COVID-19_testing\", \"COVID-19 vaccines\", \"Wuhan\", \"Transmission_of_COVID-19\",\n",
    "                \"Variants of SARS-CoV-2\"]\n",
    "\n",
    "ListOfContents = []\n",
    "for wikipage in ListWikiPages:\n",
    "    summary = wikipedia.summary(wikipage)\n",
    "    summary_emb = get_embeddings_bert(summary, tokenizer, model)\n",
    "    page = wikipedia.page(wikipage)\n",
    "    url = page.url\n",
    "    title = page.title\n",
    "    title_emb = get_embeddings_bert(title, tokenizer, model)\n",
    "    ListOfContents.append({'url':url, 'title':title, 'title_emb': title_emb, 'summary':summary, 'summary_emb':summary_emb})\n",
    "    print(wikipage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this takes some time to be computed, it might be a good idea to make a copy of the object in case something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfContents1 = ListOfContents[::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `index_documents` function, which is slightly different than the one defined on `redis_service` file, as in this case we need to create byte vectors for the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the function one can observe that each `doc` of the list of `documents` (`ListOfContents`) is processed individually. All the elements of the `doc`, that is, its `summary`, `summary_emb`, `url`, `title` and `title_emb` (the embeddings of the title) are added to the index. However, a special procedure is operated on top of the `title_emb` and of the `summary_emb` by converting the array into bytes format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(r, doc_prefix, documents, *vector_field):\n",
    "    for i, doc in enumerate(documents):\n",
    "        key = f\"{doc_prefix}{str(i)}\"\n",
    "        \n",
    "        for field in vector_field:\n",
    "            # create byte vectors for item\n",
    "            text_embedding = np.array(doc[field], dtype=np.float32).tobytes()\n",
    "\n",
    "             # replace list of floats with byte vectors\n",
    "            doc[field] = text_embedding\n",
    "        \n",
    "        r.hset(key, mapping=doc)\n",
    "    \n",
    "    print(\"Documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets proceed by indexing the documents. The following gathers all the code together. We begin by importing the functions `connect_redis` and `create_index` from the file `redis_service`, as the `index_documents` has already been defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as `index_name` (the name of the index), `doc_prefix` (the prefix for the document keys), `VECTOR_DIM` (which is the dimension of the Bert embeddings of the title, that is 512), `VECTOR_NUMBER` which is the number of docs to index and `DISTANCE_METRIC` (the similarity measure to evaluate the proximity of the query to the vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that, `url`, `title` and `summary` are defined as a `TextField`, while `title_emb` as a `VectorField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\CLASSES\\APRIL\\15_04_2024\\Notebook - Redis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'redis_service'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextField, NumericField, VectorField\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connect_redis, create_index\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'redis_service'"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis_service import connect_redis, create_index\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:wiki_hugging' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'wiki_hugging:' #prefix for the document keys\n",
    "\n",
    "# Constants\n",
    "vector_field = 'title_emb'\n",
    "vector_field1 = \"summary_emb\"\n",
    "VECTOR_DIM = len(ListOfContents[0][vector_field]) # length of the vectors\n",
    "VECTOR_NUMBER = len(ListOfContents)              # initial number of vectors\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# Define RediSearch fields\n",
    "url = TextField(name=\"url\")\n",
    "title = TextField(name=\"title\")\n",
    "summary = TextField(name=\"summary\")\n",
    "title_emb = VectorField(vector_field,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "summary_emb = VectorField(vector_field1,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "\n",
    "fields = [url, title, title_emb, summary, summary_emb]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ListOfContents, vector_field, vector_field1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on redis-insight: http://localhost:8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the docs indexed its time to search for results. We begin by defining the `search_redis` function. Note that we also need an embedding for the query. Also note that we make use of the function `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.search.query import Query\n",
    "from redis_service import connect_redis\n",
    "import numpy as np\n",
    "    \n",
    "def search_redis(query_emb, index_name, k, vector_field, return_fields, hybrid_fields = \"*\"):\n",
    "    r = connect_redis()\n",
    "       \n",
    "    # Prepare the Query\n",
    "    query = (\n",
    "        Query(f\"{hybrid_fields}=>[KNN {k} @{vector_field} $vector as score]\")\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "\n",
    "    query_params = {\"vector\": np.array(query_emb, dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = r.ft(index_name).search(query, query_params)\n",
    "    return results.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function `search_redis`. We define the query `wuhan`, ask the system to retrieve the top-`5` results, to look at the `title_emb` vector_field (the other possibility was to look at the `summary_emb`) and to get the fields designated in the `return_fields` variable (note that we are only showing the first 200 chars of the summary for a matter of convenience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "q = \"wuhan\"\n",
    "\n",
    "#get embeddings of the query\n",
    "query_emb = get_embeddings_bert(q, tokenizer, model)\n",
    "\n",
    "k = 5\n",
    "vector_field = \"title_emb\"\n",
    "return_fields = [\"id\", \"url\", \"title\", \"summary\", \"score\"]\n",
    "index_name = 'idx:wiki_hugging' #the name of the index, which you will use when doing queries\n",
    "\n",
    "results = search_redis(query_emb, index_name, k, vector_field, return_fields)\n",
    "for doc in results:\n",
    "    score = 1 - float(doc.score)\n",
    "    print(f\"{doc['title']}; score:{score:.3f}\")\n",
    "    print(doc['url'])\n",
    "    print(doc['summary'][:200])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the data and the index. To do that, we should import the function `drop_data` and `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_service import drop_data, connect_redis\n",
    "\n",
    "r = connect_redis()\n",
    "index_name = 'idx:wiki_hugging'\n",
    "drop_data(r, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Wikipedia (OpenAI Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to collect some texts from wikipedia pages related to Covid. For each title we aim to obtain a vector of embeddings using the OpenAI API, which requires you to have an API key (you will need a credit card) and to define its key in your environment. You may want to look at this [webpage](https://www.di.ubi.pt/~rcampos/LargeLanguageModels/OpenAI/1GetStarted.ipynb) to know how to get a key from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the key, you will be able to proceed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the texts from wikipedia\n",
    "- get the embeddings for your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the texts from wikipedia we will need the following python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following code to extract the url, the title and the summary of the wikipedia texts found within the list (ListWikiPages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "ListWikiPages = [\"covid-19\", \"coronavirus\", \"COVID-19 pandemic\", \"symptoms of COVID‑19\", \n",
    "                 \"long COVID\", \"COVID-19_testing\", \"COVID-19 vaccines\", \"Wuhan\", \"Transmission_of_COVID-19\",\n",
    "                \"Variants of SARS-CoV-2\"]\n",
    "                 \n",
    "ListOfContents = []\n",
    "for wikipage in ListWikiPages:\n",
    "    summary = wikipedia.summary(wikipage)\n",
    "    page = wikipedia.page(wikipage)\n",
    "    url = page.url\n",
    "    title = page.title\n",
    "    ListOfContents.append({'url':url, 'title':title, 'summary':summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by installing the `openai` python package which will enable us to get the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the embeddings we will be using the [text-embedding-ada-002](https://openai.com/blog/new-and-improved-embedding-model) model, which is the most popular embeddings model of openAI with a price of $0.0001 for 1,000 tokens and 1536 dimensions. Note that the input for api is a list of text(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embeddings_openai(ListOfTexts, model):\n",
    "    ListEmb = []\n",
    "    response = openai.Embedding.create(input=ListOfTexts, model=model)\n",
    "    for text_emb in response[\"data\"]:\n",
    "        ListEmb.append(text_emb[\"embedding\"])\n",
    "    return ListEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since executing this APIs costs some money, we will be getting embeddings just for the title texts. Note that, as referred before, we are passing for the `get_embeddings` function a list with the titles. We will then iterate over the `ListOfContents` and for each wikipedia page item, add a new field named `title_emb` with the openAI embedding for the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"text-embedding-ada-002\"\n",
    "title_emb = get_embeddings_openai([content[\"title\"] for content in ListOfContents], model)\n",
    "\n",
    "for i, content in enumerate(ListOfContents):\n",
    "    content['title_emb'] = title_emb[i]\n",
    "\n",
    "ListOfContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `index_documents` function, which is slightly different than the one defined on `redis_service` file, as in this case we need to create byte vectors for the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the function one can observe that each `doc` of the list of `documents` (`ListOfContents`) is processed individually. All the elements of the `doc`, that is, its `url`, `summary`, `title` and `title_emb` (the embeddings of the title) are added to the index. However, a special procedure is operated on top of the `title_emb` by converting the array into bytes format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def index_documents(r, doc_prefix, documents, *vector_field):\n",
    "    for i, doc in enumerate(documents):\n",
    "        key = f\"{doc_prefix}{str(i)}\"\n",
    "        \n",
    "        for field in vector_field:\n",
    "            # create byte vectors for item\n",
    "            text_embedding = np.array(doc[field], dtype=np.float32).tobytes()\n",
    "\n",
    "             # replace list of floats with byte vectors\n",
    "            doc[field] = text_embedding\n",
    "        \n",
    "        r.hset(key, mapping=doc)\n",
    "    \n",
    "    print(\"Documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets proceed by indexing the documents. The following gathers all the code together. We begin by importing the functions `connect_redis` and `create_index` from the file `redis_service`, as the `index_documents` has already been defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as `index_name` (the name of the index), `doc_prefix` (the prefix for the document keys), `VECTOR_DIM` (which is the dimension of the openAI embeddings of the title, that is 1536), `VECTOR_NUMBER` which is the number of docs to index and `DISTANCE_METRIC` (the similarity measure to evaluate the proximity of the query to the vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that, `url`, `title` and `summary` are defined as a `TextField`, while `title_emb` as a `VectorField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis_service import connect_redis, create_index\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:wiki_openai' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'wiki_openai:' #prefix for the document keys\n",
    "\n",
    "# Constants\n",
    "vector_field = 'title_emb'\n",
    "VECTOR_DIM = len(ListOfContents[0][vector_field]) # length of the vectors\n",
    "VECTOR_NUMBER = len(ListOfContents)              # initial number of vectors\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# Define RediSearch fields\n",
    "url = TextField(name=\"url\")\n",
    "title = TextField(name=\"title\")\n",
    "summary = TextField(name=\"summary\")\n",
    "title_emb = VectorField(vector_field,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "\n",
    "fields = [url, title, title_emb, summary]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ListOfContents, vector_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on redis-insight: http://localhost:8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the docs indexed its time to search for results. We begin by defining the `search_redis` function. Also note that we make use of the function `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the query `q` we need to get its embedding by passing it as a list (that is the reason why we have `[q]`). From the retrieved results of the openAI API we then want to keep only the embedding (that is reason to have `[0][\"embedding\"]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.search.query import Query\n",
    "from redis_service import connect_redis\n",
    "import numpy as np\n",
    "\n",
    "def search_redis(query_emb, index_name, k, vector_field, return_fields, hybrid_fields = \"*\"):\n",
    "    r = connect_redis()\n",
    "\n",
    "    # Prepare the Query\n",
    "    query = (\n",
    "        Query(f\"{hybrid_fields}=>[KNN {k} @{vector_field} $vector as score]\")\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "\n",
    "    query_params = {\"vector\": np.array(query_emb, dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = r.ft(index_name).search(query, query_params)\n",
    "    return results.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function `search_redis`. We define the query `wuhan`, ask the system to retrieve the top-`5` results, to look at the `title_emb` vector_field and to get the fields designated in the `return_fields` variable (note that we are only showing the first 50 chars of the summary for a matter of convenience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "q = \"wuhan\"\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "#get embeddings of the query\n",
    "query_emb = get_embeddings_openai([q], model)[0]\n",
    "\n",
    "k = 5\n",
    "vector_field = \"title_emb\"\n",
    "return_fields = [\"id\", \"url\", \"title\", \"summary\", \"score\"]\n",
    "index_name = 'idx:wiki_openai' #the name of the index, which you will use when doing queries\n",
    "\n",
    "results = search_redis(query_emb, index_name, k, vector_field, return_fields)\n",
    "for doc in results:\n",
    "    score = 1 - float(doc.score)\n",
    "    print(f\"{doc.title}; score:{score:.3f}\")\n",
    "    print(f\"{doc.url}\")\n",
    "    print(f\"{doc.summary[:50]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the data and the index. To do that, we should import the function `drop_data` and `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_service import drop_data, connect_redis\n",
    "\n",
    "r = connect_redis()\n",
    "index_name = 'idx:_openai'\n",
    "drop_data(r, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Publico (Hugging Face Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we aim to make use of the [TLS-Covid19](https://github.com/LIAAD/tls-covid19) dataset which has the key moments of the [liveblog](https://www.publico.pt/coronavirus-ao-minuto) of the Jornal Público dedicated to Covid. For each title and text (named news) we aim to obtain a vector of embeddings using the bert architecture from Hugging Face. In particular, we will be using embeddings for the Portuguese languages. The core steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the texts from TLS-Covid19\n",
    "- Load the pre-trained model and tokenizer.\n",
    "- Tokenize your text using the tokenizer.\n",
    "- Get the embeddings for your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by installing the following python package (you might have installed it before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to install PyTorch (again, you might have installed it during the previous exercise). To see which installation best fits your case, please see this webpage: https://pytorch.org/get-started/locally/. Most likely this will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will again define the `get_embeddings_bert` (which will get the `bertimbau` model: `neuralmind/bert-base-Portuguese-cased` and the `Albertina` model: `PORTULAN/albertina-ptpt`) to experiment returning the embeddings of the document with two different Portuguese models based on the `BERT` architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that BERT architectures can only accept 512 tokens (the smallest units into which a text is divided for processing by the model. These tokens are often words, subwords, or even characters). Since the size of the text can be too long, document splitting is required to split documents into smaller chunks (before it goes into the vector store). Several techniques exist to consider only 512 tokens. For example, you can take the first 512 tokens of the text and disregard the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason we add the parameters `max_length` and `truncation`:\n",
    "- `max_length=512`: This parameter specifies the maximum length of the tokenized sequences. If a sequence exceeds this length after tokenization, it will be truncated to fit within the specified maximum length. BERT models, for example, typically have a maximum token length of 512 tokens.\n",
    "- `truncation=True`: When set to True, this parameter indicates that truncation should be applied to sequences that exceed the maximum length (max_length). Truncation involves removing tokens from the end of the sequence to ensure that it does not exceed the maximum length. This is necessary to prevent sequences from being too long for the model to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bert(text, tokenizer, model):\n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Get the embeddings\n",
    "    output = model(**inputs)\n",
    "\n",
    "    document_embedding = output.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that when you load a pre-trained model from the Hugging Face Transformers library using `AutoModel.from_pretrained(model_name)`, the model is loaded into memory, and it doesn't get saved to a specific file location on your local filesystem. Thus the next step is to load the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\" #PORTULAN/albertina-ptpt\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the following code to load the TLS-Covid19 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/publico_km.json\", \"r\",encoding=\"utf8\") as readfile:\n",
    "    KeyMoments = json.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'publico', 'lan': 'pt', 'topic': 'açores', 'date': '2020-03-16', 'title': 'Casos confirmados em Portugal: 331, mais 86 do que ontem', 'news': 'Há 331 casos confirmados de infecção pelo novo coronavírus em Portugal, anunciou a Direcção-Geral da Saúde no boletim epidemiológico desta segunda-feira. São mais 86 casos do que no domingo. Há 374 casos que aguardam resultado laboratorial e três pessoas já recuperaram. Ao todo, desde 1 de Janeiro de 2020, houve 2908 casos suspeitos, sendo que 2203 não se confirmaram. Há 4592 pessoas em vigilância pelas autoridades. A região de Lisboa e Vale do Tejo regista 142 casos e o Norte tem 138 casos de infecção. No centro há 31 casos, no Algarve são 13 e nos Açores há um caso registado. Há ainda cinco pessoas infectadas com residência no estrangeiro. Dos casos confirmados, 139 estão em internamento e 18 deles em cuidados intensivos. Há 77 pessoas infectadas que têm mais de 70 anos. A maior parte das pessoas infectadas apresentava tosse (53%), e havia também quem tivesse febre (31%), cefaleia (19%), dores musculares (18%) e fraqueza generalizada (13%). Só em 9% dos casos houve dificuldades a respirar. Há 18 cadeias de transmissão activas, com casos importados sobretudo de Espanha (16), Itália (14), França (nove) e Suíça (cinco); os outros casos importados são de Andorra, Bélgica, Alemanha e Áustria.', 'url': 'https://www.publico.pt/2020/03/16/sociedade/noticia/coronavirus-portugal-meio-gas-europa-comeca-fechar-portas-1907902#36451'}]\n"
     ]
    }
   ],
   "source": [
    "print(KeyMoments[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will consider all the fields that exist in the `KeyMoments` structure and the `get_embeddings_bert` function to get the embeddings for both the title and the text. Also note that, we will only consider doing this for the top-20 results of `KeyMoments` as creating embeddings for more than 8k records is a time-consuming process. For this reason, we will create the variable `ListOfContents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfContents = KeyMoments[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code: `ListOfContents = KeyMoments[::]`, in case you want to consider the full dataset and yet preserve the contents of the `KeyMoments` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%"
     ]
    }
   ],
   "source": [
    "#Insert your code here to get the `bertimbau` embeddings for the title and for the news\n",
    "\n",
    "for i in range(len(ListOfContents)):\n",
    "    ListOfContents[i]['title_emb'] = get_embeddings_bert(ListOfContents[i]['title'], tokenizer, model)\n",
    "    ListOfContents[i]['news_emb'] = get_embeddings_bert(ListOfContents[i]['news'], tokenizer, model)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "            print(f\"\\r{100*i/len(ListOfContents):.2f}%\", end='')\n",
    "    elif i == len(ListOfContents) - 1:\n",
    "        print(f\"\\r100.00%\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "for i in range(len(ListOfContents)):\n",
    "    ListOfContents[i]['title_emb'] = get_embeddings_bert(ListOfContents[i]['title'], tokenizer, model)\n",
    "    ListOfContents[i]['news_emb'] = get_embeddings_bert(ListOfContents[i]['news'], tokenizer, model)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"\\r{100*i/len(ListOfContents):.2f}%\", end='')\n",
    "    elif i == len(ListOfContents) - 1:\n",
    "        print(f\"\\r100.00%\", end='')\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `index_documents` function, which is slightly different than the one defined on `redis_service` file, as in this case we need to create byte vectors for the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the function one can observe that each `doc` of the list of `documents` (`ListOfContents`) is processed individually. All the elements of the `doc`, that is, its `source`, `lan`, `topic`, `date` and `url` are added to the index. However, a special procedure is operated on top of the `title_emb` and of the `news_emb` by converting the array into bytes format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert your index_documents function here\n",
    "\n",
    "def index_documents(r, doc_prefix, documents, *vector_fields):\n",
    "    for i, doc in enumerate(documents):\n",
    "        key = f\"{doc_prefix}{str(i)}\"\n",
    "        \n",
    "        for field in vector_field:\n",
    "            # create byte vectors for item\n",
    "            text_embedding = np.array(doc[field], dtype=np.float32).tobytes()\n",
    "\n",
    "             # replace list of floats with byte vectors\n",
    "            doc[field] = text_embedding\n",
    "        \n",
    "        r.hset(key, mapping=doc)\n",
    "    \n",
    "    print(\"Documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "def index_documents(r, doc_prefix, documents, *vector_field):\n",
    "    for i, doc in enumerate(documents):\n",
    "        key = f\"{doc_prefix}{str(i)}\"\n",
    "        \n",
    "        for field in vector_field:\n",
    "            # create byte vectors for item\n",
    "            text_embedding = np.array(doc[field], dtype=np.float32).tobytes()\n",
    "\n",
    "             # replace list of floats with byte vectors\n",
    "            doc[field] = text_embedding\n",
    "        \n",
    "        r.hset(key, mapping=doc)\n",
    "    \n",
    "    print(\"Documents indexed\")\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets proceed by indexing the documents. The following gathers all the code together. We begin by importing the functions `connect_redis` and `create_index` from the file `redis_service`, as the `index_documents` has already been defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some variables such as `index_name` (the name of the index), `doc_prefix` (the prefix for the document keys), `VECTOR_DIM` (which is the dimension of the Bert embeddings of the title, that is 512), `VECTOR_NUMBER` which is the number of docs to index and `DISTANCE_METRIC` (the similarity measure to evaluate the proximity of the query to the vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that, `url`, `title` and `summary` are defined as a `TextField`, while `title_emb` as a `VectorField`. Finally `fields` specifies the set of fields to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jonat\\documents\\dev\\mestrado\\msc-ipt-big-data\\.venv\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "unknown command 'FT.CREATE', with args beginning with: 'idx:publico_hugging' 'PREFIX' '1' 'publico_hugging:' 'SCORE' '1.0' 'SCHEMA' 'source' 'TEXT' 'WEIGHT' '1.0' 'lan' 'TEXT' 'WEIGHT' ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\CLASSES\\APRIL\\22_04_2024\\redis_service.py:18\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(r, index_name, doc_prefix, fields)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#  check if index exists\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex already exists!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\commands\\search\\commands.py:450\u001b[0m, in \u001b[0;36mSearchCommands.info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03mGet info an stats about the the current index, including the number of\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03mdocuments, memory consumption, etc\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03mFor more information see `FT.INFO <https://redis.io/commands/ft.info>`_.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFO_CMD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_results(INFO_CMD, res)\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:543\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[1;34m(self, *args, **options)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_disconnect_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\retry.py:46\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[1;34m(self, do, fail)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:544\u001b[0m, in \u001b[0;36mRedis.execute_command.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m--> 544\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[0;32m    548\u001b[0m     )\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:520\u001b[0m, in \u001b[0;36mRedis._send_command_parse_response\u001b[1;34m(self, conn, command_name, *args, **options)\u001b[0m\n\u001b[0;32m    519\u001b[0m conn\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:560\u001b[0m, in \u001b[0;36mRedis.parse_response\u001b[1;34m(self, connection, command_name, **options)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:536\u001b[0m, in \u001b[0;36mAbstractConnection.read_response\u001b[1;34m(self, disable_decoding, disconnect_on_error, push_request)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m response\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mResponseError\u001b[0m: unknown command 'FT.INFO', with args beginning with: 'idx:publico_hugging' ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m fields \u001b[38;5;241m=\u001b[39m [source, lan, topic, date, title, news, title_emb, news_emb]\n\u001b[0;32m     43\u001b[0m r \u001b[38;5;241m=\u001b[39m connect_redis()\n\u001b[1;32m---> 44\u001b[0m \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# index_documents(r, doc_prefix, ListOfContents, vector_field, vector_field1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\CLASSES\\APRIL\\22_04_2024\\redis_service.py:22\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(r, index_name, doc_prefix, fields)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex already exists!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#create index\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefinition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIndexDefinition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc_prefix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\commands\\search\\commands.py:221\u001b[0m, in \u001b[0;36mSearchCommands.create_index\u001b[1;34m(self, fields, no_term_offsets, no_field_flags, stopwords, definition, max_text_fields, temporary, no_highlight, no_term_frequencies, skip_initial_scan)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m fields\u001b[38;5;241m.\u001b[39mredis_args()\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:543\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[1;34m(self, *args, **options)\u001b[0m\n\u001b[0;32m    540\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_disconnect_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\retry.py:46\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[1;34m(self, do, fail)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     48\u001b[0m         failures \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:544\u001b[0m, in \u001b[0;36mRedis.execute_command.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    540\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m--> 544\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[0;32m    548\u001b[0m     )\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:520\u001b[0m, in \u001b[0;36mRedis._send_command_parse_response\u001b[1;34m(self, conn, command_name, *args, **options)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03mSend a command and parse the response\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    519\u001b[0m conn\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\client.py:560\u001b[0m, in \u001b[0;36mRedis.parse_response\u001b[1;34m(self, connection, command_name, **options)\u001b[0m\n\u001b[0;32m    558\u001b[0m         options\u001b[38;5;241m.\u001b[39mpop(NEVER_DECODE)\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m EMPTY_RESPONSE \u001b[38;5;129;01min\u001b[39;00m options:\n",
      "File \u001b[1;32mc:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\redis\\connection.py:536\u001b[0m, in \u001b[0;36mAbstractConnection.read_response\u001b[1;34m(self, disable_decoding, disconnect_on_error, push_request)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ResponseError):\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m response\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m response  \u001b[38;5;66;03m# avoid creating ref cycles\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: unknown command 'FT.CREATE', with args beginning with: 'idx:publico_hugging' 'PREFIX' '1' 'publico_hugging:' 'SCORE' '1.0' 'SCHEMA' 'source' 'TEXT' 'WEIGHT' '1.0' 'lan' 'TEXT' 'WEIGHT' "
     ]
    }
   ],
   "source": [
    "#Insert here all theimport redis\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis_service import connect_redis, create_index\n",
    "import json\n",
    "import numpy as np \n",
    "\n",
    "index_name = 'idx:publico_hugging' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'publico_hugging:' #prefix for the document keys\n",
    "\n",
    "# Constants\n",
    "vector_field = 'title_emb'\n",
    "vector_field1 = \"news_emb\"\n",
    "VECTOR_DIM = len(ListOfContents[0][vector_field]) # length of the vectors\n",
    "VECTOR_NUMBER = len(ListOfContents)              # initial number of vectors\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# Define RediSearch fields\n",
    "source = TextField(name=\"source\")\n",
    "lan = TextField(name=\"lan\")\n",
    "topic = TextField(name=\"topic\")\n",
    "date = TextField(name=\"date\")\n",
    "title = TextField(name=\"title\")\n",
    "news = TextField(name=\"news\")\n",
    "title_emb = VectorField(vector_field,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "news_emb = VectorField(vector_field1,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "\n",
    "fields = [source, lan, topic, date, title, news, title_emb, news_emb]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ListOfContents, vector_field, vector_field1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "import redis\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis_service import connect_redis, create_index\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#Define variables\n",
    "index_name = 'idx:publico_hugging' #the name of the index, which you will use when doing queries\n",
    "doc_prefix = 'publico_hugging:' #prefix for the document keys\n",
    "\n",
    "# Constants\n",
    "vector_field = 'title_emb'\n",
    "vector_field1 = \"news_emb\"\n",
    "VECTOR_DIM = len(ListOfContents[0][vector_field]) # length of the vectors\n",
    "VECTOR_NUMBER = len(ListOfContents)              # initial number of vectors\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# Define RediSearch fields\n",
    "source = TextField(name=\"source\")\n",
    "lan = TextField(name=\"lan\")\n",
    "topic = TextField(name=\"topic\")\n",
    "date = TextField(name=\"date\")\n",
    "title = TextField(name=\"title\")\n",
    "news = TextField(name=\"news\")\n",
    "title_emb = VectorField(vector_field,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "news_emb = VectorField(vector_field1,\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "\n",
    "fields = [source, lan, topic, date, title, news, title_emb, news_emb]\n",
    "\n",
    "r = connect_redis()\n",
    "create_index(r, index_name, doc_prefix, fields)\n",
    "index_documents(r, doc_prefix, ListOfContents, vector_field, vector_field1)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on redis-insight: http://localhost:8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the docs indexed its time to search for results. We begin by defining the `search_redis` function. Note that we also need an embedding for the query. Also note that we make use of the function `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert here the search_redis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "from redis.commands.search.query import Query\n",
    "from redis_service import connect_redis\n",
    "import numpy as np\n",
    "    \n",
    "def search_redis(query_emb, index_name, k, vector_field, return_fields, hybrid_fields = \"*\"):\n",
    "    r = connect_redis()\n",
    "       \n",
    "    # Prepare the Query\n",
    "    query = (\n",
    "        Query(f\"{hybrid_fields}=>[KNN {k} @{vector_field} $vector as score]\")\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "\n",
    "    query_params = {\"vector\": np.array(query_emb, dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = r.ft(index_name).search(query, query_params)\n",
    "    return results.docs\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the function `search_redis`. We define the query `estado de emergência`, ask the system to retrieve the top-`5` results, to look at the `title_emb` vector_field (the other possibility was to look at the `news_emb`) and to get the fields designated in the `return_fields` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert here your query and the call to the search_redis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "#variables\n",
    "q = \"estado de emergência\"\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\" #PORTULAN/albertina-ptpt\n",
    "\n",
    "#get embeddings of the query\n",
    "query_emb = get_embeddings_bert(q, tokenizer, model)\n",
    "\n",
    "k = 5\n",
    "vector_field = \"title_emb\"\n",
    "return_fields = [\"source\", \"lan\", \"topic\", \"date\", \"title\", \"news\", \"url\", \"score\"]\n",
    "index_name = 'idx:publico_hugging' #the name of the index, which you will use when doing queries\n",
    "\n",
    "results = search_redis(query_emb, index_name, k, vector_field, return_fields)\n",
    "for doc in results:\n",
    "    score = 1 - float(doc.score)\n",
    "    print(f\"{doc['title']}; score:{score:.3f}\")\n",
    "    print(doc['news'])\n",
    "    print(doc['date'])\n",
    "    print(doc['url'])\n",
    "    print(\"\\n\")\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the data and the index. To do that, we should import the function `drop_data` and `connect_redis` from the `redis_service` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "from redis_service import drop_data, connect_redis\n",
    "\n",
    "r = connect_redis()\n",
    "index_name = 'idx:publico_hugging'\n",
    "drop_data(r, index_name)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2022/06/introduction-to-redis-using-python/\n",
    "- https://betterprogramming.pub/getting-started-with-redis-a-python-tutorial-3a18531a73a6\n",
    "- https://cookbook.openai.com/examples/vector_databases/redis/getting-started-with-redis-and-openai\n",
    "- https://redis-py.readthedocs.io/en/stable/examples/search_vector_similarity_examples.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.733px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
