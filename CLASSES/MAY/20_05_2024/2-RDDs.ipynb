{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Big Data</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Spark</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">2 - RDDs</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><a href=\"3-Spark-RDDs.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Instructions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Windows\" data-toc-modified-id=\"Windows-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Windows</a></span></li><li><span><a href=\"#Google-Colab\" data-toc-modified-id=\"Google-Colab-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Google Colab</a></span></li></ul></li><li><span><a href=\"#Setup-the-SparkContext\" data-toc-modified-id=\"Setup-the-SparkContext-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup the SparkContext</a></span></li><li><span><a href=\"#RDDs\" data-toc-modified-id=\"RDDs-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RDDs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-RDDs\" data-toc-modified-id=\"Create-RDDs-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create RDDs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parallelized-Collections\" data-toc-modified-id=\"Parallelized-Collections-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Parallelized Collections</a></span></li><li><span><a href=\"#From-Text-Files\" data-toc-modified-id=\"From-Text-Files-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>From Text Files</a></span></li></ul></li><li><span><a href=\"#Transformations\" data-toc-modified-id=\"Transformations-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Map\" data-toc-modified-id=\"Map-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Map</a></span></li><li><span><a href=\"#FlatMap\" data-toc-modified-id=\"FlatMap-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>FlatMap</a></span></li><li><span><a href=\"#Filter\" data-toc-modified-id=\"Filter-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Filter</a></span></li><li><span><a href=\"#Distinct\" data-toc-modified-id=\"Distinct-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Distinct</a></span></li></ul></li><li><span><a href=\"#Actions\" data-toc-modified-id=\"Actions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Actions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collect\" data-toc-modified-id=\"Collect-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Collect</a></span></li><li><span><a href=\"#Take\" data-toc-modified-id=\"Take-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Take</a></span></li><li><span><a href=\"#Count\" data-toc-modified-id=\"Count-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Count</a></span></li><li><span><a href=\"#CountByValue\" data-toc-modified-id=\"CountByValue-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>CountByValue</a></span></li><li><span><a href=\"#Reduce\" data-toc-modified-id=\"Reduce-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Reduce</a></span></li></ul></li></ul></li><li><span><a href=\"#Pair-RDDs\" data-toc-modified-id=\"Pair-RDDs-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pair RDDs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Pair-RDDs\" data-toc-modified-id=\"Create-Pair-RDDs-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create Pair RDDs</a></span></li><li><span><a href=\"#Transformations\" data-toc-modified-id=\"Transformations-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#ReduceByKey\" data-toc-modified-id=\"ReduceByKey-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>ReduceByKey</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-4.2.1.1\"><span class=\"toc-item-num\">4.2.1.1&nbsp;&nbsp;</span>Word Count</a></span></li><li><span><a href=\"#IP-Address-Count\" data-toc-modified-id=\"IP-Address-Count-4.2.1.2\"><span class=\"toc-item-num\">4.2.1.2&nbsp;&nbsp;</span>IP Address Count</a></span></li><li><span><a href=\"#Read-2-Files\" data-toc-modified-id=\"Read-2-Files-4.2.1.3\"><span class=\"toc-item-num\">4.2.1.3&nbsp;&nbsp;</span>Read 2 Files</a></span></li><li><span><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-4.2.1.4\"><span class=\"toc-item-num\">4.2.1.4&nbsp;&nbsp;</span>Inverted Index</a></span></li><li><span><a href=\"#Read-from-Web\" data-toc-modified-id=\"Read-from-Web-4.2.1.5\"><span class=\"toc-item-num\">4.2.1.5&nbsp;&nbsp;</span>Read from Web</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Caching-/-Persistence\" data-toc-modified-id=\"Caching-/-Persistence-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Caching / Persistence</a></span></li><li><span><a href=\"#Passing-Functions-to-Spark\" data-toc-modified-id=\"Passing-Functions-to-Spark-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Passing Functions to Spark</a></span></li><li><span><a href=\"#Save-Files\" data-toc-modified-id=\"Save-Files-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Save Files</a></span></li><li><span><a href=\"#Quiz\" data-toc-modified-id=\"Quiz-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Quiz</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá saber o que são RDDs.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should know what are RDDs.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### RDDs<a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos ao conceito de RDDs\n",
    "- criação de RDDs e de pair rdds (key/value pairs)\n",
    "- transformações\n",
    "- acções\n",
    "- cache\n",
    "- passar funções para o spark\n",
    "- guardar ficheiros\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### RDDs <a class=\"tocSkip\">\n",
    "Introducing students to RDDs\n",
    "- creation RDDs and pair RDDs (key/value pairs)\n",
    "- transformations\n",
    "- actions\n",
    "- cache\n",
    "- passing functions to spark\n",
    "- save files\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique no menu `Settings` do Jupyter notebook lab. De seguida: `settings editor`. Na nova aba aberta pesquise por `table of contents` e altere o `Maximal headings depth` para 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ainda  não o fez, comece por fazer download da pasta [data](https://www.di.ubi.pt/~rcampos/assets/files_tutorials/BigData/data.zip). De seguida extraia o conteúdo dessa pasta para uma pasta com o nome `data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução deste notebook pressupõe a existência da pasta `data`  (e dos seus conteúdos) junto do seu notebook. Necessita também de ter o docker e o pyspark a funcionar de acordo com o explicado no notebook `1 - Introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ainda não o fez, entre agora no [Google Drive](https://drive.google.com/drive/my-drive) e crie a pasta `\\Colab Notebooks\\data\\BigData\\`. De seguida, copie os conteúdos da pasta `data` existentes no seu pc para dentro da pasta `\\Colab Notebooks\\data\\BigData\\`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Google Colab, execute o seguinte comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run pyspark on a local machine we need Java and other software. So instead of the heavy installation procedure, we use Google Colaboratory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "     ---------------------------------------- 0.0/317.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/317.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/317.0 MB 1.3 MB/s eta 0:04:02\n",
      "     ---------------------------------------- 0.6/317.0 MB 5.1 MB/s eta 0:01:03\n",
      "     ---------------------------------------- 1.5/317.0 MB 8.8 MB/s eta 0:00:36\n",
      "     --------------------------------------- 3.2/317.0 MB 14.6 MB/s eta 0:00:22\n",
      "      -------------------------------------- 4.9/317.0 MB 18.3 MB/s eta 0:00:18\n",
      "      -------------------------------------- 6.2/317.0 MB 19.8 MB/s eta 0:00:16\n",
      "      -------------------------------------- 8.0/317.0 MB 22.4 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 9.6/317.0 MB 23.6 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 11.4/317.0 MB 34.4 MB/s eta 0:00:09\n",
      "     - ------------------------------------ 13.0/317.0 MB 36.4 MB/s eta 0:00:09\n",
      "     - ------------------------------------ 15.0/317.0 MB 36.3 MB/s eta 0:00:09\n",
      "     -- ----------------------------------- 16.8/317.0 MB 38.6 MB/s eta 0:00:08\n",
      "     -- ----------------------------------- 18.4/317.0 MB 40.9 MB/s eta 0:00:08\n",
      "     -- ----------------------------------- 19.7/317.0 MB 34.4 MB/s eta 0:00:09\n",
      "     -- ----------------------------------- 21.9/317.0 MB 36.4 MB/s eta 0:00:09\n",
      "     -- ----------------------------------- 23.6/317.0 MB 38.6 MB/s eta 0:00:08\n",
      "     --- ---------------------------------- 25.5/317.0 MB 38.6 MB/s eta 0:00:08\n",
      "     --- ---------------------------------- 27.0/317.0 MB 36.3 MB/s eta 0:00:08\n",
      "     --- ---------------------------------- 28.9/317.0 MB 36.3 MB/s eta 0:00:08\n",
      "     --- ---------------------------------- 30.8/317.0 MB 36.4 MB/s eta 0:00:08\n",
      "     --- ---------------------------------- 32.8/317.0 MB 38.6 MB/s eta 0:00:08\n",
      "     ---- --------------------------------- 35.3/317.0 MB 40.9 MB/s eta 0:00:07\n",
      "     ---- --------------------------------- 36.2/317.0 MB 38.5 MB/s eta 0:00:08\n",
      "     ---- --------------------------------- 37.9/317.0 MB 36.4 MB/s eta 0:00:08\n",
      "     ---- --------------------------------- 39.9/317.0 MB 38.5 MB/s eta 0:00:08\n",
      "     ---- --------------------------------- 41.5/317.0 MB 36.3 MB/s eta 0:00:08\n",
      "     ----- -------------------------------- 43.8/317.0 MB 38.5 MB/s eta 0:00:08\n",
      "     ----- -------------------------------- 45.3/317.0 MB 38.5 MB/s eta 0:00:08\n",
      "     ----- -------------------------------- 47.3/317.0 MB 40.9 MB/s eta 0:00:07\n",
      "     ----- -------------------------------- 48.7/317.0 MB 40.9 MB/s eta 0:00:07\n",
      "     ------ ------------------------------- 51.1/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     ------ ------------------------------- 53.3/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     ------ ------------------------------- 54.9/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     ------ ------------------------------- 57.0/317.0 MB 40.9 MB/s eta 0:00:07\n",
      "     ------- ------------------------------ 58.6/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     ------- ------------------------------ 60.5/317.0 MB 40.9 MB/s eta 0:00:07\n",
      "     ------- ------------------------------ 61.9/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     ------- ------------------------------ 63.4/317.0 MB 36.3 MB/s eta 0:00:07\n",
      "     ------- ------------------------------ 65.1/317.0 MB 34.6 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 67.1/317.0 MB 36.4 MB/s eta 0:00:07\n",
      "     -------- ----------------------------- 68.8/317.0 MB 36.4 MB/s eta 0:00:07\n",
      "     -------- ----------------------------- 71.0/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     -------- ----------------------------- 72.8/317.0 MB 40.9 MB/s eta 0:00:06\n",
      "     --------- ---------------------------- 75.1/317.0 MB 43.5 MB/s eta 0:00:06\n",
      "     --------- ---------------------------- 75.6/317.0 MB 38.5 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 77.3/317.0 MB 36.3 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 79.1/317.0 MB 38.6 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 81.4/317.0 MB 38.6 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 83.6/317.0 MB 40.9 MB/s eta 0:00:06\n",
      "     ---------- --------------------------- 84.9/317.0 MB 36.4 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 86.7/317.0 MB 40.9 MB/s eta 0:00:06\n",
      "     ---------- --------------------------- 88.5/317.0 MB 40.9 MB/s eta 0:00:06\n",
      "     ---------- --------------------------- 89.8/317.0 MB 38.5 MB/s eta 0:00:06\n",
      "     ----------- -------------------------- 92.4/317.0 MB 38.5 MB/s eta 0:00:06\n",
      "     ----------- -------------------------- 93.1/317.0 MB 32.8 MB/s eta 0:00:07\n",
      "     ----------- -------------------------- 94.3/317.0 MB 32.8 MB/s eta 0:00:07\n",
      "     ----------- -------------------------- 96.3/317.0 MB 34.4 MB/s eta 0:00:07\n",
      "     ----------- -------------------------- 98.6/317.0 MB 36.4 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 100.6/317.0 MB 38.5 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 103.2/317.0 MB 40.9 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 105.5/317.0 MB 50.4 MB/s eta 0:00:05\n",
      "     ------------ ------------------------ 106.7/317.0 MB 43.5 MB/s eta 0:00:05\n",
      "     ------------ ------------------------ 108.4/317.0 MB 43.7 MB/s eta 0:00:05\n",
      "     ------------ ------------------------ 108.6/317.0 MB 38.5 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 108.9/317.0 MB 31.2 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 109.9/317.0 MB 28.5 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 111.6/317.0 MB 28.5 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 111.7/317.0 MB 27.3 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 113.0/317.0 MB 24.2 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 116.3/317.0 MB 25.1 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 118.0/317.0 MB 25.2 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 119.7/317.0 MB 34.4 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 121.9/317.0 MB 36.4 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 122.8/317.0 MB 43.7 MB/s eta 0:00:05\n",
      "     -------------- ---------------------- 124.7/317.0 MB 38.5 MB/s eta 0:00:05\n",
      "     -------------- ---------------------- 126.2/317.0 MB 34.4 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 128.4/317.0 MB 36.3 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 130.2/317.0 MB 36.3 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 132.1/317.0 MB 36.4 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 134.5/317.0 MB 40.9 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 135.2/317.0 MB 40.9 MB/s eta 0:00:05\n",
      "     ---------------- -------------------- 137.5/317.0 MB 40.9 MB/s eta 0:00:05\n",
      "     ---------------- -------------------- 138.7/317.0 MB 38.6 MB/s eta 0:00:05\n",
      "     ---------------- -------------------- 140.4/317.0 MB 38.6 MB/s eta 0:00:05\n",
      "     ---------------- -------------------- 141.9/317.0 MB 36.3 MB/s eta 0:00:05\n",
      "     ---------------- -------------------- 144.1/317.0 MB 34.4 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 146.5/317.0 MB 38.6 MB/s eta 0:00:05\n",
      "     ----------------- ------------------- 148.1/317.0 MB 40.9 MB/s eta 0:00:05\n",
      "     ----------------- ------------------- 150.7/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 151.9/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 153.8/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 155.2/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 157.0/317.0 MB 38.5 MB/s eta 0:00:05\n",
      "     ------------------ ------------------ 158.6/317.0 MB 38.5 MB/s eta 0:00:05\n",
      "     ------------------ ------------------ 160.2/317.0 MB 36.4 MB/s eta 0:00:05\n",
      "     ------------------ ------------------ 162.2/317.0 MB 36.3 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 163.7/317.0 MB 36.3 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 165.8/317.0 MB 38.6 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 168.1/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 170.2/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 171.7/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 173.6/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 175.0/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 177.1/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 179.6/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 181.2/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 183.7/317.0 MB 43.7 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 185.1/317.0 MB 40.9 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 186.9/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 188.5/317.0 MB 38.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 189.8/317.0 MB 36.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 192.7/317.0 MB 38.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 194.2/317.0 MB 36.3 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 195.0/317.0 MB 34.6 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 197.9/317.0 MB 38.6 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 200.1/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 201.8/317.0 MB 40.9 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 203.1/317.0 MB 38.6 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 205.1/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 207.0/317.0 MB 40.9 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 209.0/317.0 MB 38.5 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 210.9/317.0 MB 40.9 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 213.2/317.0 MB 40.9 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 215.7/317.0 MB 46.7 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 217.3/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 219.0/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 221.1/317.0 MB 43.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 223.9/317.0 MB 43.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 225.4/317.0 MB 43.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 227.9/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 229.2/317.0 MB 43.7 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 231.8/317.0 MB 46.7 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 233.5/317.0 MB 40.9 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 235.6/317.0 MB 43.7 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 238.0/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 239.7/317.0 MB 43.5 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 242.4/317.0 MB 50.4 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 243.3/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 245.1/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 246.4/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 246.4/317.0 MB 36.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 250.6/317.0 MB 38.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 252.4/317.0 MB 36.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 254.3/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 255.1/317.0 MB 36.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 255.8/317.0 MB 31.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 258.9/317.0 MB 38.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 260.1/317.0 MB 34.4 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 261.9/317.0 MB 34.4 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 264.3/317.0 MB 34.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 266.3/317.0 MB 46.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 267.7/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 269.7/317.0 MB 38.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 271.7/317.0 MB 40.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 274.1/317.0 MB 43.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 275.8/317.0 MB 43.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 277.5/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 279.3/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 281.5/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 283.6/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 284.8/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 287.6/317.0 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 289.0/317.0 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 291.1/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 292.9/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 294.8/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 297.3/317.0 MB 43.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 298.9/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 301.2/317.0 MB 43.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 303.2/317.0 MB 43.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 304.4/317.0 MB 38.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 306.8/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 308.0/317.0 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  310.5/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  312.1/317.0 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  313.4/317.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  315.3/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- 317.0/317.0 MB 14.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "   ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 200.5/200.5 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488532 sha256=8c142aa87eb52a5e0a11479aab49e462fbcd9d502a279415b4d53e7e7451f33b\n",
      "  Stored in directory: c:\\users\\jonat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\95\\13\\41\\f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by definning a Spark context, which is essentially a client of Spark’s execution environment and acts as the master of your Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sense, we will use Spark local mode, which is useful for (1) experimentation on small data (when you do not have a Spark cluster available) and (2) to teach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[*]')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte é uma alternativa mais detalhada ao código anteriormente executado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note no entanto que não é possível executar múltiplos sparkContexts. You can run only one spark context for one python kernel (notebook). If you need another spark context you can open another notebook, otherwise, there are no reason for multiple spark contexts on the same notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, antes de executar o código seguinte comece por parar o atual sparkcontext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=test>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 63921)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Documents\\DEV\\MESTRADO\\MsC-IPT-Big-Data\\.venv\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Uma ligação existente foi forçada a fechar pelo anfitrião remoto\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#AppName Sets a name for the application, which will be shown in the Spark web UI.\n",
    "#setMaster --master option specifies the master URL for a distributed cluster (spark://HOST:PORT : Connect to the given Spark standalone cluster master. The port must be whichever one your master is configured to use, which is 7077 by default), or local to run locally with one thread, or local[N] to run locally with N threads. You should start by using local for testing. local[*] Run Spark locally with as many worker threads as logical cores on your machine.\n",
    "#set - spark.driver.cores: Number of cores to use. This variable only takes effect in cluster deployment mode. More config options could be defined through the set reserved word\n",
    "#(https://spark.apache.org/docs/latest/configuration.html)\n",
    "\n",
    "confSpark = SparkConf().setAppName(\"test\").setMaster(\"local[*]\").set(\"spark.driver.cores\", 1)\n",
    "sc = SparkContext(conf = confSpark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark é uma framework que executa processamento em memória - sem utilização de escrita e leitura em disco rígido - com o objetivo de ser superior ao algoritmo MapReduce. Para superar as limitações do MapReduce, o Spark faz uso de Resilient Distributed Datasets (RDDs), uma coleção de elementos particionados por entre os nós de um cluster e que podem ser operados em paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez criado, o dataset distribuído (rdd) pode ser operado em paralelo (cada elemento da partição, será processada numa máquina distinta). By Default, Spark creates one Partition for each block of the file (habitualmente o tamanho de cada bloco é de 128Mb). However, one can explicitly specify the number of partitions to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelized Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entendermos o uso de spark vamos considerar a paralelização de uma lista que contém hashtags duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"#epicfail\",  \"#hadoop\", \"#rstats\",  \"#rstudio\", \"#rstats\", \"#spark\", \"#hadoop\", \"#hdfs\",\n",
    "\"#hadoop\",  \"#oreilly\", \"#spark\", \"#python\", \"#spark\", \"#scala\", \"#spark\", \"#strataconf\", \"#strataconf\", \"#oreilly\",\n",
    "\"#spark\", \"#databricks\", \"#hadoop\", \"#hdfs\", \"#spark\",  \"#hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "#rdd_hashtags = sc.parallelize(data, 4) dividiria (de forma explicita) o RDD em 4 partições\n",
    "type(rdd_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O comando `sc.textFile` permite criar um RDD a partir de um ficheiro externo (no caso um ficheiro local)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exercício em questão pretende-se determinar o length (número de chars) de cada linha existente no ficheiro `text.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "\n",
    "#É o mesmo que fazer:\n",
    "#rdd_text = sc.textFile(\"/home/jovyan/word/data/text.txt\"), onde /home/jovyan/word/data/text.txt pode ser entendido com a path HDFS\n",
    "\n",
    "type(rdd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado do `sc.textFile` é um RDD de strings. Note que o RDD é apenas um apontador para o ficheiro, ou seja não contém nenhum conteúdo. The RDD describes the partitions that, when an action is called, become tasks that will read their parts of the input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformação `map` aplica uma função a cada elemento do RDD e devolve um RDD com o resultado. Recorde que a função `map` é uma transformação, o que significa que o Spark não vai avaliar o seu RDD (\"preguiça\") até que uma ação corra em cima dele. No exemplo seguinte tomamos como base o `rdd_text` e calculamos o tamanho em caracteres de cada string (linha) do RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.map(x => len(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "rdd_map = rdd_text.map(lambda s: len(s)) #transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida procedemos à aplicação da ação `collect()` uma ação que procede à coleta os resultados. Em concreto, o Spark divide a computação em tarefas (a correr em diferentes máquinas), e cada máquina corre a sua parte do `map`, retornando apenas a sua resposta ao programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Spark1.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 18]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.collect() #action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a ação `collect` é apenas apropriada para pequenos datasets. A sua ação em cima de grandes datasets pode consumir toda a memória ram, uma vez que os resultados RDD são todos eles puxados para uma única máquina. Se  necessitar de imprimir apenas uns quantos elementos pode, em alternativa, recorrer à ação `take()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.take(1) #action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FlatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` e `FlatMap` são transformaçoes similares, no sentido em que recebem como input uma linha RDD e aplicam uma função em cima dela. \n",
    "\n",
    "`map` converte um RDD de tamanho `n` noutro RDD de tamanho `n`. Ou dito de outra forma, um elemento no input é mapeado num único elemento no output (`one element in` -> `one element out`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Map1.jpg\" width=\"300\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte exemplifica a aplicação do `map`, o qual transforma um RDD de tamanho três `[3,4,5]` noutro RDD de tamanho três `[[3, 9], [4, 16], [5, 25]]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).map(lambda x: [x, x*x]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também `flatMap` transforma uma coleção RDD noutra coleção. No entanto, aqui, não há uma condição que diga que o tamanho do input tem que ser igual ao do output. Na verdade, um elemento no input pode ser mapeado para 0 ou mais elementos no output (`one element in` -> `0 or more elements out`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Map2.jpg\" width=\"300\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte exemplifica a aplicação do `FlatMAP`, o qual transforma um RDD de tamanho três `[3,4,5]` num RDD de tamanho 9 `[3,9,4,16,5,25]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMap` é tipicamente usado para extrair palavras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os exemplos seguintes mostram a diferença entre `map` e `flatMap` para o mesmo conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#epicfail', '#hadoop'],\n",
       " ['#rstats', '#rstudio'],\n",
       " ['#rstats', '#spark'],\n",
       " ['#hadoop', '#hdfs'],\n",
       " ['#hadoop', '#oreilly'],\n",
       " ['#spark', '#python'],\n",
       " ['#spark', '#scala'],\n",
       " ['#spark', '#strataconf'],\n",
       " ['#strataconf', '#oreilly'],\n",
       " ['#spark', '#databricks'],\n",
       " ['#hadoop', '#hdfs'],\n",
       " ['#spark', '#hdfs']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"#epicfail #hadoop\", \"#rstats #rstudio\", \"#rstats #spark\", \"#hadoop #hdfs\", \"#hadoop #oreilly\", \"#spark #python\", \"#spark #scala\", \"#spark #strataconf\", \"#strataconf #oreilly\", \"#spark #databricks\", \"#hadoop #hdfs\", \"#spark #hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "rdd_map = rdd_hashtags.map(lambda x: x.split(' '))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#epicfail',\n",
       " '#hadoop',\n",
       " '#rstats',\n",
       " '#rstudio',\n",
       " '#rstats',\n",
       " '#spark',\n",
       " '#hadoop',\n",
       " '#hdfs',\n",
       " '#hadoop',\n",
       " '#oreilly',\n",
       " '#spark',\n",
       " '#python',\n",
       " '#spark',\n",
       " '#scala',\n",
       " '#spark',\n",
       " '#strataconf',\n",
       " '#strataconf',\n",
       " '#oreilly',\n",
       " '#spark',\n",
       " '#databricks',\n",
       " '#hadoop',\n",
       " '#hdfs',\n",
       " '#spark',\n",
       " '#hdfs']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"#epicfail #hadoop\", \"#rstats #rstudio\", \"#rstats #spark\", \"#hadoop #hdfs\", \"#hadoop #oreilly\", \"#spark #python\", \"#spark #scala\", \"#spark #strataconf\", \"#strataconf #oreilly\", \"#spark #databricks\", \"#hadoop #hdfs\", \"#spark #hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "rdd_map = rdd_hashtags.flatMap(lambda x: x.split(' '))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Spark8.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformação `filter` devolve um rdd que consiste apenas dos elementos que passam na condição estipulada no método filter. No exemplo seguinte tomamos como base o `rdd_text` e procedemos à devolução (com recurso à ação `collect`) dos textos que contenham a palavra `new`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.filter(s ==> \"new\" in s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a new test']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "rdd_filter = rdd_text.filter(lambda s: \"new\" in s)\n",
    "rdd_filter.collect() #action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformação `Distinct` permite remover os duplicados. Para exemplificação desta transformação tomamos como base o `rdd_hashtags`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.distinct()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#python',\n",
       " '#rstudio',\n",
       " '#strataconf',\n",
       " '#rstats',\n",
       " '#hdfs',\n",
       " '#scala',\n",
       " '#spark',\n",
       " '#epicfail',\n",
       " '#databricks',\n",
       " '#hadoop',\n",
       " '#oreilly']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"#epicfail\",  \"#hadoop\", \"#rstats\",  \"#rstudio\", \"#rstats\", \"#spark\", \"#hadoop\", \"#hdfs\",\n",
    "\"#hadoop\",  \"#oreilly\", \"#spark\", \"#python\", \"#spark\", \"#scala\", \"#spark\", \"#strataconf\", \"#strataconf\", \"#oreilly\",\n",
    "\"#spark\", \"#databricks\", \"#hadoop\", \"#hdfs\", \"#spark\",  \"#hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "\n",
    "rdd_distinct = rdd_hashtags.distinct() #Transformation\n",
    "rdd_distinct.collect() #Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Spark9.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ação `collect` já foi explicada no decorrer da aplicação de transformações. Ela permite retornar todos os elementos de um RDD e deve ser usada com precaução em cima de grandes datasets uma vez pode consumir toda a memória ram, dado que os resultados RDD são todos eles puxados para uma única máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma alternativa ao `Collect`, como visto anteriormente, é a ação `Take` a qual permite retornar um determinado número de elementos do RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.take(num)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ação `count` permite contabilizar o número de elementos de um RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.count()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"#epicfail\",  \"#hadoop\", \"#rstats\",  \"#rstudio\", \"#rstats\", \"#spark\", \"#hadoop\", \"#hdfs\",\n",
    "\"#hadoop\",  \"#oreilly\", \"#spark\", \"#python\", \"#spark\", \"#scala\", \"#spark\", \"#strataconf\", \"#strataconf\", \"#oreilly\",\n",
    "\"#spark\", \"#databricks\", \"#hadoop\", \"#hdfs\", \"#spark\",  \"#hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "\n",
    "rdd_distinct = rdd_hashtags.distinct() #Transformation\n",
    "rdd_distinct.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ação `countByValue` permite contabilizar o número de vezes que cada elemento ocorre no RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.countByValue()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"#epicfail\",  \"#hadoop\", \"#rstats\",  \"#rstudio\", \"#rstats\", \"#spark\", \"#hadoop\", \"#hdfs\",\n",
    "\"#hadoop\",  \"#oreilly\", \"#spark\", \"#python\", \"#spark\", \"#scala\", \"#spark\", \"#strataconf\", \"#strataconf\", \"#oreilly\",\n",
    "\"#spark\", \"#databricks\", \"#hadoop\", \"#hdfs\", \"#spark\",  \"#hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "\n",
    "rdd_distinct = rdd_hashtags.distinct() #Transformation\n",
    "rdd_distinct.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ação `reduce` combina os elementos dos RDDs em paralelo (fazendo uso de uma função, e.g., sum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rdd.reduce((x,y) => x + y)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exemplificarmos este processo recorremos ao ficheiro `text.txt`. O objetivo é determinar o número de chars em cada linha (com recurso ao `map`) e proceder à soma agregada através da ação `reduce`. Neste momento, o Spark divide a computação em tarefas (a correr em diferentes máquinas), e cada máquina corre a sua parte do map e da redução, retornando apenas a sua resposta ao programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Spark2.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "rdd_map = rdd_text.map(lambda s: len(s))\n",
    "totalLens = rdd_map.reduce(lambda a, b: a + b)\n",
    "totalLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to get pair RDDs in Spark. Many formats will directly return pair RDDs for their key/value data. In other cases, we have a regular RDD that we want to turn into a pair RDD. We can do this by running a map() function that returns key/value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo seguinte `rdd_text` é um rdd normal carregado a partir de um novo ficheiro `text1.txt`. Com vista à criação de um pair rdd recorremos ao `map`. Assim, cada linha lida a partir do ficheiro vai gerar um tuplo com 1 ocorrência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text1.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text1.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "rdd_map = rdd_text.map(lambda s: (s, 1))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey` é uma transformação que recorre a uma função (e.g., soma) para agregar valores que tenham a mesma key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "O código seguinte usa a transformação `reduceByKey` em cima de pares key-value, para contar quantas vezes cada linha do texto ocorre no ficheiro `text1.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Spark3.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text1.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text1.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "rdd_map = rdd_text.map(lambda s: (s, 1))\n",
    "counts = rdd_map.reduceByKey(lambda a, b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exercício pretende-se que conte o número de ocorrências de cada palavra existente no ficheiro `text1.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/text1.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/text1.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "counts = rdd_text.flatMap(lambda line: line.split(\" \"))\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IP Address Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva um programa em spark que tenha como input o ficheiro `LogSample.txt` e como output o número de ocorrências de cada endereço IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/LogSample.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/LogSample.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_logs = sc.textFile(filename)\n",
    "counts = rdd_logs.map(lambda ip: (ip[0:ip.index(' ')], 1)).reduceByKey(lambda a,b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em cada linha do ficheiro encontramos um ip. \n",
    "\n",
    "`rdd_logs` consegue detetar cada uma dessas linhas (uma vez que estão em linhas diferentes). \n",
    "O comando `ip[0:ip.index(' ')]` vai ficar apenas com o IP (que está compreendido entre o caracter 0 e o espaço `''`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read 2 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo exercício pretende-se que leia os 2 ficheiros de texto: `text` e `text1`:\n",
    "\n",
    "<b>text</b>\n",
    "This is a test\n",
    "This is a new test\n",
    "\n",
    "<b>text1</b>\n",
    "This is a test\n",
    "This is a new test\n",
    "This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/Spark6.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o código é bastante similar ao de cima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"data/text.txt\"\n",
    "#filename1 = '/content/drive/My Drive/Colab Notebooks/data/BigData/text.txt' #Google Colab\n",
    "\n",
    "filename2 = \"data/text1.txt\"\n",
    "#filename2 = '/content/drive/My Drive/Colab Notebooks/data/BigData/text1.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = sc.textFile(\"data\") This will read all the files that are found within the data folder into a single RDD\n",
    "#lines = sc.textFile(\"data/*.txt\") This will read all the txt files that are found within the data folder into a single RDD\n",
    "\n",
    "rdd_text1 = sc.textFile(filename1)\n",
    "rdd_text2 = sc.textFile(filename2)\n",
    "\n",
    "rdd_text = rdd_text1.union(rdd_text2)\n",
    "\n",
    "counts = rdd_text.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exercício seguinte vamos criar um `inverted index` (um processo bastante comum em Information Retrieval) para todos os ficheiros de texto existentes no folder `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/MapReduceIndexing.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Source: https://www.youtube.com/watch?v=sUCiLIXjENw</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com vista a esse objetivo recorremos ao método `wholeTextFile` o qual permite ler todos os ficheiros de um dado folder. Contrariamente ao `textFile`, o `wholeTextFile` devolve não só o conteúdo mas também o nome do ficheiro lido. Mais concretamente devolve um `tuplo (file, content)`. Para acedermos a cada das posições do tuplo (que denominados de `XY`) teremos que indexar `XY[0]` para o file e `XY[1]` para o content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste sentido o conteúdo, através do `XY[1]` é passado para uma função denominada `tokenization` com o objetivo de dividir o texto em tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada token a função lambda vai produzir uma estrutura `((token, nameOfFile), 1)`, onde `token` é o token presentemente a ser processado, `nameOfFile` é o nome do ficheiro (sem a extensão txt) onde o token foi encontrado a partir do código `os.path.basename(xy[0])[:-4]` e 1 corresponde a uma ocorrência do token. Finalmente o sistema vai proceder ao `reduceByKey` agrupando as respetivas words por `fileName`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "#path = '/content/drive/My Drive/Colab Notebooks/data/BigData/' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tokenization(content):\n",
    "    tokens = content.split()\n",
    "    return tokens\n",
    "    \n",
    "rdd_text = sc.wholeTextFiles(path)\n",
    "invertedIndex = rdd_text.flatMap(lambda xy:[((token, (os.path.basename(xy[0])[:-4])),1) for token in tokenization(xy[1])])\\\n",
    "        .reduceByKey(lambda a,b: a+b)\\\n",
    "        .sortBy(lambda xy: xy[0], False)\n",
    "invertedIndex.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte toma como base o RDD anterior e determina o `TotFreq` (ou seja, a frequencia total da palavra no conjunto de todos os documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotFreq = invertedIndex.map(lambda xy: (xy[0][0], xy[1]))\\\n",
    "        .reduceByKey(lambda a,b: a+b)\\\n",
    "        .sortBy(lambda xy: xy[0], False)\n",
    "TotFreq.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read from Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processe o algoritmo WordCount em cima de um ficheiro de um ficheiro de texto um pouco maior. Uma vez que o ficheiro se encontra na web (http://www.gutenberg.org/files/2701/2701-0.txt) recorremos ao código seguinte para guardar o seu conteúdo no disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "url = 'http://www.gutenberg.org/files/2701/2701-0.txt'\n",
    "text = urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarde o conteúdo (text) em /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/Guten.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/Guten.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(filename, 'wb')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o seguinte código para proceder à contabilização da ocorrência das palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile(filename)\n",
    "counts = rdd_text.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a,b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching / Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Spark podemos guardar resultados intermédios na memória (caching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo exemplo vamos trabalhar com o ficheiro `example.log` que segue a seguinte estrutura:\n",
    "\n",
    "```python\n",
    "2015-12-10T13:36:47|86.88.135.133|FR|SUCCESS\n",
    "\n",
    "2015-12-10T13:36:47|25.53.251.132|FR|SUCCESS\n",
    "\n",
    "2015-12-10T15:55:47|28.25.45.154|MA|ERROR\n",
    "\n",
    "2015-12-10T15:55:47|197.127.23.222|MA|ERROR\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na primeira parte do exercício vamos começar por determinar o número total de linhas (existem 731.905 IPs), mostrar as 15 primeiras linhas e por fim determinar o número total de chars existentes no ficheiro (existem mais de 31M de chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/example.log\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/example.log' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read log file\n",
    "rdd_logs = sc.textFile(filename)\n",
    "\n",
    "# Print total number of lines\n",
    "print(rdd_logs.count())\n",
    "\n",
    "# Show the fist 15 lines of data\n",
    "print(rdd_logs.take(15))\n",
    "\n",
    "# Print total lenght of file (chars)\n",
    "rdd_map = rdd_logs.map(lambda s: len(s))\n",
    "total_len = rdd_map.reduce(lambda a, b: a + b)\n",
    "print(total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos dividir cada linha por `\"|\"`. Recorde que cada linha segue a seguinte estrutura: `2015-12-10T15:55:47|197.127.23.222|MA|ERROR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos guardar os resultados em cache para posterior utilização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate items from each line and store results in cache to speed up subsequent access\n",
    "rdd_map = rdd_logs.map(lambda line: line.split(\"|\"))\n",
    "rdd_map.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ter a noção da estrutura de uma linha execute o seguinte código (que deverá devolver: `[['2015-12-10T13:36:47', '86.88.135.133', 'FR', 'SUCCESS']]`). Recorde que existem 731.905 entradas da lista, similares a esta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos determinar o número de logs por país. Observe que a cada country domain e.g., `\"FR\"` (`line[2]`) há lugar a uma ocorrência. Posteriormente o método `reduceByKey` vai proceder à soma das ocorrências por `country domain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of logs per country\n",
    "country_weights = rdd_map.map(lambda line: (line[2], 1))\n",
    "country_weights_res = country_weights.reduceByKey(lambda x, y: x + y)\n",
    "res = country_weights_res.collect()\n",
    "print(len(res)) #imprime o número total de domínios\n",
    "print(res[:10]) #imprime os 10 primeiros resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos determinar o número de logs por tipo. Observe que a cada tipo e.g., \"Sucess\" (line[3]) há lugar a 1 ocorrência. Posteriormente o reduceByKey vai proceder à soma das ocorrências por tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of logs per type\n",
    "type_count = rdd_map.map(lambda line: (line[3], 1))\n",
    "type_res = type_count.reduceByKey(lambda x, y: x + y)\n",
    "res = type_res.collect()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos determinar quando é que os erros começaram. Primeiro, começamos por filtrar os dados que temos em cache apenas a registos marcados como ERROR (line[3]). Posteriormente, para cada linha recebida ficamos apenas com a data, devolvendo por fim a data mínima (oul seja a primeira data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When did the errors start?\n",
    "lines_error = rdd_map.filter(lambda line: line[3] == \"ERROR\")\n",
    "dates_error = lines_error.map(lambda line: line[0])\n",
    "dates_first_error = dates_error.min()\n",
    "print(dates_first_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vamos determinar quantas mensagens de erro foram obtidas num período de tempo compreendido entre 2 a 3h a partir do início do ficheiro de logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte permite determinar a data do 1.º log (que no fundo é a data mínima)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries to work with dates\n",
    "from dateutil.parser import parse\n",
    "\n",
    "line_dates = rdd_map.map(lambda line: [parse(line[0], fuzzy_with_tokens=True)[0]] + line[1:])\n",
    "min_date = line_dates.map(lambda line: line[0]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para perceber melhor a primeira linha de código vamos decompor o problema em partes. Recorde que `rdd_map` tem a estrutura: `[['2015-12-10T13:36:47', '86.88.135.133', 'FR', 'SUCCESS']]`, pelo que `line[0]` é um objeto do tipo `'2015-12-10T13:36:47'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dates = rdd_map.map(lambda line: line[0])\n",
    "line_dates.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte faz uso da library `parse` (`from dateutil.parser import parse`). When fuzzy_with_tokens is true, the parser returns a tuple of a datetime and a tuple of ignored tokens (with the used tokens removed). Assim o código seguinte vai devolver: `[[(datetime.datetime(2015, 12, 10, 13, 36, 47), ('T',))]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dates = rdd_map.map(lambda line: [parse(line[0], fuzzy_with_tokens=True)])\n",
    "line_dates.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como queremos ter acesso apenas ao tuplo da data filtramos os resultados ao primeiro elemento do tuplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dates = rdd_map.map(lambda line: [parse(line[0], fuzzy_with_tokens=True)[0]])\n",
    "line_dates.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim `+ line[1:]` permite adicionar os restantes elementos, ou seja, o IP, o domínio e o tipo. O objeto `line_dates` fica então com o seguinte aspeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[datetime.datetime(2015, 12, 10, 13, 36, 47),\n",
    "  '86.88.135.133',\n",
    "  'FR',\n",
    "  'SUCCESS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dates = rdd_map.map(lambda line: [parse(line[0], fuzzy_with_tokens=True)[0]] + line[1:])\n",
    "line_dates.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exercício pretendemos obter o número total de erros ocorridos desde o primeiro erro num periodo de tempo compreendido entre 2 e 3h. O limite inferior (`lower_th`) é assim definido como a primeira data onde ocorreram erros + 2h enquanto o limite superior (`upper_th`) é definido como a primeira data onde ocorreram erros + 3h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.relativedelta as dateutil_rd\n",
    "\n",
    "# Time window limits\n",
    "lower_th = min_date + dateutil_rd.relativedelta(hours=2) #adiciona 2h à primeira data onde começaram a aparecer erros\n",
    "upper_th =  min_date + dateutil_rd.relativedelta(hours=3) #adiciona 3h à primeira data onde começaram a aparecer erros\n",
    "\n",
    "print(lower_th)\n",
    "print(upper_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida vamos extrair os dados compreendidos entre a janela temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No primeiro passo vamos filtrar o objeto `line_dates`, mais concretamente a data (que poderá ser encontrada em `line[0]`) ao periodo compreendido entre `lower_th` e `upper_th`. Recordamos a estrutura de `line_dates`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[[datetime.datetime(2015, 12, 10, 13, 36, 47),\n",
    "  '86.88.135.133',\n",
    "  'FR',\n",
    "  'SUCCESS']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente (e para o periodo definido) vamos contabilizar as ocorrências de `\"SUCCESS\"` and \"`ERROR\"`, assim como proceder à soma das suas ocorrências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction for this time window\n",
    "win_logs = line_dates.filter(lambda line: line[0] >= lower_th and line[0] <= upper_th)\n",
    "win_type_count = win_logs.map(lambda line: (line[3], 1))\n",
    "win_type_res = win_type_count.reduceByKey(lambda x, y: x + y)\n",
    "res = win_type_res.collect()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Functions to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of Spark’s transformations, and some of its actions, depend on passing in functions that are used by Spark to compute data. For shorter functions, we can pass in lambda expressions. lternatively, we can pass in top-level functions, or locally defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_even(num):\n",
    "    return num % 2 == 0\n",
    "\n",
    "# Create a list of numbers\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd_numbers = sc.parallelize(data)\n",
    "\n",
    "# Use the filter function with the defined function\n",
    "filtered_rdd = rdd_numbers.filter(filter_even)\n",
    "\n",
    "# Collect the filtered data\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo abaixo ilustração a passagem de uma função (a aplicar a todos os elementos do RDD) para um map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to square a number\n",
    "def square(num):\n",
    "    return num ** 2\n",
    "\n",
    "# Create a list of numbers\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd_numbers = sc.parallelize(data)\n",
    "\n",
    "# Use the map function with the defined function\n",
    "squared_rdd = rdd_numbers.map(square)\n",
    "\n",
    "# Collect the squared data\n",
    "squared_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também passar uma função com argumentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the power of a number with a given exponent\n",
    "def power(num, exponent):\n",
    "    return num ** exponent\n",
    "\n",
    "# Create a list of tuples containing numbers and their corresponding exponents\n",
    "data = [(2, 3), (3, 2), (4, 2), (5, 3)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd_numbers = sc.parallelize(data)\n",
    "\n",
    "# Use the map function with the defined function and parameters\n",
    "powered_rdd = rdd_numbers.map(lambda x: power(x[0], x[1]))\n",
    "\n",
    "# Collect the squared data\n",
    "powered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo abaixo indica que o resultado do processamento spark deve ficar guardado na diretoria `output`. Note que esta diretoria não pode existir previamente (ela será criada assim que o comando saveAsTextFile for executado). Não se esqueça de apagar a diretoria no caso de voltar a executar novamente o código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"#epicfail #hadoop\", \"#rstats #rstudio\", \"#rstats #spark\", \"#hadoop #hdfs\", \"#hadoop #oreilly\", \"#spark #python\", \"#spark #scala\", \"#spark #strataconf\", \"#strataconf #oreilly\", \"#spark #databricks\", \"#hadoop #hdfs\", \"#spark #hdfs\"]\n",
    "\n",
    "rdd_hashtags = sc.parallelize(data)\n",
    "rdd_map = rdd_hashtags.flatMap(lambda x: x.split(' '))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/output/'\n",
    "#path = '/content/drive/My Drive/Colab Notebooks/data/BigData/output/' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map.saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva um programa em spark que tenha como input o ficheiro `LogSample.txt` (ficheiro de logs) e como output o número de ocorrências de cada imagem com extensão (`gif` e `jpg`).\n",
    "\n",
    "Comece por definir as seguintes funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def check(text):\n",
    "    pattern = u'.jpg '\n",
    "    regexp=re.compile(pattern)\n",
    "    if regexp.search(text):\n",
    "        return True\n",
    "    pattern = u'.gif '\n",
    "    regexp=re.compile(pattern)\n",
    "    if regexp.search(text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ext(text):\n",
    "    pattern = u'.jpg '\n",
    "    regexp=re.compile(pattern)\n",
    "    if regexp.search(text):\n",
    "        return 'jpg',1\n",
    "    pattern = u'.gif '\n",
    "    regexp=re.compile(pattern)\n",
    "    if regexp.search(text):\n",
    "        return 'gif',1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/LogSample.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/LogSample.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insira o seu código aqui\n",
    "\n",
    "#proceda ao load do dataset\n",
    "\n",
    "#Selecione apenas as linhas que contêm .gif ou .jpg\n",
    "\n",
    "#Contabilize o número de ocorrências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução\n",
    "<!--\n",
    "rdd_text = sc.textFile(filename)\n",
    "\n",
    "#Para selecionar apenas as linhas que contém .gif ou .jpg vamos utilizar a transformação filter\n",
    "linesGifJpg = rdd_text.filter(lambda t: (check(t) == True))\n",
    "\n",
    "#Para contabilizar o número de ocorrências\n",
    "counts = linesGifJpg.map(lambda text: (ext(text))).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "counts.collect()\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
