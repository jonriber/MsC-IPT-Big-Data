{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Big Data</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Spark</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">4 - Streaming</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><a href=\"4-Streaming.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Open-Terminal\" data-toc-modified-id=\"Open-Terminal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Open Terminal</a></span><ul class=\"toc-item\"><li><span><a href=\"#jupyter-notebook\" data-toc-modified-id=\"jupyter-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>jupyter notebook</a></span></li><li><span><a href=\"#docker\" data-toc-modified-id=\"docker-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>docker</a></span></li></ul></li><li><span><a href=\"#Setup-the-SparkStreamingSession\" data-toc-modified-id=\"Setup-the-SparkStreamingSession-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup the SparkStreamingSession</a></span></li><li><span><a href=\"#Input-Sources\" data-toc-modified-id=\"Input-Sources-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Input Sources</a></span><ul class=\"toc-item\"><li><span><a href=\"#queueStream\" data-toc-modified-id=\"queueStream-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>queueStream</a></span></li><li><span><a href=\"#TextFileStream\" data-toc-modified-id=\"TextFileStream-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>TextFileStream</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Count---Lorem.txt\" data-toc-modified-id=\"Word-Count---Lorem.txt-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Word Count - Lorem.txt</a></span></li><li><span><a href=\"#Logs\" data-toc-modified-id=\"Logs-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Logs</a></span></li></ul></li><li><span><a href=\"#SocketTextStream\" data-toc-modified-id=\"SocketTextStream-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>SocketTextStream</a></span><ul class=\"toc-item\"><li><span><a href=\"#ncat\" data-toc-modified-id=\"ncat-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>ncat</a></span><ul class=\"toc-item\"><li><span><a href=\"#Consuming-data-from-a-Web-Server-through-ncat\" data-toc-modified-id=\"Consuming-data-from-a-Web-Server-through-ncat-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Consuming data from a Web Server through ncat</a></span></li><li><span><a href=\"#Setting-up-a-&quot;nc&quot;-Server-and-a-&quot;nc&quot;-Client\" data-toc-modified-id=\"Setting-up-a-&quot;nc&quot;-Server-and-a-&quot;nc&quot;-Client-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>Setting up a \"nc\" Server and a \"nc\" Client</a></span></li><li><span><a href=\"#Setting-up-a-&quot;nc&quot;-Server-and-a-DStream-(socketTextStream)-client\" data-toc-modified-id=\"Setting-up-a-&quot;nc&quot;-Server-and-a-DStream-(socketTextStream)-client-3.3.1.3\"><span class=\"toc-item-num\">3.3.1.3&nbsp;&nbsp;</span>Setting up a \"nc\" Server and a DStream (socketTextStream) client</a></span></li></ul></li><li><span><a href=\"#Twitter\" data-toc-modified-id=\"Twitter-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Twitter</a></span><ul class=\"toc-item\"><li><span><a href=\"#Search-API\" data-toc-modified-id=\"Search-API-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>Search API</a></span></li><li><span><a href=\"#Streaming-API\" data-toc-modified-id=\"Streaming-API-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>Streaming API</a></span><ul class=\"toc-item\"><li><span><a href=\"#Obtain-Tweets-from-Twitter-in-Streaming-Mode-and-make-them-available-on-a-socket-ready-for-being-consumed\" data-toc-modified-id=\"Obtain-Tweets-from-Twitter-in-Streaming-Mode-and-make-them-available-on-a-socket-ready-for-being-consumed-3.3.2.2.1\"><span class=\"toc-item-num\">3.3.2.2.1&nbsp;&nbsp;</span>Obtain Tweets from Twitter in Streaming Mode and make them available on a socket ready for being consumed</a></span></li><li><span><a href=\"#Consume-tweets-available-on-socket-through-a-DStream-(socketTextStream)\" data-toc-modified-id=\"Consume-tweets-available-on-socket-through-a-DStream-(socketTextStream)-3.3.2.2.2\"><span class=\"toc-item-num\">3.3.2.2.2&nbsp;&nbsp;</span>Consume tweets available on socket through a DStream (socketTextStream)</a></span></li></ul></li></ul></li></ul></li></ul></li><li><span><a href=\"#Output-Operations\" data-toc-modified-id=\"Output-Operations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Output Operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#pprint\" data-toc-modified-id=\"pprint-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>pprint</a></span></li><li><span><a href=\"#saveAsTextFiles\" data-toc-modified-id=\"saveAsTextFiles-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>saveAsTextFiles</a></span></li><li><span><a href=\"#foreachRDD\" data-toc-modified-id=\"foreachRDD-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>foreachRDD</a></span></li></ul></li><li><span><a href=\"#Transformations-on-DStreams\" data-toc-modified-id=\"Transformations-on-DStreams-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Transformations on DStreams</a></span><ul class=\"toc-item\"><li><span><a href=\"#updateStateByKey\" data-toc-modified-id=\"updateStateByKey-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>updateStateByKey</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quiz-1\" data-toc-modified-id=\"Quiz-1-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Quiz 1</a></span></li></ul></li></ul></li><li><span><a href=\"#Window-Operations\" data-toc-modified-id=\"Window-Operations-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Window Operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#reduceByKeyAndWindow\" data-toc-modified-id=\"reduceByKeyAndWindow-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>reduceByKeyAndWindow</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quiz-1\" data-toc-modified-id=\"Quiz-1-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Quiz 1</a></span></li><li><span><a href=\"#Quiz-2\" data-toc-modified-id=\"Quiz-2-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Quiz 2</a></span></li></ul></li></ul></li><li><span><a href=\"#DataFrames-on-DStreams\" data-toc-modified-id=\"DataFrames-on-DStreams-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>DataFrames on DStreams</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataFrame-and-SQL\" data-toc-modified-id=\"DataFrame-and-SQL-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>DataFrame and SQL</a></span></li><li><span><a href=\"#Dynamic-Plot\" data-toc-modified-id=\"Dynamic-Plot-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Dynamic Plot</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá saber efetuar streaming de dados com recurso ao Spark.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should know how to stream data using Spark.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Streaming<a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos ao processamento em streaming\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Streaming <a class=\"tocSkip\">\n",
    "Introducing students to streaming\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SocketSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo vamos obter texto (em tempo real) a partir do estabelecimento de um socket TCP (a connection between two computers uses a socket. A socket is the combination of IP address + port). Para recordar o que são sockets veja o seguinte vídeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<div align=\"center\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Y0g3M4VG6Ns\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exemplificar este processo vamos usar o comando `nc` (de NetCat). Ncat is a feature-packed networking utility which reads and writes data across networks from the command line. By default, NCat uses TCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nc` always operates in one of two basic modes: connect mode and listen mode. In <b>listen mode</b>, `nc`  waits for an incoming connection. A listening server normally accepts only one connection and will exit after the client disconnects. In <b>connect mode</b>, `nc`  initiates a connection to a service that is listening somewhere. You can think of listen mode as “server” mode and of connect mode as “client” mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabalhar com o ncat abra a linha de comandos do anaconda e estabeleça comunicação via docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker start Spark\n",
    "docker exec -u root -w /home/jovyan -it Spark bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já no terminal proceda à instalação do comando `ncat`: `apt-get update && apt-get install -y netcat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consuming data from a Web Server through ncat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aprendermos a trabalhar com o ncat vamos começar por nos ligar (enquanto clientes) a uma página web (servidor) e falar com ela (tal como os browsers fazem em background). No terminal execute o seguinte comando. Dê dois Enters a seguir ao `GET`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "nc -C scanme.nmap.org 80\n",
    "GET / HTTP/1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have instructed `nc` to connect to the host scanme.nmap.org on port 80. GET / HTTP/1.0 requests the root document of the server. The web server responds with a status code (HTTP/1.1 200 OK), followed by the HTTP header and the text of the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo exemplo vamos usar o comando `nc` em modo server (listen) e em modo cliente (connect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No server (que deverá ficar à escuta) execute o seguinte comando.\n",
    "```\n",
    "nc -vvv -l -p 9999\n",
    "```\n",
    "\n",
    "- -vvv de verbose, permite ter acesso a um conjunto de mensagens informativas.\n",
    "- -l de listen, permite ficar à escuta\n",
    "- -p 9999, indicação do porto onde fica à escuta. Neste caso 9999 mas poderia ser qualquer outro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abra agora um novo terminal na linha de comandos do anaconda: `docker exec -u root -w /home/jovyan -it Spark bash`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No terminal da nova linha de comandos escreva o seguinte: `nc localhost 9999`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva um texto em qualquer um deles e observe que o mesmo flui entre os dois terminais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de passar para o próximo exemplo prima CTRL+C no cliente por forma a interromper a ligação. Recorde que: A listening server normally accepts only one connection and will exit after the client disconnects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente pode fechar este terminal cliente. Já não vai precisar mais dele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de termos visto que o ncat (1) funciona como cliente; (2) funciona como servidor e cliente entre 2 terminais; vamos agora usar o comando `nc` para montar um servidor que fica à espera de ligação por parte de um cliente que neste caso há-de vir do próprio notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste [exemplo](https://subhamkharwal.medium.com/pyspark-structured-streaming-read-from-sockets-bb3a3c761a5e) vamos obter texto a partir de um socket, vamos contabilizar as ocorrências das palavras as quais serão posteriormente mostradas na consola do anaconda (onde iniciou o container)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkSocket.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For beginning, we will be reading data from a socket using Spark Structured Streaming in real-time to find out the count of each word posted in string. we will start with creating a SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the master URL and the Application name. Master URL can be a Spark, Mesos or Yarn cluster URL or it can be defined as local to run in local mode. When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Socket Word Count\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to read the streaming data — create a `DataFrameStreamReader` (`streaming_df`) dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the streaming dataframe to read from socket\n",
    "# Sockets are not recommended for Production applications is only for debugging and testing applications\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", \"9999\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the socket will only push string data, we have only one column value with datatype as String. Split the string and explode to create a dataframe with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split the strings based on spaces and explode the list to create words column\n",
    "words_df = streaming_df.selectExpr(\"explode(split(value, ' ')) as word\")\n",
    "\n",
    "# Check the schema\n",
    "words_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can aggregate on the words dataframe to find the count of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets aggregate the words_df to find the word counts\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Change the shuffle partitions to 4 as we dont want to run through 200 partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)\n",
    "\n",
    "# Generate aggregated dataframe for word count\n",
    "agg_words_df = words_df \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .agg(count(\"word\").alias(\"count\"))\n",
    "\n",
    "# Print the schema to validate\n",
    "agg_words_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the application, open a terminal (or use the one that is already open) to attach port 9999 to send data. In anaconda prompt write the following: `docker exec -u root -w /home/jovyan -it Spark bash`. If `ncat` is not installed (`nc -h` to verify) you may install it: `apt-get update && apt-get install -y netcat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "nc -vvv -l -p 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will choose the sink as console and outputMode as complete (este código só pode ser executado depois de o nc já ter sido anteriormente lançado no terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink to check the output\n",
    "writing_df = agg_words_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkSocket1.jpg\" width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the results in this jupyter notebook interrupt the previous code and execute the following. Don't forget to launch the `nc` command before in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to display each micro-batch\n",
    "def display_batch(df, epoch_id):\n",
    "    pd_df = df.toPandas()  # Convert to Pandas DataFrame\n",
    "    display(pd_df)         # Display in Jupyter notebook cell\n",
    "    print(f\"Batch {epoch_id} processed\")  # Optional: Print batch info\n",
    "\n",
    "# Write the output to a foreachBatch sink\n",
    "writing_df = agg_words_df.writeStream \\\n",
    "    .foreachBatch(display_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming application\n",
    "writing_df.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://medium.com/expedia-group-tech/apache-spark-structured-streaming-input-sources-2-of-6-6a72f798838c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With file input source, our application will wait for available data in the specified directory. Supported file formats are text, CSV, JSON, ORC, Parquet. We will use some of the Apple stock data `AAPL_2015.csv`, `AAPL_2016.csv` and `AAPL_2017.csv` We will keep all the CSV files locally under the `data` folder. Also, create another folder `data/stream` which we will use to simulate the streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, reverse, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"File Source\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark logging level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains the fields Date,Open,High,Low,Close,Adj Close,Volume and we will extract Name from the filename using a custom function. Here we define the schema and write a custom function to extract the stock ticker symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Adjusted Close\", DoubleType(), True),\n",
    "    StructField(\"Volume\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Extract the Name of the stock from the file name\n",
    "def get_file_name():\n",
    "    file_name = reverse(split(input_file_name(), \"/\")).getItem(0)\n",
    "    stock_name = split(file_name, \"_\").getItem(0)\n",
    "    return stock_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a streaming DataFrame to read csv data from a specified directory data/stream and apply the above schema. We also specify the param `maxFilesPerTrigger = 2`, which means our application will process a maximum of 2 CSV files in each batch. At the end, we create another column called `Name` using the function `getFileName`. That column contains stock ticker symbols like `GOOGL`, `AMZN`, `AAPL` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streaming DataFrame by reading data from File Source\n",
    "initDF = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"maxFilesPerTrigger\", 2)  # This will read a maximum of 2 files per mini-batch. However, it can read less than 2 files.\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(\"data/stream\")\n",
    "    .withColumn(\"Name\", get_file_name())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform basic aggregation on our streaming DataFrame. We group the data based on stock `Name`, `Year` and find the maximum value of the `HIGH` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql.functions import col, year, max\n",
    "\n",
    "# Group by stock name and year, then aggregate to find the maximum \"High\" value\n",
    "stock_df = initDF \\\n",
    "    .groupBy(col(\"Name\"), year(col(\"Date\")).alias(\"Year\")) \\\n",
    "    .agg(max(\"High\").alias(\"Max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the above transformation using a SQL query. In this code sample, we register the streaming DataFrame as a temporary view and execute a SQL query on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Register DataFrame as view\n",
    "initDF.createOrReplaceTempView(\"stockView\")\n",
    "\n",
    "# Run SQL Query\n",
    "query = \"\"\"\n",
    "SELECT year(Date) AS Year, Name, MAX(High) AS Max \n",
    "FROM stockView \n",
    "GROUP BY Name, Year\n",
    "\"\"\"\n",
    "stock_df = spark.sql(query)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the contents of streaming DatFrame to console using `complete` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink to check the output\n",
    "query = stock_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 3) \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming application to run until manual interruption or exception\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start our streaming application now, it waits for data in the `data/stream` folder. Copy files in the sequence below from `data` to `data/stream` to simulate streaming:\n",
    "- `AAPL_2015.csv`\n",
    "- `AAPL_2016.csv`\n",
    "- `AAPL_2017.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the prompt to see the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkFileSource.jpg\" width=\"400\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo exercício vamos simular a criação de um conjunto de ficheiros `txt`. Para isso, considere o ficheiro python `CreateFiles.py` que deverá guardar junto do seu notebook. Não execute o código (executá-lo-á mais tarde no terminal). O objetivo deste código é criar no folder `data/stream` um conjunto de ficheiros `log.txt` (`log1.txt`, `log2.txt` ..., `log30.txt`) no ficheiro `data/lorem.txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "This is use for create 30 file one by one in each 5 seconds interval. \n",
    "These files will store content dynamically from 'lorem.txt' using below code\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    a = 1\n",
    "    with open('data/lorem.txt', 'r') as file:  # reading content from 'lorem.txt' file\n",
    "        lines = file.readlines()\n",
    "        while a <= 30:\n",
    "            totalline = len(lines)\n",
    "            linenumber = randint(0, totalline - 10)\n",
    "            with open('data/stream/log{}.txt'.format(a), 'w') as writefile:\n",
    "                writefile.write(' '.join(line for line in lines[linenumber:totalline]))\n",
    "            print('creating file log{}.txt'.format(a))\n",
    "            a += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida execute o seguinte código. Note que o código é um pouco mais lento porque estamos a ordenar os dados de forma descendente a cada iteração. Sem essa ordenação a execução do código seria mais rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark logging level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Read streaming text files from the source directory\n",
    "lines = spark.readStream \\\n",
    "    .format(\"text\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 2) \\\n",
    "    .load(\"data/stream\")\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Generate running word count\n",
    "word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Sort by count in descending order\n",
    "sorted_word_counts = word_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Write the sorted word counts to the console sink\n",
    "query = sorted_word_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming application to run until manual interruption or exception\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, num terminal do seu notebook ou na prompt do anaconda, desloque-se para a pasta onde tem o ficheiro `CreateFiles.py` e execute o ficheiro: `python CreateFiles.py`. Uma vez que o ficheiro vai criar 30 ficheiros de log na pasta `data/stream` e o código acima está definido para processar 2 ficheiros em cada batch, teremos um total de 15 batches no final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe os resultados na linha de comandos onde instanciou o container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkFileSource1.jpg\" width=\"400\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo seguinte vamos processar alguns logs. Com vista a esse objetivo recorremos ao `apache_log_parser`, uma biblioteca de extrema utilidade no processamento de logs. Comece por proceder à sua instalação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache_log_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entrar no streaming, vamo-nos familiarizar com a biblioteca e a forma como ela nos auxilia no processamento dos logs. Suponha um log com a seguinte estrutura: host - - [timestamp] \"request\" http_code bytes_in_the_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = '109.169.248.247 - - [12/Dec/2015:18:25:11 +0100] \"GET /administrator/ HTTP/1.1\" 200 4263'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por forma a processarmos o log, fazemos uso do `make_parser` uma função da biblioteca `apache_log_parser` que cria uma função capaz de processar o log no formato adequado. Quando aplicada ao log, a função devolverá os valores analisados num dicionário (através de um conjunto de keys pré-definidas, e.g., remote_host, request_method, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para processarmos este log, usamos `%h` para acedermos ao host, `%t` para o timestamp, `%r` para o request, `%s` para o status e `%b` para os bytes devolvidos. Uma lista completa dos valores suportados pode ser encontrada no [github](https://github.com/rory/apache-log-parser) do apache_log_parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_log_parser import make_parser\n",
    "\n",
    "parser = make_parser('%h - - %t \\\"%r\\\" %s %b')\n",
    "parsed_line = parser (line)\n",
    "parsed_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte permite-nos aceder aos valores das keys aí especificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_line['remote_host'])\n",
    "print(parsed_line['time_received_datetimeobj'])\n",
    "print(parsed_line['request_first_line'])\n",
    "print(parsed_line['status'])\n",
    "print(parsed_line['response_bytes_clf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que entendeu o funcionamento da biblioteca defina a função `to_log_line` que tem por objetivo converter uma linha de log num formato estruturado. Note que as exceções podem ocorrer por via da existência de erros nos logs, pelo que é necessário proceder ao seu tratamento através de um try; except."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_log_parser import make_parser\n",
    "\n",
    "def to_log_line(line):\n",
    "    parser = make_parser('%h - - %t \\\"%r\\\" %s %b')\n",
    "    try:\n",
    "        parsed_line = parser(line)\n",
    "        return (parsed_line['remote_host'], \\\n",
    "                parsed_line['time_received_datetimeobj'], \\\n",
    "                parsed_line['request_first_line'].replace('GET', '').replace('HTTP/1.0',''), \\\n",
    "                int(parsed_line['status']), \\\n",
    "                int(parsed_line['response_bytes_clf'].replace('-','0')))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from apache_log_parser import make_parser\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Log Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark logging level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"remote_host\", StringType(), True),\n",
    "    StructField(\"time_received\", TimestampType(), True),\n",
    "    StructField(\"request\", StringType(), True),\n",
    "    StructField(\"status\", IntegerType(), True),\n",
    "    StructField(\"response_bytes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# UDF to apply the parsing function\n",
    "parse_udf = udf(lambda line: to_log_line(line), schema) #Define um UDF chamado parse_udf para aplicar a função de parsing to_log_line a cada linha do DataFrame.\n",
    "\n",
    "# Read streaming text files from the source directory\n",
    "lines = spark.readStream \\\n",
    "    .format(\"text\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 2) \\\n",
    "    .load(\"data/stream\")\n",
    "\n",
    "# Apply the parsing function to each line\n",
    "parsed_logs = lines.select(parse_udf(col(\"value\")).alias(\"parsed_value\")) #Aplica o UDF para analisar cada linha do DataFrame de entrada.\n",
    "\n",
    "# Select the fields from the parsed logs\n",
    "log_df = parsed_logs.select(\n",
    "    col(\"parsed_value.remote_host\").alias(\"remote_host\"),\n",
    "    col(\"parsed_value.time_received\").alias(\"time_received\"),\n",
    "    col(\"parsed_value.request\").alias(\"request\"),\n",
    "    col(\"parsed_value.status\").alias(\"status\"),\n",
    "    col(\"parsed_value.response_bytes\").alias(\"response_bytes\")\n",
    ")\n",
    "\n",
    "# Write the output to console sink to check the output\n",
    "query = log_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming application to run until manual interruption or exception\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do código fornecido, o modo \"append\" foi escolhido porque estamos simplesmente adicionando novos resultados à medida que novos dados são processados. Cada novo registro de log contribui para o conjunto de resultados sem afetar ou modificar registros anteriores. Se precisássemos de uma visão completa e atualizada de todos os dados processados até o momento, independentemente do volume de dados ou do tempo, poderíamos considerar o modo \"complete\". Se estivéssemos realizando alguma forma de agregação ou contagem e precisássemos apenas dos resultados atualizados ou novos, poderíamos considerar o modo \"update\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora inicie o processo de streaming copiando os ficheiros `access1.log` (54KB), `access2.log` (169KB) e `access3.log` (10.936KB) para dentro da pasta `data/stream`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkFileSource2.jpg\" width=\"400\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://subhamkharwal.medium.com/pyspark-structured-streaming-read-from-files-c46fa0ce8888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular thermostat company takes reading from their customer devices to understand device usage. The device data JSON files are generated and dumped in an input folder. The notification team requires the data to be flattened in real-time and written in output in JSON format for sending out notifications and reporting accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkFileSource3.jpg\" width=\"400\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON input file contains the following data points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/SparkFileSource4.jpg\" width=\"400\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to design the real time streaming pipeline to ingest the file and flatten, lets create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Process Files\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will use the `spark.sql.streaming.schemaInference` to allow automatic schemaInference while reading. So before executing the code, make sure if copy/paste the file `device_01.json` to the folder `data/stream`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the schema inferred, we will use `spark.read` instead of `spark.readStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "streaming_df = spark.read\\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"cleanSource\", \"archive\") \\\n",
    "    .option(\"sourceArchiveDir\", \"data/archive/device_data/\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"data/stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read \n",
    "streaming_df.printSchema()\n",
    "streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the DataFrameStreamReader dataframe with the usual `sark.readStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "streaming_df = spark.readStream\\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"cleanSource\", \"archive\") \\\n",
    "    .option(\"sourceArchiveDir\", \"data/archive/device_data/\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"data/stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points to note:\n",
    "- Option `cleanSource` — It can archive or delete the source file after processing. Values can be archive, delete and default is off.\n",
    "- Option `sourceArchiveDir` — Archive directory if the cleanSource option is set to archive.\n",
    "- Option `maxFilesPerTrigger` — Sets how many files to process in a single go.\n",
    "- We need to set `spark.sql.streaming.schemaInference` to True to allow streaming schemaInference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is for multiple devices in list/array we need to explode the dataframe before flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains list/array of device reading\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = streaming_df \\\n",
    "    .select(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \"data\") \\\n",
    "    .withColumn(\"devices\", explode(\"data.devices\")) \\\n",
    "    .drop(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the exploded df\n",
    "flattened_df = exploded_df \\\n",
    "    .selectExpr(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \n",
    "                \"devices.deviceId as deviceId\", \"devices.measure as measure\", \n",
    "                \"devices.status as status\", \"devices.temperature as temperature\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write the streaming data to output folder in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink to check the output\n",
    "writing_df = flattened_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"data/output\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "    \n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que foi criado um output na pasta `data/output` com os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie agora para a pasta `data/stream` o ficheiro `device_02.json`. Observe que o ficheiro `device_01.json` foi entretanto arquivado na pasta `data/archive`. Proceda da mesma forma para o ficheiro `device_03.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data at the output location\n",
    "\n",
    "out_df = spark.read.json(\"data/output/\")\n",
    "out_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
