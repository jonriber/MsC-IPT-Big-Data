{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Big Data</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Spark</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">3 - Dataframes</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "<p><a href=\"3-Dataframes.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Instructions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Windows\" data-toc-modified-id=\"Windows-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Windows</a></span></li><li><span><a href=\"#Google-Colab\" data-toc-modified-id=\"Google-Colab-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Google Colab</a></span></li></ul></li><li><span><a href=\"#Setup-the-SparkSession\" data-toc-modified-id=\"Setup-the-SparkSession-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup the SparkSession</a></span></li><li><span><a href=\"#Create-DataFrame\" data-toc-modified-id=\"Create-DataFrame-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create DataFrame</a></span><ul class=\"toc-item\"><li><span><a href=\"#From-Files\" data-toc-modified-id=\"From-Files-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>From Files</a></span><ul class=\"toc-item\"><li><span><a href=\"#CSV\" data-toc-modified-id=\"CSV-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>CSV</a></span></li><li><span><a href=\"#JSON\" data-toc-modified-id=\"JSON-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>JSON</a></span></li><li><span><a href=\"#TextFile\" data-toc-modified-id=\"TextFile-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>TextFile</a></span></li></ul></li><li><span><a href=\"#From-RDDs\" data-toc-modified-id=\"From-RDDs-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>From RDDs</a></span></li><li><span><a href=\"#Pandas\" data-toc-modified-id=\"Pandas-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Pandas</a></span></li><li><span><a href=\"#Schema\" data-toc-modified-id=\"Schema-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Schema</a></span></li></ul></li><li><span><a href=\"#USStockPrices\" data-toc-modified-id=\"USStockPrices-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>USStockPrices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-Methods\" data-toc-modified-id=\"Inspect-Methods-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Inspect Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#PrintSchema\" data-toc-modified-id=\"PrintSchema-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>PrintSchema</a></span></li><li><span><a href=\"#dtypes\" data-toc-modified-id=\"dtypes-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>dtypes</a></span></li><li><span><a href=\"#columns\" data-toc-modified-id=\"columns-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>columns</a></span></li><li><span><a href=\"#Count\" data-toc-modified-id=\"Count-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Count</a></span></li><li><span><a href=\"#stats:-describe\" data-toc-modified-id=\"stats:-describe-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>stats: describe</a></span></li><li><span><a href=\"#Take\" data-toc-modified-id=\"Take-4.1.6\"><span class=\"toc-item-num\">4.1.6&nbsp;&nbsp;</span>Take</a></span></li><li><span><a href=\"#Show\" data-toc-modified-id=\"Show-4.1.7\"><span class=\"toc-item-num\">4.1.7&nbsp;&nbsp;</span>Show</a></span></li><li><span><a href=\"#First\" data-toc-modified-id=\"First-4.1.8\"><span class=\"toc-item-num\">4.1.8&nbsp;&nbsp;</span>First</a></span></li><li><span><a href=\"#Distinct\" data-toc-modified-id=\"Distinct-4.1.9\"><span class=\"toc-item-num\">4.1.9&nbsp;&nbsp;</span>Distinct</a></span></li></ul></li><li><span><a href=\"#Operation-Methods\" data-toc-modified-id=\"Operation-Methods-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Operation Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Columns-Manipulation\" data-toc-modified-id=\"Columns-Manipulation-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Columns Manipulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-Column\" data-toc-modified-id=\"Add-Column-4.2.1.1\"><span class=\"toc-item-num\">4.2.1.1&nbsp;&nbsp;</span>Add Column</a></span></li><li><span><a href=\"#Update-Column\" data-toc-modified-id=\"Update-Column-4.2.1.2\"><span class=\"toc-item-num\">4.2.1.2&nbsp;&nbsp;</span>Update Column</a></span></li><li><span><a href=\"#Delete-Column\" data-toc-modified-id=\"Delete-Column-4.2.1.3\"><span class=\"toc-item-num\">4.2.1.3&nbsp;&nbsp;</span>Delete Column</a></span></li></ul></li><li><span><a href=\"#Querying-Data\" data-toc-modified-id=\"Querying-Data-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Querying Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select\" data-toc-modified-id=\"Select-4.2.2.1\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;</span>Select</a></span></li><li><span><a href=\"#Filter\" data-toc-modified-id=\"Filter-4.2.2.2\"><span class=\"toc-item-num\">4.2.2.2&nbsp;&nbsp;</span>Filter</a></span></li><li><span><a href=\"#GroupBy\" data-toc-modified-id=\"GroupBy-4.2.2.3\"><span class=\"toc-item-num\">4.2.2.3&nbsp;&nbsp;</span>GroupBy</a></span></li></ul></li><li><span><a href=\"#OrderBy\" data-toc-modified-id=\"OrderBy-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>OrderBy</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Functions</a></span></li></ul></li><li><span><a href=\"#Save-Data-to-File\" data-toc-modified-id=\"Save-Data-to-File-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Save Data to File</a></span></li><li><span><a href=\"#Spark-SQL\" data-toc-modified-id=\"Spark-SQL-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Spark SQL</a></span></li></ul></li><li><span><a href=\"#Twitter-(Quiz)\" data-toc-modified-id=\"Twitter-(Quiz)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Twitter (Quiz)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Determine-the-total-number-of-tweets\" data-toc-modified-id=\"Determine-the-total-number-of-tweets-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Determine the total number of tweets</a></span></li><li><span><a href=\"#Print-the-Schema\" data-toc-modified-id=\"Print-the-Schema-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Print the Schema</a></span></li><li><span><a href=\"#List-the-Columns-of-the-Dataset\" data-toc-modified-id=\"List-the-Columns-of-the-Dataset-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>List the Columns of the Dataset</a></span></li><li><span><a href=\"#Create-New-Dataframe\" data-toc-modified-id=\"Create-New-Dataframe-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Create New Dataframe</a></span></li><li><span><a href=\"#Total-Number-of-Users\" data-toc-modified-id=\"Total-Number-of-Users-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Total Number of Users</a></span></li><li><span><a href=\"#Top-10-users-who-post-the-most\" data-toc-modified-id=\"Top-10-users-who-post-the-most-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Top-10 users who post the most</a></span></li><li><span><a href=\"#Users-with-the-largest-Tweets\" data-toc-modified-id=\"Users-with-the-largest-Tweets-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Users with the largest Tweets</a></span></li><li><span><a href=\"#Name-of-User-with-the-largest-Tweet\" data-toc-modified-id=\"Name-of-User-with-the-largest-Tweet-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Name of User with the largest Tweet</a></span></li><li><span><a href=\"#Tweets'-origin\" data-toc-modified-id=\"Tweets'-origin-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;</span>Tweets' origin</a></span></li><li><span><a href=\"#About-time\" data-toc-modified-id=\"About-time-5.11\"><span class=\"toc-item-num\">5.11&nbsp;&nbsp;</span>About time</a></span></li><li><span><a href=\"#Most-popular-mentioned-users\" data-toc-modified-id=\"Most-popular-mentioned-users-5.12\"><span class=\"toc-item-num\">5.12&nbsp;&nbsp;</span>Most popular mentioned users</a></span></li><li><span><a href=\"#Most-popular-hashtags\" data-toc-modified-id=\"Most-popular-hashtags-5.13\"><span class=\"toc-item-num\">5.13&nbsp;&nbsp;</span>Most popular hashtags</a></span></li><li><span><a href=\"#Presence-of-the-candidates-in-the-Tweets\" data-toc-modified-id=\"Presence-of-the-candidates-in-the-Tweets-5.14\"><span class=\"toc-item-num\">5.14&nbsp;&nbsp;</span>Presence of the candidates in the Tweets</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá saber usar Spark em dataframes.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should know how to use Spark to process dataframes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Introdução a Spark dataframes<a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos a Spark dataframes\n",
    "   \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Introduction to Spark dataframes <a class=\"tocSkip\">\n",
    "Introducing students to Spark dataframes\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was partially based on this article:https://towardsdatascience.com/beginners-guide-to-pyspark-bbe3b553b79f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ainda  não o fez, comece por fazer download da pasta [data](https://www.di.ubi.pt/~rcampos/assets/files_tutorials/BigData/data.zip). De seguida extraia o conteúdo dessa pasta para uma pasta com o nome `data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente faremos uso de dois outros ficheiros:\n",
    "- o ficheiro `stocks_price_final.csv` disponibilizado na página do kaggle: [USStockPrices](https://www.kaggle.com/datasets/dinnymathew/usstockprices), which gathers  NASDAQ and NYSE stocks with their market cap, sectors, etc from Jan 2019 to July 2020. Assim, proceda ao download do ficheiro e coloque-o dentro da pasta `data\\USStockPrices` que deverá criar no seu computador.\n",
    "- o ficheiro `cache-0-json.gz` disponibilizado na página do Internet Archive: [twitter](https://web.archive.org/web/20180417213757/https://ckannet-storage.commondatastorage.googleapis.com/2015-07-09T02:10:43.036Z/cache-0-json.gz) é um sub-set composto por 1M de tweets (de um total de 170M) coletados durante a campanha para as eleições dos EUA em 2012. O ficheiro ocupa 400MB comprimido e 3GB quando descomprimido. Assim, proceda ao download do ficheiro e coloque-o dentro da pasta `data\\twitter` que deverá criar no seu computador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução deste notebook pressupõe a existência da pasta `data`  (e dos seus conteúdos) junto do seu notebook. Necessita também de ter o docker e o pyspark a funcionar de acordo com o explicado no notebook `1 - Introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ainda não o fez, entre agora no [Google Drive](https://drive.google.com/drive/my-drive) e crie a pasta `\\Colab Notebooks\\data\\BigData\\`. De seguida, copie os conteúdos da pasta `data` existentes no seu pc para dentro da pasta `\\Colab Notebooks\\data\\BigData\\`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Google Colab, execute o seguinte comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run pyspark on a local machine we need Java and other software. So instead of the heavy installation procedure, we use Google Colaboratory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be created by a SparkSession object. The SparkSession does for DataFrames what the SparkContext does for RDDs: gives us an entry point to all of the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.cores\", 1).appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em Spark, um DataFrame é uma coleção distribuída de dados, organizados de forma tabular. No exemplo seguinte procuramos criar um DataFrame a partir de um ficheiro csv. O ficheiro `retailCompany.csv` reúne informação das compras realizadas por um conjunto de clientes numa determinada empresa durante o período de um mês (mais de 500k linhas). Para lá do montante despendido, o ficheiro contém, entre outras, informação sobre a idade, género e o estado civil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/retailCompany.csv\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/retailCompany.csv' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(filename, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das formas de criar um spark DataFrame é através da leitura de um ficheiro json. Para exemplificar o processo vamos recorrer ao ficheiro `estudantes.json`, que reúne informações de 10k estudantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/estudantes.json\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/estudantes.json' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(filename)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto o JSON que carregamos tinha uma estrutura linear, a grande maioria destes ficheiros segue uma estrutura encadeada. Este é o caso do JSON seguinte, um JSON de Tweets. Clique [aqui](https://gist.github.com/hrp/900964) para ter acesso à estrutura JSON de um tweet. Observe que na linha 79 dessa página web vai encontrar por exemplo a referência ao \"user.location\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/15m-sample.json\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/15m-sample.json' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(filename)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse sentido, o acesso aos elementos encadeados é feito com recurso a \".\". Veja-se o exemplo seguinte para o elemento \"user\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"created_at\",\"text\",\"user.name\",\"user.location\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/quixote.txt\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/quixote.txt' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(filename)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a DataFrame with a single column named value, which contains the contents of each line in the text file. If you want to split the contents of each line into multiple columns, you can use the split() function to specify the delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `select`: Use this method when you need to apply column expressions and PySpark functions programmatically.\n",
    "- `selectExpr`: Use this method when you prefer to write SQL-like expressions for column transformations, which can sometimes simplify the syntax for complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.select(F.split(df.value, ',').alias('columns'))\n",
    "\n",
    "df = df.selectExpr(\"columns[0] as column1\", \"columns[1] as column2\", \"columns[2] as column3\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possível criar DataFrames a partir de RDDs. No exemplo seguinte começamos por criar um RDD a partir de uma lista de dados. Uma vez criado o RDD podemos criar o DataFrame. Note que, ao contrário de JSON e CSV, onde a operação de leitura cria automaticamente um DataFrame, no caso dos RDDs, a criação de um DataFrame obriga a uma instrução explícitamente definida para tal (`createDataFrame`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([\"Javier 35\", \"Susana 19\", \"Mateo 25\"])\n",
    "people = rdd.map(lambda x: x.split(' '))\n",
    "\n",
    "df = spark.createDataFrame(people, [\"name\", \"age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url_api = \"https://api.coindesk.com/v1/bpi/currentprice.json\"\n",
    "r = requests.get(url_api)\n",
    "content = r.json()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_rdd = spark.sparkContext.parallelize([content])\n",
    "\n",
    "# Read from JSON\n",
    "df = spark.read.json(payload_rdd)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"bpi\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand further elements to read USD data\n",
    "df.select(\"bpi.*\").select(\"USD.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame({\"name\": [\"John\", \"Mary\", \"Mike\"], \"age\": [25, 30, 35]})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert it again to pandas format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code creates a list of tuples, where each tuple contains a name and an age. Then it creates an RDD from the list of tuples and specifies the column names as “name” and “age”. Finally, it converts the RDD to a DataFrame using the createDataFrame() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "data = [(\"John\", 25), (\"Mary\", 30), (\"Mike\", 35)]\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"name\", StringType()),\n",
    "  StructField(\"age\", IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USStockPrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomemos agora como base o ficheiro `stocks_price_final.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** A execução do código seguinte, pressupõe que as intruções do ponto 1 e o setup do ponto 2 foram realizados. *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/USStockPrices/stocks_price_final.csv\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/USStockPrices/stocks_price_final.csv' #Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before changing schema\n",
    "df = spark.read.csv(filename, sep = ',', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PrintSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o código seguinte é possível imprimir o esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos das variáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It returns a list that contains the column names of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o DataFrame tem 10.000 registos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stats: describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe('high').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obter dados do primeiro registo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar os primeiros registos da tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode mostrar mais do que apenas 20 linhas de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `first` permite devolver informações acerca do primeiro registo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo seguinte determina o setor do primeiro registo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()['sector']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode usar a transformação Distinct para entender quantas `symbol` distintos há no DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('symbol').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we show the result of adding a new column named `year`, based on the values found in the `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df.withColumn('year', year(df.date)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that the result is not embedded into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we should execute the following code instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df = df.withColumn('year', year(df.date))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we show how the column `Excellent` could be added with boolean values (true or false) that depend on the following condition: `df[\"volume\"] > 100000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"high\", \"low\", \"volume\").withColumn('Excellent', df[\"volume\"] > 100000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `withColumnRenamed` which takes two parameters existing column name and new column name to rename the existing column. See the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('year', 'Year')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `drop` method to delete a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Year')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used to select single or multiple columns using the names of the columns. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"close\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar o resultado de um conjunto de colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('open', 'high', 'low').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte permite selecionar um conjunto de colunas e recorrer ao `alias` para alterar os seus nomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df['open'].alias('open_value'), df['high'], df['low']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar um conjunto específico (filtrado) de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df['date'] >= '2020-01-01') & (df['date'] <= '2020-01-31')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um outro exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('open', 'high', 'low').filter(df[\"open\"] > 60).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado apenas aos registos que não são nulos: `isNotNull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('open', 'high', 'low').filter(df.symbol.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name itself explains that it groups the data by the given column name and it can perform different operations such as sum, mean, min, max, e.t.c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example explains how to get the average opening, closing, and adjusted stock price concerning industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['industry', 'open', 'close', 'adjusted']).groupBy('industry').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrderBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte exemplifica a função de ordenação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df.groupby('symbol').count().orderBy(desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark oferece-nos um conjunto de funções de agregação que podem ser aplicadas aos dados:\n",
    "- mean: média\n",
    "- count: número total de registos\n",
    "- first: primeiro registo\n",
    "- last: primeiro registo\n",
    "- max: valor máximo\n",
    "- min: valor mínimo\n",
    "- sum: soma dos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, mean, sum\n",
    "df.agg(min(\"high\"),max(\"high\"),mean(\"high\"),sum(\"high\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, min, mean\n",
    "df.groupBy('symbol').agg(min('high'), mean('low')).orderBy(asc('avg(low)')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras funções como o `length` permitem determinar o tamanho de um texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df.select('industry', length('industry').alias('length')).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `to_timestamp` permite converter uma data (em formato string) para um formato de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df.select(to_timestamp('date', 'yyyy-MM-dd')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente podemos criar uma nova coluna denominada `timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df = df.withColumn('timestamp', to_timestamp('date', 'yyyy-MM-dd'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a partir dessa coluna determinar qual a data mais antiga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "df.select('timestamp').agg(min('timestamp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df.write.csv('data/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "df.write.save('data/json', format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing selected data to different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df.select(['date', 'open', 'close', 'adjusted'])\\\n",
    "            .write.csv('data/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "df.select(['date', 'open', 'close', 'adjusted'])\\\n",
    "    .write.save('data/json', format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao contrário dos RDDs, em DataFrames é possível correr SQL queries. Para usar queries teremos no entanto que registar o DataFrame como uma view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente podemos executar código SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from table\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select symbol, high from table where high > 200 and high < 300 order by high desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select mean(high) from table').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Twitter (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício baseado no livro [Large-Scale Data Analytics with Python and Spark](https://www.amazon.com/Large-Scale-Data-Analytics-Python-Spark/dp/100931825X) by Isaac Triguero and Mikel Galar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exercício vamos analisar parte de um dataset que contém todos os tweets relacionados à campanha presidencial dos EUA de 2012, onde Mitt Romney (Republicano) e Barack Obama (Democrata) se enfrentaram. Para determinar os tweets relacionados às eleições, uma série de termos de pesquisa foi escolhida. Esta eleição foi relevante no campo das redes sociais e análise de Big Data porque parte do sucesso de Barack Obama em vencer a eleição foi atribuída ao uso dessas tecnologias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Este grande conjunto de dados foi publicado no [Data Hub](https://datahub.io), não se encontrando mais disponível no seu link original [(quebrado)](https://datahub.io/dataset/twitter-2012-presidential-election). No entanto, graças ao projeto [Internet Archive](https://archive.org/), ainda podemos obtê-lo através deste [link](https://web.archive.org/web/20150831144839/http://datahub.io:80/dataset/twitter-2012-presidential-election). Note que o conjunto de dados é enorme, com mais de 170 milhões de tweets. Neste lab concentramo-nos nos primeiros 1 milhão de tweets, o ficheiro `cache-0-json.gz` (cerca de 400MB comprimidos e 3GB de dados descompactados, ver ponto 1 deste notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** A execução do código seguinte, pressupõe que as intruções do ponto 1 e o setup do ponto 2 foram realizados. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comece por carregar os dados `cache-0-json.gz` em memória (em formato comprimido). Proceda ao `cache` dos dados para não estar constantemente a ler os dados a partir do disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/twitter/cache-0-json.gz\"\n",
    "#filename = '/content/drive/My Drive/Colab Notebooks/data/BigData/twitter/cache-0-json.gz' #Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a execução do código demora apenas 5 minutos (sensivelmente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(filename).cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the total number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df.count()\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter: `1000000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df.printSchema()\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the Columns of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df.columns\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um novo dataframe contendo apenas as seguintes informações:\n",
    "- id - unique Tweet id. Rename it as tweet_id.\n",
    "- user.name - user display name shown in the Tweet. Rename it as displayed_username.\n",
    "- user.screen_name - username identifier (the unique id that corresponds to the @ in Twitter). Rename it as username_id.\n",
    "- user.followers_count - number of followers of the user.\n",
    "- text - text of the Tweet.\n",
    "- retweet_count - Number of retweets of the Tweet.\n",
    "- place.country - Country where the Tweet was published.\n",
    "- entities.user_mentions - Mentions to other users in the Tweet (using @user).\n",
    "- entities.hashtags - Hashtags present in the Tweet (using #hashtag).\n",
    "- created_at - When the Tweet was posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df = df.select(df['id'].alias('tweet_id'), df['user.name'].alias('displayed_username'), df['user.screen_name'].alias('username_id'), 'user.followers_count', 'text', \n",
    "                     'retweet_count', 'place.country','entities.user_mentions', 'entities.hashtags', 'created_at')\n",
    "df.show()\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprima novamente o schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Number of Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sabe este dataset é composto por 1M de tweets. Determine o número total de utilizadores responsáveis por gerar esses tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter: `580606`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df.select('username_id').distinct().count()\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-10 users who post the most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine quais os top-10 utilizadores que mais postaram no twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "from pyspark.sql.functions import desc\n",
    "df.groupby('username_id').count().orderBy(desc('count')).show(10)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('.', 229), ('The Action Group', 211), ('Chris', 131), ('David', 120), ('Alex', 118), ('†', 102),\n",
    "                                  ('Michael', 99), ('∞', 94), ('Matt', 91), ('Sarah', 89)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users with the largest Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine o comprimento médio dos Tweets por utilizador, ordenado do mais longo para o mais curto. Considere apenas as contas do Twitter que tenham postado mais de 100 tweets.\n",
    "\n",
    "Steps to follow:\n",
    "1. Use a `select()` to get the length of the Tweet along with each username (`username_id`).\n",
    "2. Group by username and obtain (by applying aggregation function) the average length of the text and the count of usernames\n",
    "3. Filter by count of usernames (> 100).\n",
    "5. Sort by average length and count of usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "from pyspark.sql.functions import length, mean, count, desc\n",
    "users_tweet_len_df = df.select('username_id', length('text').alias('len')) \\\n",
    "            .groupBy('username_id').agg(mean('len'), count('username_id')) \\\n",
    "            .filter('count(username_id) > 100') \\\n",
    "            .sort(desc('avg(len)'), desc('count(username_id)'))\n",
    "            \n",
    "users_tweet_len_df.show(5)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('TheCypressGang', 143.7364, 129), ('tobreakthenews', 139.5352, 256), ('incognito912', 137.5345, 275),\n",
    "         ('WaterWynd', 137.3333, 192), ('onevoice2', 136.5149, 101), ('USA_Internship', 136.2981, 104),\n",
    "         ('marineagency', 136.2941, 102), ('severewxman', 136.0117, 256), ('chaseboyerSVN', 136.0, 229),\n",
    "         ('FloridaWXalerts', 136.0, 159)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name of User with the largest Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize a informação anterior para determinar o nome do utilizador com o comprimento médio mais longo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "users_tweet_len_df.first()['username_id']\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter: `TheCypressGang`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets' origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao trabalhar com um conjunto de dados das eleições presidenciais dos EUA, poder-se-ia presumir que a maioria dos Tweets viria dos EUA. Vamos verificar se isso é verdade, obtendo o número de Tweets postados para cada país (apenas possível para aqueles Tweets que têm o país definido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduza o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df.select('country') \\\n",
    "                    .filter(df.country.isNotNull()) \\\n",
    "                    .groupBy('country').count() \\\n",
    "                    .sort('count', 'country', ascending=[False, True]).show()\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(u'United States', 5240), (u'Brasil', 1212), (u'United Kingdom', 386), (u'Germany', 223), (u'Indonesia', 188),\n",
    "             (u'Spain', 188), (u'Mexico', 174), (u'Italy', 150), (u'Canada', 121), (u'France', 94)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora obter algumas informações sobre quando os Tweets foram postados, mais concretamente, quantos dias cobre este sub-set? Para isso vamos determinar a data do primeiro e do último Tweet no DataFrame.\n",
    "\n",
    "**Nota:** De acordo com o observado anteriormente a coluna `created_at` é do tipo String no esquema, em vez de um tipo TimeStamp ou DateTime. Isso significa que não podemos trabalhar diretamente com datas. Nesse sentido, precisamos primeiro converte-la para o formato correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('created_at').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do resultado é possível observar que o timestamp de data e hora vem no froamto: dia da semana, mês, dia do mês, hora, fuso horário e ano. O problema é que, o formato correspondente ao dia da semana ('Dom', por exemplo) é esperado apenas para ser usado na formatação de saída de dados de hora, mas não para ser analisado/parsed. Podemos tentar converter isso usando o padrão `\"EEE MMM dd HH:mm:ss Z yyyy\"`, mas tal código retornará um erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df.select(to_timestamp('created_at', 'EEE MMM dd HH:mm:ss Z yyyy')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal tem a ver com uma mudança no Spark 3.0 em relação as strings que podem ser analisadas. Como dito antes,\n",
    "> Símbolos de 'E', 'F', 'q' e 'Q' só podem ser usados para formatação de data e hora, por exemplo, date_format. Não podem ser usados para análise de data e hora, por exemplo, `to_timestamp` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alteramos também outro parâmetro na configuração referente ao fuso horário (para UTC) por forma a garantir que os resultados neste notebook não mudem dependendo de onde o mesmo é executando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, estmaos em condições de proceder à conversão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df.select(to_timestamp('created_at', 'EEE MMM dd HH:mm:ss Z yyyy')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora adicionar estes dados ao dataframe numa coluna denominada `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('timestamp', to_timestamp('created_at', 'EEE MMM dd HH:mm:ss Z yyyy'))\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos agora finalmente obter a data do primeiro e do último tweet do sub-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insira aqui o seu código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "from pyspark.sql.functions import min, max\n",
    "df.select(df.timestamp)\\\n",
    "               .agg(min('timestamp').alias('first'), max('timestamp').alias('last'))\\\n",
    "               .show(truncate=False)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora calcular o número de tweets postados por dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.select(to_date('timestamp').alias('day'))\\\n",
    "                 .groupby('day').count().sort('day')\\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não sabemos porquê, mas parece que não há Tweets do dia 12 de setembro. Pode haver menos Tweets no dia 9 porque a captura pode ter começado durante o dia (de fato, o primeiro Tweet é das 21h). Da mesma forma, os Tweets do dia 13 provavelmente estariam presentes no próximo bloco de dados que não baixámos.\n",
    "\n",
    "Podemos aprender algo sobre as horas em que a discussão estava mais ativa? Ou seja, vamos pegar nos dois dias em que parece haver dados completos (dia 10 e 11) e obter o número de Tweets publicados em cada hora desses dias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "hour_count_df = df.filter((df['timestamp'] >= '2012-09-10') & (df['timestamp'] < '2012-09-12'))\\\n",
    "    .select(hour('timestamp').alias('hour'))\\\n",
    "    .groupby('hour').count().sort('hour')\n",
    "\n",
    "hour_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora plotar esta informação. Repare que `hour_count_df.collect()` é uma lista de tuplos no formato [(hora, número de tweets),...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_count_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código `zip(*hour_count_df.collect())` permite-nos fazer o zip das horas e o zip do número de tweets. Ao adicionar o \"*\" antes de `hour_count_df.collect()` estamos a transformar as tuplas em listas separadas de elementos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_values, y_values) = zip(*hour_count_df.collect())\n",
    "x_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código para plotar os dados encontra-se abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_values, y_values) = zip(*hour_count_df.collect())\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Twitting activity by hour of day')\n",
    "plt.xlabel('Hour of day')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular mentioned users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exercício pretendemos determinar os utilizadores cujo user foi mais mencionado. Utilizadores mencionados aparecem na coluna `user_mentions`, a qual é um array de dicionários. Para desagrupar o array, recorre-se à função `explode`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('user_mentions').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(explode('user_mentions')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após aplicá-la, fazemos outra seleção para manter apenas o nome do utilizador (lembre-se de que `screen_name` é o nome de utilizador exclusivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insira o seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique duas vezes <b>aqui</b> para aceder à solução completa\n",
    "<!--\n",
    "df.select(explode('user_mentions').alias('mentioned')) \\\n",
    "                    .select('mentioned.screen_name') \\\n",
    "                    .groupBy('screen_name').count() \\\n",
    "                    .sort('count', ascending=False) \\\n",
    "                    .show(10)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado a obter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('BarackObama', 13751), ('NICKIMINAJ', 9726), ('YouTube', 5127),\n",
    "                                 ('MittRomney', 3955), ('billmaher', 3906), ('ShareThis', 3876),\n",
    "                                 ('2ChainzLyrics', 3862), ('KattWillliams', 3059), ('ppppolls', 2776),\n",
    "                                 ('HumorOrTruth', 2097)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer o mesmo com as hashtags. Começamos por inspecionar o esquema do DataFrame para entender como pode obter o texto das hashtags e contá-las. Para evitar problemas com maiúsculas e minúsculas, coloque todas em minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "df.select(explode('hashtags').alias('tags')) \\\n",
    "                .select(lower('tags.text').alias('tag')) \\\n",
    "                .groupBy('tag').count() \\\n",
    "                .sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence of the candidates in the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos exercícios anteriores, focamo-nos apenas nas hashtags e menções do utilizador, mas não prestamos atenção ao texto dos Tweets. No exercício a seguir, vamos tentar descobrir quantos Tweets cada partido/candidato esteve presente. \n",
    "\n",
    "Para simplificar a tarefa, vamos assumir que as palavras relacionadas a cada candidato/partido são:\n",
    "- Obama/Democratas: 'obama' e 'democrata';\n",
    "- Romney/Republicanos: 'romney', 'republicano', 'teaparty' e 'tcot'.\n",
    "\n",
    "O nosso primeiro objetivo é criar um DataFrame no qual tenhamos uma coluna que indique se menciona Obama/democratas e outra se menciona Romney/republicanos.\n",
    "\n",
    "Para realizar essa tarefa, seguimos as etapas abaixo:\n",
    "1. Vamos trabalhar apenas com o texto do Tweet. Portanto, comece por obter um novo DataFrame apenas com essas informações. Nomeie-o como `tweet_text_df`. Coloque todo o texto em minúsculas.\n",
    "2. Adicione uma nova coluna ao dataframe chamada `democrata`, que indicará se alguma das palavras correspondentes a democratas ('obama', 'democrata') foi mencionada nesse tweet. Se alguma delas tiver sido mencionada, a coluna terá o valor 'Democrata', caso contrário, o valor será '-'.\n",
    "3. Verifique o número de Tweets nos quais os democratas aparecem agrupando e contando na nova coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, when\n",
    "\n",
    "df = df.withColumn('democrat', when(lower('text').contains('obama'), 'Democrat')\\\n",
    "                                            .when(lower('text').contains('democrat'), 'Democrat')\\\n",
    "                                            .otherwise('-'))\n",
    "\n",
    "df.groupby('democrat').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Repita o passo anterior para adicionar outra coluna para as menções republicanas ('romney', 'republicano', 'teaparty' e 'tcot'). A nova coluna será chamada `Republicano`.\n",
    "5. Verifique o número de Tweets nos quais os republicanos aparecem agrupando e contando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, when\n",
    "\n",
    "df = df.withColumn('republican', when(lower('text').contains('romney'), 'Republican')\\\n",
    "                                            .when(lower('text').contains('republican'), 'Republican')\\\n",
    "                                            .when(lower('text').contains('teaparty'), 'Republican')\\\n",
    "                                            .when(lower('text').contains('tcot'), 'Republican')\\\n",
    "                                            .otherwise('-'))\n",
    "\n",
    "df.groupby('republican').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenha a percentagem de Tweets nos quais apenas democratas, apenas republicanos, tanto democratas quanto republicanos e nenhum deles são mencionados. Arredonde as percentagens para 4 casas decimais e armazene o resultado em `party_percentage_df` (as porcentagens devem ser ordenadas por ordem decrescente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "party_percentage_df = df.groupby('democrat', 'republican').count()\\\n",
    "                                   .select('democrat', 'republican', round((col('count') / ntweets * 100), 4).alias('percentage'))\\\n",
    "                                   .sort('percentage', ascending=False)\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "party_percentage_df.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
