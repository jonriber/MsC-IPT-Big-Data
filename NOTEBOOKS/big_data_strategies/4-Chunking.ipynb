{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Big Data</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Big Data Strategies</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">4 - Chunking</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "<p><a href=\"4-Chunking.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-a-Specific-Subset\" data-toc-modified-id=\"Load-a-Specific-Subset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load a Specific Subset</a></span></li><li><span><a href=\"#Load-Multiple-Subsets\" data-toc-modified-id=\"Load-Multiple-Subsets-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Multiple Subsets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Working-with-Values\" data-toc-modified-id=\"Working-with-Values-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Working with Values</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unique()\" data-toc-modified-id=\"Unique()-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Unique()</a></span></li><li><span><a href=\"#Mean()\" data-toc-modified-id=\"Mean()-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Mean()</a></span></li></ul></li><li><span><a href=\"#Concatenating-Dataframes\" data-toc-modified-id=\"Concatenating-Dataframes-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Concatenating Dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter\" data-toc-modified-id=\"Filter-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Filter</a></span></li><li><span><a href=\"#GroupBy()\" data-toc-modified-id=\"GroupBy()-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>GroupBy()</a></span></li></ul></li><li><span><a href=\"#Operations-Resulting-in-a-Series\" data-toc-modified-id=\"Operations-Resulting-in-a-Series-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Operations Resulting in a Series</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-Null-Values\" data-toc-modified-id=\"Count-Null-Values-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Count Null Values</a></span></li><li><span><a href=\"#Value_Counts()\" data-toc-modified-id=\"Value_Counts()-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Value_Counts()</a></span></li><li><span><a href=\"#Groupby()\" data-toc-modified-id=\"Groupby()-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Groupby()</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá saber efetuar chunking como forma de processar elevados volumes de dados.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should know how to perform chunking to handle big data files.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Técnicas de Compressão<a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos ao chunking de elevados volumes de dados\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Compression Techniques <a class=\"tocSkip\">\n",
    "Introducing students to the process of chunking big data files\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte desta seção teve por base o seguinte artigo: https://towardsdatascience.com/loading-large-datasets-in-pandas-11bdddd36f7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Specific Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma de lidar com os problemas colocados pela leitura de datasets de considerável tamanho, passa por recorrer ao chunking, um processo que divide um dataset num conjunto de sub-datasets mais pequenos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por outras palavras, em vez de ler todos os dados de uma vez na memória, podemos dividir em partes ou blocos menores. No caso de ficheiros CSV, isso significaria carregar apenas algumas linhas em memória num determinado momento. A função `read_csv ()` do Pandas vem com um parâmetro `chunksize` que permite definir o tamanho do chunking. No código seguinte escolhemos um chunk_size de 50.000, o que significa que vamos carregar em memória apenas as top-50.000 linhas de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is the `yellow_tripdata`  which can be found here: https://www.kaggle.com/datasets/bharath150/taxi-data?select=yellow_tripdata_2016-03.csv. Yellow taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=50000\n",
    "chunk = pd.read_csv('data\\yellow_tripdata_2016-03.csv',chunksize=chunk_size)\n",
    "#chunk = pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv',chunksize=chunk_size)\n",
    "print(type(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida recorremos ao `get_chunk` para ter acesso ao respetivo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = chunk.get_chunk(chunk_size)\n",
    "df_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturalmente a quantidade de memória usada pelo dataframe deste chunk em particular (e porque se tratam apenas de 50,000 linhas) é agora muito menor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importação de um pequeno segmento do dataframe (em linha com o aprendido em notebooks anteriores, nomeadamente através do parâmetro skiprows ou do recurso a bases de dados e queries sql) possibilita a execução de um conjunto de operações e uma análise preliminar do dataframe, sem comprometer a rapidez das operações (dada a reduzida dimensão do dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos por exemplo beneficiar da informação anterior para tomar decisões relativamente à seleção de um conjunto restrito de colunas, ou transformação de dados (de object para datetime, através do parâmetro `parse_date`; de int64 para int16, etc, através do parâmetro `dtype`) e avaliar o seu impacto em termos de memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=50000\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "path = \"data\\yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "chunk = pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"})\n",
    "\n",
    "df_chunk = chunk.get_chunk(chunk_size)\n",
    "df_chunk.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliadas as consequências destas opções poderíamos então aplicá-las a todo o dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Multiple Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma outra possibilidade passa por recorrer ao chunking para dividir e guardar os chunks em múltiplos ficheiros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/chunking.jpg\" width=\"300\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Source: https://towardsdatascience.com/loading-large-datasets-in-pandas-11bdddd36f7b</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código seguinte escolhemos um chunk_size de 50.000, o que significa que cada novo ficheiro chunk a ser gerado incluirá apenas 50.000 linhas de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=50000\n",
    "batch_no=1\n",
    "\n",
    "#for chunk in pd.read_csv('data/yellow_tripdata_2016-03.csv',chunksize=chunk_size):\n",
    "for chunk_df in pd.read_csv('f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv',chunksize=chunk_size):\n",
    "    chunk_df.to_csv('f:/O meu disco/data/BigData/yellow_taxi_data/data/chunk'+str(batch_no)+'.csv',index=False)\n",
    "    #chunk_df.to_csv('data/chunk'+str(batch_no)+'.csv',index=False)\n",
    "    print(batch_no)\n",
    "    batch_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada chunk é na verdade um dataframe (chunk_df)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importação de um chunk em específico faz-se depois a partir do seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"f:/O meu disco/data/BigData/yellow_taxi_data/data/chunk1.csv\"\n",
    "#path = \"data/chunk1.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturalmente a quantidade de memória usada pelo dataframe deste chunk em particular (e porque se tratam apenas de 50,000 linhas) é agora muito menor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A divisão do dataset em múltiplos datasets permitiria agora não só a sua exploração sob o ponto de vista de uma análise preliminar (a partir do load de um dataset em específico), mas também a execução de uma série de operações nos vários datasets. Tal procedimento não obriga no entanto a guardar dos vários subsets. De facto podemos proceder à divisão do mesmos por intermédio do chunking, efetuar as necessárias operações, e reunir os resultados num único dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura seguinte exemplifica este processo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/BigData/chunking1.jpg\" width=\"800\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>https://www.aiplusinfo.com/blog/pandas-and-large-dataframes-how-to-read-in-chunks/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo seguinte enviamos para uma lista o número total de `passenger_count` (i.e., de passageiros transportados nas viagens) em cada chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "ListOfResults = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df['passenger_count'].unique()\n",
    "    ListOfResults.extend(chunk_result)\n",
    "        \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente convertemos a lista para um set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ListOfResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo seguinte determinamos a média do campo `fare_amount` para cada chunk, adicionando-a respetivamente a cada posição da lista de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "ListOfResults = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df['fare_amount'].mean()\n",
    "    ListOfResults.append(chunk_result)\n",
    "        \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ListOfResults` é assim uma lista com todas as médias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para termos acesso à média deveremos proceder ao seguinte cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ListOfResults)/len(ListOfResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos começar por exemplificar este processo com recurso a operações de filtro, as quais têm por objetivo selecionar apenas umas quantas observações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte (**uma solução melhor que esta encontra-se mais abaixo nesta seção**) permite dividir o dataset em parcelas de `500,000` linhas e em cada uma dessas parcelas aplicar um filtro que seleciona apenas os registos onde `passenger_count = 4`. O conjunto desses registos é concatenado ao dataframe anterior a cada nova iteração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "df = pd.DataFrame(columns = column_names) #define the df with the header\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df[chunk_df['passenger_count']==4]\n",
    "\n",
    "    df = pd.concat([df, chunk_result])\n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que ocupa apenas 32.5MB de memória RAM ao invés dos iniciais 3.8GB (que o dataset ocupa quando carregado na totalidade em memória)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is better to append the aggregated data to a list, and then build the DataFrame from the list with one call to pd.concat which is a O(N) operation (where N is the size of the chunks). That is, it is better to concat once with a list of many dataframes `pd.concat([list_of_dfs])` instead of doing something like `df = pd.concat([df, chunk])` many times in a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cope with this, we define a `result_temp` list which will get all the chunk dataframes `chunk_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "result_temp = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df[chunk_df['passenger_count']==4]\n",
    "\n",
    "    result_temp.append(chunk_result)\n",
    "    \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1\n",
    "\n",
    "df = pd.concat(result_temp)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is likely not only quicker, but also occupies less memory, only `11MB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um outro exemplo prende-se com a lista de todas as observações NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "result_temp = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df[chunk_df.isnull().any(axis=1)]\n",
    "\n",
    "    result_temp.append(chunk_result)\n",
    "    \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1\n",
    "\n",
    "df = pd.concat(result_temp)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As operações de groupby quando restritas a mais do que uma coluna devolvem o resultado num dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "result_temp = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df.groupby('VendorID').mean()[['trip_distance','fare_amount','tip_amount','tolls_amount','total_amount']]\n",
    "\n",
    "    result_temp.append(chunk_result)\n",
    "    \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1\n",
    "\n",
    "df = pd.concat(result_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar abaixo, temos agora um conjunto de groupbys para cada chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para apurar o resultado final basta fazer novamente um groupby (similar ao anterior) em cima do dataframe apurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('VendorID').mean()[['trip_distance','fare_amount','tip_amount','tolls_amount','total_amount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations Resulting in a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para as operações de agregação recorre-se também a estruturas de dados que vão acumulando os valores agregados de cada chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo seguinte determinamos a percentagem de null values para cada uma das variáveis. Este é um processo um pouco mais trabalhoso dado que os valores obtidos de cada `chunk_df` vêem numa estrutura de pandas series. De qualquer das formas e uma vez que a estrutura se mantém em cada chunk (ou seja, é sempre o mesmo número de variáveis) podemos acumular os vários valores das series de cada dataframe na variável `ListOfValues`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ListOfValues = np.zeros(len(column_names))\n",
    "ListOfValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "shape = 0\n",
    "ListOfValues = np.zeros(len(column_names))\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    shape+= chunk_df.shape[0]\n",
    "    \n",
    "    chunk_result = chunk_df.isna().sum()\n",
    "    ListOfIndexes = chunk_result.index\n",
    "    ListOfValues += chunk_result.values\n",
    "        \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente mostramos os resultados numa series final composta pelos índices e pelos valores acumulados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(ListOfValues,index=ListOfIndexes)\n",
    "series/shape* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value_Counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo seguinte determinamos os `value_counts` do `VendorID`. Neste caso concreto, a estrutura que se obtém a partir de cada chunk (uma série de dados) pode naturalmente incluir valores distintos. Assim, para acumular os valores de cada chunk recorremos a um dicionário. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk['passenger_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "dict_temp = {}\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df['VendorID'].value_counts()\n",
    "    ListOfIndexes = chunk_result.index\n",
    "    ListOfValues = chunk_result.values\n",
    "    \n",
    "    for i in range(len(ListOfIndexes)):\n",
    "        dict_temp[ListOfIndexes[i]] = dict_temp.get(ListOfIndexes[i],0) + ListOfValues[i]\n",
    "        \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente poderíamos enviar estes dados para um dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecounts_dict_df = pd.DataFrame([dict_temp]).T  # Let's tranpose it from row to column vector\n",
    "valuecounts_dict_df = valuecounts_dict_df.reset_index()  # Let's reset the index from the rating keys back to default\n",
    "valuecounts_dict_df.columns=['valuecounts_Keys','Count']\n",
    "valuecounts_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma alternativa passar por obter a série de dados em cada chunk, transformá-los num dataframe, enviá-los para uma lista e no final de todos os chunks concatenar a lista de dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "result_temp = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df['VendorID'].value_counts()\n",
    "    \n",
    "    df = pd.DataFrame([chunk_result]).T  # Let's tranpose it from row to column vector\n",
    "    df = df.reset_index()  # Let's reset the index from the rating keys back to default\n",
    "    df.columns=['VendorID','Counts']\n",
    "\n",
    "    result_temp.append(df)\n",
    "           \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1\n",
    "\n",
    "df = pd.concat(result_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida podemos ver o dataframe concatenado. Naturalmente teremos ainda que fazer um groupby (ver abaixo) para agrupar os valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte executa o groupby procedendo à soma dos valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['VendorID'])['Counts'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo exemplo pretendemos determinar a média do `trip_distance` por `VendorID`, razão pela qual vamos proceder a um `groupby`. O código abaixo mantém-se igual ao anterior, exceção feita à substituição do código de `value_counts` pelo código do `group_by`. De facto, o sistema continuará a gerar um pandas series. Só que ao invés do value_counts onde contabilizava valores, no caso da operação abaixo proceda à soma de valores médios, razão pela qual, posteriormente ainda vamos ter que dividir esses valores pelo número total de chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "dict_temp = {}\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df.groupby('VendorID').mean()['trip_distance']\n",
    "    ListOfIndexes = chunk_result.index\n",
    "    ListOfValues = chunk_result.values\n",
    "    \n",
    "    for i in range(len(ListOfIndexes)):\n",
    "        dict_temp[ListOfIndexes[i]] = dict_temp.get(ListOfIndexes[i],0) + ListOfValues[i]\n",
    "        \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como referido anteriormente, para apurarmos a média teremos ainda que dividir a soma dos valores operados pelo número de chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_temp:\n",
    "    dict_temp[key] = dict_temp[key]/chunk_no\n",
    "\n",
    "dict_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em alternativa podemos obter a série de dados em cada chunk, transformá-los num dataframe, enviá-los para uma lista e no final de todos os chunks concatenar a lista de dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size=500000\n",
    "chunk_no=0\n",
    "column_names = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"payment_type\", \"fare_amount\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "result_temp = []\n",
    "\n",
    "path = \"data/yellow_tripdata_2016-03.csv\"\n",
    "#path = \"f:\\O meu disco\\data\\BigData\\yellow_taxi_data\\yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "for chunk_df in pd.read_csv(path,chunksize=chunk_size,\n",
    "                    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                    usecols=column_names,\n",
    "                    dtype={\"VendorID\":\"int16\", \"passenger_count\": \"int8\", \"trip_distance\":\"float32\", \"payment_type\":\"int8\",  \"fare_amount\":\"float32\",\"tip_amount\":\"float32\",\"tolls_amount\":\"float32\",\"total_amount\":\"float32\"}):\n",
    "    \n",
    "    chunk_result = chunk_df.groupby('VendorID').mean()['trip_distance']\n",
    "    \n",
    "    df = pd.DataFrame([chunk_result]).T  # Let's tranpose it from row to column vector\n",
    "    df = df.reset_index()  # Let's reset the index from the rating keys back to default\n",
    "    df.columns=['VendorID','trip_distance']\n",
    "\n",
    "    result_temp.append(df)\n",
    "           \n",
    "    print(f\"chunk number = {chunk_no}\")\n",
    "    chunk_no+=1\n",
    "\n",
    "df = pd.concat(result_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida podemos ver o dataframe concatenado. Naturalmente teremos ainda que fazer um groupby (ver abaixo) para agrupar os valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código seguinte executa o groupby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('VendorID').mean()['trip_distance']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
