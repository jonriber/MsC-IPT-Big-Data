{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISE DE GRANDE VOLUME DE DADOS - MsC - IPT-PT\n",
    "\n",
    "## PROJETO 1\n",
    "\n",
    "### JONATAS RIBEIRO\n",
    "\n",
    "### AQUISIÇÃO, ARMAZENAMENTO E RECUPERAÇÃO DE DADOS EM LARGA ESCALA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREFA 1 - FAMILIARIZAÇÃO COM A OBTENÇÃO DE DADOS A PARTIR DE FICHEIROS PDF\n",
    "\n",
    "Optei por obter os 100 ficheiros de forma automatizada, a utilizar web scrapping.\n",
    "\n",
    "O link utilizado para obtenção dos ficheiros é : <\"https://openresearch.ceu.edu/browse?type=dateissued\">\n",
    "\n",
    "O principal tema são trabalhos acadêmicos da Central European University, a respeito de sociologia.\n",
    "\n",
    "- Conjunto aproximado de 100 ficheiros em PDF na pasta \"PDF_DATA_SOURCE\"\n",
    "- Tema: Sociologia Europeia\n",
    "- Modo de obtenção: web scrapping\n",
    "  - Selenium v4\n",
    "    - obtenção de 100 urls de artigos científicos na Central European University\n",
    "    - Página inicial disponibiliza apenas 20 items por vez\n",
    "    - Carregar no Botão next até o limite de 100 itens atingido\n",
    "    - Segunda parte do script é aceder a cada um dos links obtidos e realizar o download\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-requisitos para esta task\n",
    "\n",
    "Obter os 100 pdf´s de forma automatizada, através de web scrapping.\n",
    "\n",
    "O código utilizado pode ser verificado nas boxes a seguir.\n",
    "\n",
    "Não se faz necessário sua execução, uma vez que a pasta com o conteúdo extraido se encontra em meu Microsoft sharepoint\n",
    "sharepoint:\n",
    "\n",
    "- <https://politecnicotomar-my.sharepoint.com/:f:/g/personal/aluno26099_ipt_pt/Es_dk916aRZPhHvo-DKdekYBAmTehcrHRR4wFcZEVG6uJA?email=ricardo.campos%40ipt.pt&e=uUaouO>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso o utilizador deste notebook deseje verificar a execução do script de obtenção de pdfs, se faz necessária \n",
    "a instalação dos packages a seguir para a correta execução do script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "%pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import shutil\n",
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://openresearch.ceu.edu/browse?type=dateissued\")\n",
    "\n",
    "article_url_count = 0\n",
    "href_links = []\n",
    "while article_url_count < 100:\n",
    "    articles = driver.find_elements(\"css selector\", \".description-content a\")\n",
    "\n",
    "    for article in articles:\n",
    "        href_links.append(article.get_attribute(\"href\"))\n",
    "        article_url_count += 1\n",
    "        if article_url_count == 100:\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CLASS_NAME, 'next-page-link')\n",
    "        \n",
    "        if next_button:\n",
    "            next_button.click()\n",
    "        else:\n",
    "            break\n",
    "    except:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"href_links:\",href_links)\n",
    "\n",
    "destination_folder = \"pdf_source\"\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "    \n",
    "for url in href_links:\n",
    "    driver.get(url)\n",
    "    \n",
    "    download_button = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'file-download-button')))\n",
    "    download_link = download_button.get_attribute('href')\n",
    "    \n",
    "\n",
    "    try:\n",
    "        driver.get(download_link)\n",
    "\n",
    "        file_name = download_link.split('/')[-1].split('?')[0]\n",
    "        # time.sleep(20)  # Adjust the sleep time as needed\n",
    "\n",
    "        if not os.path.exists(destination_folder):\n",
    "            os.makedirs(destination_folder)\n",
    "        time.sleep(20)  \n",
    "\n",
    "        shutil.move(file_name, f\"{destination_folder}/{file_name}\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        pass\n",
    "    \n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de texto de cada ficheiro PDF e Ficheiro JSON com estrutura apropriada\n",
    "\n",
    "Em seguida, com os documentos armazenados no ambiente de programação, preferencialmente na pasta nomeada:\n",
    "\n",
    "'pdf_source'\n",
    "\n",
    "O script a seguir é utilizado para a extração de informações (texto) de cada um dos pdf´s.\n",
    "\n",
    "As informações consideradas relevantes são:\n",
    "- author\n",
    "- title\n",
    "- abstract\n",
    "- publication_date\n",
    "- id\n",
    "\n",
    "Com os dados relevantes, diversos itens são armazenadas e devidamente ordenados em um novo ficheiro JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication_date(metadata):\n",
    "    date = metadata.get('CreationDate', '')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        \n",
    "\n",
    "        print(\"stating to extract info from pdf\")\n",
    "        info = {}\n",
    "        info['id'] = generate_unique_id()\n",
    "        info['authors'] = pdf.metadata.get('Author', '')\n",
    "        info['title'] = pdf.metadata.get('Title', '')\n",
    "        info['abstract'] = ''\n",
    "        info['publication_date'] = get_publication_date(pdf.metadata)\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if 'abstract' in text.lower():\n",
    "                # Assuming abstract starts with the word \"Abstract\"\n",
    "                abstract_index = text.lower().find('abstract')\n",
    "                abstract = text[abstract_index:]\n",
    "                info['abstract'] += abstract\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract information from PDFs, using previous declared functions\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "pdf_folder = 'PROJECTS/PROJ1/pdf_source'\n",
    "output_data = []\n",
    "\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        print(\"pdf file found\", file_name)\n",
    "        pdf_path = os.path.join(pdf_folder, file_name)\n",
    "        info = extract_info_from_pdf(pdf_path)\n",
    "        output_data.append(info)\n",
    "print(\"output_data:\",output_data)        \n",
    "output_file = 'extracted_info.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(output_data, json_file, indent=4)\n",
    "    \n",
    "print(f\"Extracted information saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conteúdo do ficheiro JSON e nuvem de palavras\n",
    "\n",
    "Para a criação da nuvem de palavras, foi utilizada a biblioteca wordCloud, com base nos abstracts e titles obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## opening JSON file function\n",
    "\n",
    "def open_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "\n",
    "## main function to generate word clouds\n",
    "    \n",
    "data = open_json_file('extracted_info.json')\n",
    "abstracts_text = ' '.join(item['abstract'] for item in data)\n",
    "word_cloud = WordCloud(width=800, height=400, background_color='white').generate(abstracts_text)\n",
    "\n",
    "abstracts_text = ' '.join(item['title'] for item in data)\n",
    "word_cloud2 = WordCloud(width=800, height=400, background_color='white').generate(abstracts_text)\n",
    "\n",
    "## Abstract word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "## Title word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(word_cloud2, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over the first 5 items in the data and print information\n",
    "\n",
    "for item in data[:5]:\n",
    "        print(\"ID:\", item['id'])\n",
    "        print(\"Authors:\", item['authors'])\n",
    "        print(\"Title:\", item['title'])\n",
    "        print(\"Abstract:\", item['abstract'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREFA 2 - Familiarização com a obtenção de dados a partir de packages Python\n",
    "\n",
    "- A utilizar o recurso do package do wikipedia para a criação de um dataset com 2000 imagens\n",
    "- Dois temas distintos\n",
    "  - Design\n",
    "  - Restaurants\n",
    "\n",
    "Disponibilizo a pasta partilhada com o download realizado de 2000 imagens:\n",
    "\n",
    "- <https://politecnicotomar-my.sharepoint.com/:f:/g/personal/aluno26099_ipt_pt/Eg8VdHKeNPlJipYZazCRanABaKpougjpEtpKvWkOnFQyQg?email=ricardo.campos%40ipt.pt&e=7aXO3A>\n",
    "\n",
    "A seguir, o script utilizado para a obtenção do dataset utilizando o package 'wikipedia'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## installing necessary packages\n",
    "\n",
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query wikipedia for a given search term\n",
    "\n",
    "def search_wikipedia(query, results=10):\n",
    "    return wikipedia.search(query, results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download image function\n",
    "\n",
    "def download_image(article):\n",
    "    print(\"Downloading images for article:\", article)\n",
    "    try:\n",
    "        page = wikipedia.page(article)\n",
    "        print(\"Found page:\", page.images)\n",
    "        images = page.images\n",
    "        for image in images:\n",
    "            if image.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "                print(\"Found image:\", image)\n",
    "                if image:\n",
    "                    image_urls.append(image)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # Skip disambiguation pages\n",
    "        pass\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        print(f\"Page not found for article: {article}\")\n",
    "        pass\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save images to a folder function\n",
    "  \n",
    "def try_to_save_image(url, i, save_dir):\n",
    "    print(f\"Downloading image {i}: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        extension = url.split('.')[-1]\n",
    "        with open(save_dir + f'image_{i}.{extension}', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image {i}: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function  \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "print(\"starting the script\")\n",
    "\n",
    "design_articles = search_wikipedia(\"Design\", results=1000)\n",
    "restaurant_articles = search_wikipedia(\"Restaurant\", results=1000)\n",
    "\n",
    "articles = design_articles + restaurant_articles\n",
    "\n",
    "print(articles)\n",
    "\n",
    "image_urls = []\n",
    "\n",
    "for article in articles:\n",
    "    download_image(article)\n",
    "\n",
    "save_dir = 'image_dataset/'\n",
    "\n",
    "#Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for i, url in enumerate(image_urls):\n",
    "\n",
    "    try_to_save_image(url, i, save_dir)\n",
    "\n",
    "print(\"Script finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREFA 3 - Familiarização com Web Scraping\n",
    "\n",
    "- A utilizar o recurso de web scrapping do Selenium v4\n",
    "- O tema principal é a obtenção de informações sobre o parlamento Europeu, nomeadamente: \n",
    "  - ùltimas notícias relevantes\n",
    "  - Próximos eventos agendados\n",
    "- Os dados são disponibilizados em uma lista final de dicionários (objetos), afim de facilitar o consumo da informação\n",
    "em outras aplicações.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_URL = \"https://www.europarl.europa.eu/news/pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver functions\n",
    "# using Chrome to access web\n",
    "\n",
    "def init_driver():\n",
    "    driver = webdriver.Chrome()\n",
    "    return driver\n",
    "\n",
    "def quit_driver():\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_news(id,driver):\n",
    "    try:\n",
    "        # Get the last news\n",
    "        news = driver.find_elements(By.CLASS_NAME, id)\n",
    "        return news\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_events_list(id,driver):\n",
    "    \n",
    "    try:\n",
    "        event_list_element = driver.find_element(By.ID, id)\n",
    "        list_items = event_list_element.find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "        return list_items\n",
    "    except Exception as e:\n",
    "        print(\"No list elements found:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_context(event):\n",
    "    try:\n",
    "        date_element = event.find_element(By.CLASS_NAME, \"ep_date\")\n",
    "        date = date_element.text.strip()\n",
    "        \n",
    "        type_element = event.find_element(By.CLASS_NAME, \"ep_type\")\n",
    "        event_type = type_element.text.strip()\n",
    "        \n",
    "        # Extract the event title\n",
    "        title_element = event.find_element(By.CLASS_NAME, \"ep_title\")\n",
    "        title = title_element.text.strip()\n",
    "        \n",
    "        # Extract the event location\n",
    "        location_element = event.find_element(By.CLASS_NAME, \"ep_location\")\n",
    "        location = location_element.text.strip()\n",
    "    \n",
    "     # Print the extracted information for each event\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Type:\", event_type)\n",
    "        print(\"Title:\", title)\n",
    "        print(\"Location:\", location)\n",
    "        print() \n",
    "        return {date, event_type, title, location}\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_news_context(news):\n",
    "    \n",
    "    # print(\"Extracting news context...\",news)\n",
    "    try:\n",
    "        # Get the news context\n",
    "        title_element = news.find_element(By.CLASS_NAME, \"ep_title\")\n",
    "        title = title_element.text\n",
    "        \n",
    "        # date_element = news.find_element(By.CSS_SELECTOR, \"time[itemprop='datePublished']\")\n",
    "        # date = date_element.get_attribute(\"datetime\")\n",
    "        \n",
    "        content_element = news.find_element(By.CLASS_NAME, \"ep-a_text\")\n",
    "        content = content_element.text\n",
    "        \n",
    "        if len(title) > 0 and len(content) > 0:\n",
    "        \n",
    "            # print(\"Title:\", title)\n",
    "            # print(\"Publication Date:\", date)  \n",
    "            # print(\"Content:\", content)\n",
    "            return {title, content}\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main script to obtain desired information\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "driver = init_driver()\n",
    "\n",
    "driver.get(TARGET_URL)\n",
    "\n",
    "extracted_news = get_last_news(\"ep_gridcolumn\",driver)\n",
    "# if len(extracted_news) >5:\n",
    "#     extracted_news = extracted_news[:5]\n",
    "# print(\"Extracted news:\", extracted_news)\n",
    "final_news = []\n",
    "for news in extracted_news:\n",
    "    new = get_news_context(news)\n",
    "    if new != None:\n",
    "        final_news.append(new)\n",
    "        # print(\"New:\", new)\n",
    "print(\"Final news:\", final_news)\n",
    "\n",
    "final_event_list = []\n",
    "extracted_next_events = get_next_events_list(\"event-list\",driver)\n",
    "\n",
    "for event in extracted_next_events:\n",
    "    event = get_event_context(event)\n",
    "    if event != None:\n",
    "        final_event_list.append(event)\n",
    "        # print(\"Event:\", event)\n",
    "print(\"Final event list:\", final_event_list)\n",
    "\n",
    "quit_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREFA 4 - Familiarização com a obtenção de dados a partir de APIs\n",
    "\n",
    "- A utilizar o recurso de requisição web requests para a chamada de uma API gratuíta\n",
    "- O tema principal é relacionado as frases do famoso 'chuck norris' \n",
    "- Os dados são obtidos a cada 30 minutos\n",
    "- Uma VM foi provisionada no Microsoft Azure, para executar o script python\n",
    "- Os logs estão a ser gravados em um ficheiro a parte, para o registo do status da requisição e seu conteúdo\n",
    "- O ficheiro log é finalmente convertido para JSON\n",
    "\n",
    "Minha maior dificuldade após o término do script de aquisição de dados da API na VM, foi exatamente o download dos \n",
    "ficheiros.\n",
    "\n",
    "Para isso, se fez necessário a configuração de SSH e o uso do terminal com o comando 'SCP' \n",
    "\n",
    "Abaixo, o link para acesso a pasta que contém os logs, armazenados no sharepoint:\n",
    "\n",
    "- <https://politecnicotomar-my.sharepoint.com/:f:/g/personal/aluno26099_ipt_pt/EvEawYLBSsBFpqZFYOa-pdoBPGKo41IHpa3GFp_K2d-pAw?email=ricardo.campos%40ipt.pt&e=mP0wRV>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(filename='api_data.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def retrieve_data_from_api():\n",
    "    try:\n",
    "        url = \"https://matchilling-chuck-norris-jokes-v1.p.rapidapi.com/jokes/random\"\n",
    "\n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"X-RapidAPI-Key\": \"aa24d1a197msh7c322f1111480c2p16c025jsn35b22e6582d6\",\n",
    "            \"X-RapidAPI-Host\": \"matchilling-chuck-norris-jokes-v1.p.rapidapi.com\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        print(response.json())\n",
    "        \n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        logging.info(f'Data retrieved successfully from the API: {response.json()}')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {str(e)}')\n",
    "        logging.error(f'An error occurred: {str(e)}')\n",
    "        \n",
    "def job():\n",
    "    print(\"Job started\")\n",
    "    retrieve_data_from_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "print(\"Init script\")\n",
    "\t#schedule.every().hour.do(job)\n",
    "for _ in range(24 * 5):\n",
    "    print(\"Attempt:\",_)\n",
    "    job()\n",
    "    #schedule.run_pending()\n",
    "    time.sleep(1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o código acima, realizei o logging das chamadas a API do chuck norris.\n",
    "\n",
    "Em seguida, tenho que realizar o parse dos logs para o formato JSON.\n",
    "\n",
    "Os logs estão na pasta chamada data_logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "def process_log_file(log_file):\n",
    "    parsed_logs = []\n",
    "    with open(log_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        parsed_logs.append(parse_log_line(line))\n",
    "    \n",
    "    return parsed_logs\n",
    "\n",
    "def parse_log_line(line):\n",
    "    print(\"parsing log line:\",line)\n",
    "    json_content = line.split('{', 1)[1].rsplit('}', 1)[0]\n",
    "    print(\"json_content:\",json_content)\n",
    "    \n",
    "    data_dict = literal_eval(\"{\" + json_content + \"}\")\n",
    "    # Parse the JSON content\n",
    "    return {\n",
    "        'log_data': data_dict\n",
    "    }\n",
    "def main(logs_folder, output_file):\n",
    "    all_logs = []\n",
    "    for filename in os.listdir(logs_folder):\n",
    "        if filename.endswith('.log'):\n",
    "            log_file = os.path.join(logs_folder, filename)\n",
    "            all_logs.extend(process_log_file(log_file))\n",
    "    \n",
    "    json_logs = json.dumps(all_logs, indent=4)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(json_logs)\n",
    "        \n",
    "        \n",
    "# PROJECTS\\PROJ1\\data_logs\n",
    "# if __name__ == \"__main__\":\n",
    "logs_folder = 'data_logs'\n",
    "output_file = 'api_response_2.json'\n",
    "main(logs_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima irá produzir um ficheiro JSON, com o conteúdo da minha API.\n",
    "\n",
    "Para iterar sobre os 5 primeiros items do meu ficheiro, podemos utilizar o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = 'api_response_2.json'  \n",
    "with open(filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in data[:5]:\n",
    "    log_data = item['log_data']\n",
    "    print(\"ID:\", log_data['id'])\n",
    "    print(\"Value:\", log_data['value'])\n",
    "    print(\"Categories:\", log_data['categories'])\n",
    "    print(\"Created At:\", log_data['created_at'])\n",
    "    print(\"Updated At:\", log_data['updated_at'])\n",
    "    print(\"Icon URL:\", log_data['icon_url'])\n",
    "    print(\"URL:\", log_data['url'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREFA 5 - Familiarização com o armazenamento e recuperação de dados em larga escala \n",
    "\n",
    "- A utilizar o recurso do redis + docker\n",
    "\n",
    "Primeiramente, é necessário garantir que 'docker' está corretamente instalado na máquina.\n",
    "\n",
    "Em seguida, utilizar a imagem oficial do 'REDIS', disponível em 'Docker Hub'.\n",
    "\n",
    "\n",
    "'docker run --name proj1 -d -p 6379:6379 redis' \n",
    "\n",
    "Em seguida instalar o package do redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "\n",
    "filename = 'api_response_2.json'  # Replace with your JSON file path\n",
    "with open(filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Index the JSON data into Redis\n",
    "for i, item in enumerate(data):\n",
    "    r.set(f'item:{i}', json.dumps(item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
